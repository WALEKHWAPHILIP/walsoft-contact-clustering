[
  {
    "objectID": "walsoft_contact_segmentation.html",
    "href": "walsoft_contact_segmentation.html",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "",
    "text": "Segment contacts into actionable behavioral groups to support outreach planning, CRM prioritization, and relationship management.\n\n\n\n\n\n\nImportantHard constraints enforced in this project\n\n\n\nTime bands (exact):\n\nbusiness_hours: 08:00–17:59\n\nevening: 18:00–20:59\n\nlate_night: 21:00–23:59\n\nearly_morning: 00:00–07:59\n\nWorking days:\n\nOpen days: Monday–Saturday\n\nClosed day: Sunday only\n\n\n\n\n\n\n\n\n\nTipWorkflow (phases)\n\n\n\nPhase 1 Audit → Phase 2 Contact-level features → Phase 3 Preprocess → Phase 4 Choose K → Phase 5 Fit/Interpret → then stability/ARI → sensitivity (feature blocks) → PCA viz → final recommendations.",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#business-objective",
    "href": "walsoft_contact_segmentation.html#business-objective",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "",
    "text": "Segment contacts into actionable behavioral groups to support outreach planning, CRM prioritization, and relationship management.\n\n\n\n\n\n\nImportantHard constraints enforced in this project\n\n\n\nTime bands (exact):\n\nbusiness_hours: 08:00–17:59\n\nevening: 18:00–20:59\n\nlate_night: 21:00–23:59\n\nearly_morning: 00:00–07:59\n\nWorking days:\n\nOpen days: Monday–Saturday\n\nClosed day: Sunday only\n\n\n\n\n\n\n\n\n\nTipWorkflow (phases)\n\n\n\nPhase 1 Audit → Phase 2 Contact-level features → Phase 3 Preprocess → Phase 4 Choose K → Phase 5 Fit/Interpret → then stability/ARI → sensitivity (feature blocks) → PCA viz → final recommendations.",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#phase-1-audit-call-level",
    "href": "walsoft_contact_segmentation.html#phase-1-audit-call-level",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "2 Phase 1 — Audit (call-level)",
    "text": "2 Phase 1 — Audit (call-level)\n\n2.1 Setup (imports + toggles)\n\n\n2.2 Load the dataset\n\n2.2.1 Load data\n\n\nCode\n# Load dataset (no assumptions, no output)\ndf = pd.read_csv(DATA_URL)\n\n\n\n\n2.2.2 Dataset overview\n\n\nCode\n# Dataset overview (executive-level structural metrics)\n\ndataset_overview = pd.DataFrame({\n    \"Metric\": [\n        \"Rows\",\n        \"Columns\",\n        \"Total cells\",\n        \"Memory usage (MB)\"\n    ],\n    \"Value\": [\n        int(df.shape[0]),\n        int(df.shape[1]),\n        int(df.shape[0] * df.shape[1]),\n        f\"{df.memory_usage(deep=True).sum() / 1e6:.2f}\"\n    ]\n})\n\ndataset_overview\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nRows\n24952\n\n\n1\nColumns\n9\n\n\n2\nTotal cells\n224568\n\n\n3\nMemory usage (MB)\n9.10\n\n\n\n\n\n\n\n\n\n2.2.3 Column inventory (schema inspection)\n\n\nCode\n# Column inventory (schema audit table)\n\ncolumn_inventory = (\n    pd.DataFrame({\n        \"Column name\": df.columns,\n        \"Data type\": df.dtypes.astype(str),\n        \"Non-null count\": df.notna().sum().values,\n        \"Non-null rate\": (df.notna().mean()).round(3).values\n    })\n    .sort_values(\"Column name\")\n    .reset_index(drop=True)\n)\n\ncolumn_inventory\n\n\n\n\n\n\n\n\n\nColumn name\nData type\nNon-null count\nNon-null rate\n\n\n\n\n0\ncategory\nstr\n24952\n1.0\n\n\n1\ndate_stamp\nstr\n24952\n1.0\n\n\n2\nday-of_week\nstr\n24952\n1.0\n\n\n3\ndialled_phone_number\nint64\n24952\n1.0\n\n\n4\nduration_in_seconds\nint64\n24952\n1.0\n\n\n5\nmonth\nstr\n24952\n1.0\n\n\n6\nname\nstr\n24952\n1.0\n\n\n7\ntime\nstr\n24952\n1.0\n\n\n8\nyear\nint64\n24952\n1.0\n\n\n\n\n\n\n\n\n\n2.2.4 Sample records (visual sanity check)\n\n\nCode\n# Sample records (human sanity check only)\ndf.head(3)\n\n\n\n\n\n\n\n\n\ndate_stamp\ntime\nday-of_week\nmonth\nyear\ndialled_phone_number\nname\nduration_in_seconds\ncategory\n\n\n\n\n0\n1/1/2022\n16:03:01\nSaturday\nJanuary\n2022\n648578192\nAbel\n179\nUnknown\n\n\n1\n1/1/2022\n16:06:17\nSaturday\nJanuary\n2022\n814500001\nHusband CEL01\n66\nFamily\n\n\n2\n1/1/2022\n19:08:44\nSaturday\nJanuary\n2022\n814500001\nHusband CEL01\n38\nFamily\n\n\n\n\n\n\n\n\n\n\n\n2.3 Identify key columns (robust, from the data itself)\n\n\nCode\n# Identify key columns (schema audit, no assumptions)\n\n\n# ------------------------------------------------------------------\n# Normalise column names once (single source of truth)\n# ------------------------------------------------------------------\ncol_map = {c: c.lower().strip() for c in df.columns}\n\n# ------------------------------------------------------------------\n# Detection rules (explicit, auditable, extensible)\n# ------------------------------------------------------------------\nDETECTION_RULES = OrderedDict({\n    \"Date\": {\n        \"description\": \"Calendar date component\",\n        \"match_any\": [\"date\"]\n    },\n    \"Time\": {\n        \"description\": \"Clock time component\",\n        \"match_exact_or_contains\": [\"time\"]\n    },\n    \"Duration\": {\n        \"description\": \"Call duration (numeric, any unit)\",\n        \"match_any\": [\"duration\", \"seconds\", \"secs\", \"sec\", \"minutes\", \"mins\"]\n    },\n    \"Identifier\": {\n        \"description\": \"Contact or phone identifier\",\n        \"match_any\": [\"dialled\", \"dialed\", \"phone\", \"number\", \"contact\"]\n    },\n    \"Category\": {\n        \"description\": \"Human or system call classification\",\n        \"match_any\": [\"category\"]\n    }\n})\n\n# ------------------------------------------------------------------\n# Apply detection rules\n# ------------------------------------------------------------------\nrecords = []\n\nfor field, rule in DETECTION_RULES.items():\n    detected = []\n\n    for col, lc in col_map.items():\n        if \"match_exact_or_contains\" in rule:\n            if any(lc == k or k in lc for k in rule[\"match_exact_or_contains\"]):\n                detected.append(col)\n        elif \"match_any\" in rule:\n            if any(k in lc for k in rule[\"match_any\"]):\n                detected.append(col)\n\n    records.append({\n        \"Field type\": field,\n        \"Purpose\": rule[\"description\"],\n        \"Detected columns\": \", \".join(detected) if detected else \"—\",\n        \"Count\": len(detected),\n        \"Status\": \"OK\" if detected else \"Missing\"\n    })\n\n# ------------------------------------------------------------------\n# Canonical schema audit table (Quarto renders automatically)\n# ------------------------------------------------------------------\nschema_audit = pd.DataFrame(records)\n\nschema_audit\n\n\n\n\n\n\n\n\n\nField type\nPurpose\nDetected columns\nCount\nStatus\n\n\n\n\n0\nDate\nCalendar date component\ndate_stamp\n1\nOK\n\n\n1\nTime\nClock time component\ntime\n1\nOK\n\n\n2\nDuration\nCall duration (numeric, any unit)\nduration_in_seconds\n1\nOK\n\n\n3\nIdentifier\nContact or phone identifier\ndialled_phone_number\n1\nOK\n\n\n4\nCategory\nHuman or system call classification\ncategory\n1\nOK\n\n\n\n\n\n\n\n\n\n2.4 Build canonical timestamp and numeric duration (audit-quality checks)\n\n2.4.1 Validate required schema\n\n\nCode\n# Validate required columns exist (hard audit gate)\n\nrequired_cols = [\n    \"date_stamp\",\n    \"time\",\n    \"duration_in_seconds\",\n    \"dialled_phone_number\"\n]\n\nmissing_required = [c for c in required_cols if c not in df.columns]\n\nschema_validation = pd.DataFrame({\n    \"Required column\": required_cols,\n    \"Present in dataset\": [c in df.columns for c in required_cols]\n})\n\nschema_validation\n\n\n\n\n\n\n\n\n\nRequired column\nPresent in dataset\n\n\n\n\n0\ndate_stamp\nTrue\n\n\n1\ntime\nTrue\n\n\n2\nduration_in_seconds\nTrue\n\n\n3\ndialled_phone_number\nTrue\n\n\n\n\n\n\n\n\n\nCode\n# Stop execution if schema is invalid\nassert len(missing_required) == 0, f\"Missing required columns: {missing_required}\"\n\n\n\n\n\n2.4.2 Build canonical fields\n\n\nCode\n# Canonical timestamp\ndf[\"call_ts\"] = pd.to_datetime(\n    df[\"date_stamp\"].astype(str).str.strip() + \" \" +\n    df[\"time\"].astype(str).str.strip(),\n    errors=\"coerce\"\n)\n\n# Canonical duration (seconds)\ndf[\"dur_sec\"] = pd.to_numeric(\n    df[\"duration_in_seconds\"],\n    errors=\"coerce\"\n)\n\n\n\n\n\n2.4.3 Data integrity audit\n\n\nCode\nbad_ts = int(df[\"call_ts\"].isna().sum())\nbad_dur = int(df[\"dur_sec\"].isna().sum())\nnegative_dur = int((df[\"dur_sec\"] &lt; 0).sum())\n\ntimestamp_min = df[\"call_ts\"].min()\ntimestamp_max = df[\"call_ts\"].max()\n\nintegrity_audit = pd.DataFrame({\n    \"Check\": [\n        \"Timestamp parse failures\",\n        \"Duration numeric failures\",\n        \"Negative durations\",\n        \"Timestamp range (min)\",\n        \"Timestamp range (max)\",\n        \"Timestamp success rate\",\n        \"Duration success rate\"\n    ],\n    \"Value\": [\n        bad_ts,\n        bad_dur,\n        negative_dur,\n        str(timestamp_min),\n        str(timestamp_max),\n        f\"{df['call_ts'].notna().mean():.3f}\",\n        f\"{df['dur_sec'].notna().mean():.3f}\"\n    ]\n})\n\nintegrity_audit\n\n\n\n\n\n\n\n\n\nCheck\nValue\n\n\n\n\n0\nTimestamp parse failures\n0\n\n\n1\nDuration numeric failures\n0\n\n\n2\nNegative durations\n0\n\n\n3\nTimestamp range (min)\n2022-01-01 16:03:01\n\n\n4\nTimestamp range (max)\n2024-10-04 20:15:16\n\n\n5\nTimestamp success rate\n1.000\n\n\n6\nDuration success rate\n1.000\n\n\n\n\n\n\n\n\n\n\n2.4.4 Hard integrity gates\n\n\nCode\nassert bad_ts == 0, \"Timestamp parsing failed for some rows.\"\nassert bad_dur == 0, \"Duration numeric conversion failed for some rows.\"\nassert negative_dur == 0, \"Negative durations detected.\"\n\n\n\n\n\n2.5 Hash a stable contact_id (do not export raw identifiers)\n\n\nCode\ndef stable_contact_id(x: int | str) -&gt; str:\n    s = str(x).encode(\"utf-8\")\n    return hashlib.sha256(s).hexdigest()[:12]\n\ndf[\"contact_id\"] = df[\"dialled_phone_number\"].map(stable_contact_id)\n\nprint(\"Unique contacts (hashed):\", int(df[\"contact_id\"].nunique()))\nassert df[\"contact_id\"].isna().sum() == 0\n\n\nUnique contacts (hashed): 2091\n\n\n\n\n2.6 Derive time-band + open/closed-day flags (hard constraints)\n\n\nCode\ndf[\"hour\"] = df[\"call_ts\"].dt.hour\ndf[\"dow\"] = df[\"call_ts\"].dt.dayofweek  # Mon=0..Sun=6\n\ndef assign_time_band(hour: int) -&gt; str:\n    if TIME_BANDS[\"early_morning\"][0] &lt;= hour &lt;= TIME_BANDS[\"early_morning\"][1]:\n        return \"early_morning\"\n    if TIME_BANDS[\"business_hours\"][0] &lt;= hour &lt;= TIME_BANDS[\"business_hours\"][1]:\n        return \"business_hours\"\n    if TIME_BANDS[\"evening\"][0] &lt;= hour &lt;= TIME_BANDS[\"evening\"][1]:\n        return \"evening\"\n    if TIME_BANDS[\"late_night\"][0] &lt;= hour &lt;= TIME_BANDS[\"late_night\"][1]:\n        return \"late_night\"\n    raise ValueError(f\"Hour out of range: {hour}\")\n\ndf[\"time_band\"] = df[\"hour\"].map(assign_time_band)\n\ndf[\"is_open_day\"] = df[\"dow\"].isin(OPEN_DOW).astype(int)\ndf[\"is_closed_day\"] = df[\"dow\"].isin(CLOSED_DOW).astype(int)\n\n# sanity checks: mutually exclusive and exhaustive\nassert set(df[\"time_band\"].unique()) == {\"early_morning\", \"business_hours\", \"evening\", \"late_night\"}\nassert int((df[\"is_open_day\"] + df[\"is_closed_day\"]).min()) == 1\nassert int((df[\"is_open_day\"] + df[\"is_closed_day\"]).max()) == 1\n\ndf[[\"call_ts\",\"hour\",\"dow\",\"time_band\",\"is_open_day\",\"is_closed_day\"]].head(5)\n\n\n\n\n\n\n\n\n\ncall_ts\nhour\ndow\ntime_band\nis_open_day\nis_closed_day\n\n\n\n\n0\n2022-01-01 16:03:01\n16\n5\nbusiness_hours\n1\n0\n\n\n1\n2022-01-01 16:06:17\n16\n5\nbusiness_hours\n1\n0\n\n\n2\n2022-01-01 19:08:44\n19\n5\nevening\n1\n0\n\n\n3\n2022-01-01 20:03:11\n20\n5\nevening\n1\n0\n\n\n4\n2022-01-02 14:22:44\n14\n6\nbusiness_hours\n0\n1\n\n\n\n\n\n\n\n\n\n2.7 Executive summary table (stakeholder-ready)\n\n\nCode\ncat_col = \"category\" if \"category\" in df.columns else None\n\nsummary = pd.DataFrame({\n    \"Metric\": [\n        \"Rows (calls)\",\n        \"Columns\",\n        \"Unique contacts (hashed)\",\n        \"Timestamp parsable rate\",\n        \"Timestamp range (min)\",\n        \"Timestamp range (max)\",\n        \"Duration numeric rate\",\n        \"Duplicate rows\",\n        \"Category column present?\",\n        \"Category non-null rate (if present)\",\n        \"Data readiness score (ts & dur)\"\n    ],\n    \"Value\": [\n        int(df.shape[0]),\n        int(df.shape[1]),\n        int(df[\"contact_id\"].nunique()),\n        f\"{df['call_ts'].notna().mean():.3f}\",\n        str(df[\"call_ts\"].min()),\n        str(df[\"call_ts\"].max()),\n        f\"{df['dur_sec'].notna().mean():.3f}\",\n        int(df.duplicated().sum()),\n        bool(cat_col is not None),\n        f\"{df[cat_col].notna().mean():.3f}\" if cat_col else \"N/A\",\n        f\"{(df['call_ts'].notna() & df['dur_sec'].notna()).mean():.3f}\",\n    ]\n})\nsummary\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nRows (calls)\n24952\n\n\n1\nColumns\n17\n\n\n2\nUnique contacts (hashed)\n2091\n\n\n3\nTimestamp parsable rate\n1.000\n\n\n4\nTimestamp range (min)\n2022-01-01 16:03:01\n\n\n5\nTimestamp range (max)\n2024-10-04 20:15:16\n\n\n6\nDuration numeric rate\n1.000\n\n\n7\nDuplicate rows\n0\n\n\n8\nCategory column present?\nTrue\n\n\n9\nCategory non-null rate (if present)\n1.000\n\n\n10\nData readiness score (ts & dur)\n1.000\n\n\n\n\n\n\n\n\n\n2.8 Visual 1 — Monthly call volume (coverage)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth_id\nn_calls\n\n\n\n\n22\n2023-11\n627\n\n\n23\n2023-12\n691\n\n\n24\n2024-01\n944\n\n\n25\n2024-02\n608\n\n\n26\n2024-03\n783\n\n\n27\n2024-04\n516\n\n\n28\n2024-05\n165\n\n\n29\n2024-06\n1034\n\n\n30\n2024-07\n1049\n\n\n31\n2024-08\n193\n\n\n32\n2024-09\n107\n\n\n33\n2024-10\n128\n\n\n\n\n\n\n\n\n\n2.9 Visual 2 — Duration distribution (outlier awareness)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nduration_seconds\n\n\n\n\n0.00\n1.00\n\n\n0.25\n9.00\n\n\n0.50\n34.00\n\n\n0.75\n84.00\n\n\n0.90\n211.00\n\n\n0.95\n396.00\n\n\n0.99\n1273.98\n\n\n1.00\n7200.00\n\n\n\n\n\n\n\n\n\n2.10 Visual 3 — Time-band mix + open/closed-day mix (hard-constraint diagnostics)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(                n_calls\n time_band              \n business_hours    20624\n early_morning      1071\n evening            2877\n late_night          380,\n                     n_calls\n is_closed_day              \n Open day (Mon–Sat)    23300\n Closed day (Sun)       1652)\n\n\n\n\n\n\n\n\n\nNotePhase 1 — Audit summary (stakeholder-ready)\n\n\n\nDataset health (ready for segmentation):\n\n24,952 call records across 2,091 unique contacts (hashed).\nTime coverage: 2022-01-01 → 2024-10-04 (timestamps parse perfectly; durations fully numeric).\nNo duplicates detected; category is fully populated.\nData readiness score = 1.000 (timestamp + duration fully usable).\n\nBehavioral signals visible already:\n\nCalls concentrate in business hours (20,624 / 24,952 ≈ 82.7%).\nSmaller but meaningful activity in evening (2,877 ≈ 11.5%) and early morning (1,071 ≈ 4.3%); late night is rare (380 ≈ 1.5%).\nMost calls happen on open days (Mon–Sat) (23,300 ≈ 93.4%); Sunday exists but is limited (1,652 ≈ 6.6%).\n\nDuration profile (important for feature design):\n\nTypical call is short: median 34s, 75th percentile 84s.\nHeavy tail: 99th percentile ~1,274s, max 7,200s (2 hours) → we will use robust statistics (median, IQR, log transforms, and long-call share) instead of relying only on means.\n\nCoverage caution (modeling hygiene):\n\nMonthly volume is broadly stable until mid-2024, then shows sharp drops in some late months.\nWe will treat this as coverage/behavior regime change and rely on contact-level aggregates + recency (not month-level trends) for clustering.\n\n\n2.11 Transition to Phase 2\nNext, we convert the call-level table into a contact-level feature table (1 row per contact).\nThis is the modeling dataset for clustering.",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#phase-2-contact-level-features",
    "href": "walsoft_contact_segmentation.html#phase-2-contact-level-features",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "3 Phase 2 — Contact-level features",
    "text": "3 Phase 2 — Contact-level features\nThis phase transforms the call-level dataset into a contact-level feature table, producing one row per contact. These features capture activity intensity, temporal patterns, and category distributions, forming the basis for clustering and behavioral segmentation.\n\n\n3.1 Guardrails & Setup\nWe first ensure all required columns from Phase 1 exist. This prevents accidental execution on incomplete data.\n\n\nCode\n# Guardrails: ensure critical columns exist\nassert \"contact_id\" in df.columns, \"contact_id missing (run Phase 1 first).\"\nassert \"call_ts\" in df.columns, \"call_ts missing (run Phase 1 first).\"\nassert \"dur_sec\" in df.columns, \"dur_sec missing (run Phase 1 first).\"\nassert \"time_band\" in df.columns, \"time_band missing (run Phase 1 first).\"\nassert \"is_open_day\" in df.columns and \"is_closed_day\" in df.columns, \"open/closed flags missing.\"\n\n\nHelper for safe division (avoids division by zero):\n\n\nCode\ndef safe_div(a, b):\n    return np.where(b == 0, 0.0, a / b)\n\n\nReference timestamp for recency calculations:\n\n\nCode\nASOF_TS = df[\"call_ts\"].max()\n\n\n\n\n\n3.2 Base Aggregations\nCompute core contact-level metrics: total calls, active days, duration statistics, and recency/tenure measures.\n\n\nCode\ng = df.groupby(\"contact_id\", as_index=False)\n\nbase = g.agg(\n    n_calls=(\"call_ts\", \"size\"),\n    n_days_active=(\"call_ts\", lambda x: x.dt.date.nunique()),\n    first_call_ts=(\"call_ts\", \"min\"),\n    last_call_ts=(\"call_ts\", \"max\"),\n    total_dur_sec=(\"dur_sec\", \"sum\"),\n    mean_dur_sec=(\"dur_sec\", \"mean\"),\n    median_dur_sec=(\"dur_sec\", \"median\"),\n    p90_dur_sec=(\"dur_sec\", lambda x: x.quantile(0.90)),\n)\n\n# Recency and tenure (days)\nbase[\"recency_days\"] = (ASOF_TS - base[\"last_call_ts\"]).dt.total_seconds() / (24 * 3600)\nbase[\"tenure_days\"] = (base[\"last_call_ts\"] - base[\"first_call_ts\"]).dt.total_seconds() / (24 * 3600)\n\n# Call rates\nbase[\"calls_per_active_day\"] = safe_div(base[\"n_calls\"], base[\"n_days_active\"])\nbase[\"calls_per_tenure_day\"] = safe_div(base[\"n_calls\"], (base[\"tenure_days\"] + 1.0))\n\n# Log transforms to stabilize heavy-tailed distributions\nbase[\"log_n_calls\"] = np.log1p(base[\"n_calls\"])\nbase[\"log_total_dur\"] = np.log1p(base[\"total_dur_sec\"])\nbase[\"log_median_dur\"] = np.log1p(base[\"median_dur_sec\"])\n\n\n\n\n\n3.3 Long-call Share\nDefine long calls as those exceeding the 95th percentile. Compute each contact’s share of long calls to capture tail behavior.\n\n\nCode\nLONG_CALL_THRESHOLD = float(df[\"dur_sec\"].quantile(0.95))\ndf[\"_is_long_call\"] = (df[\"dur_sec\"] &gt;= LONG_CALL_THRESHOLD).astype(int)\n\nlong_share = (\n    df.groupby(\"contact_id\", as_index=False)\n      .agg(long_call_share=(\"_is_long_call\", \"mean\"))\n)\n\n\n\n\n\n3.4 Time-Band Mix\nContacts may have different temporal activity patterns. We compute proportions of calls per time band (business hours, evening, late night, early morning).\n\n\nCode\nband_mix = (\n    pd.crosstab(df[\"contact_id\"], df[\"time_band\"], normalize=\"index\")\n      .reset_index()\n      .rename(columns={\n          \"business_hours\": \"share_business_hours\",\n          \"evening\": \"share_evening\",\n          \"late_night\": \"share_late_night\",\n          \"early_morning\": \"share_early_morning\",\n      })\n)\n\n# Ensure all expected columns exist\nfor col in [\"share_business_hours\",\"share_evening\",\"share_late_night\",\"share_early_morning\"]:\n    if col not in band_mix.columns:\n        band_mix[col] = 0.0\n\n\n\n\n\n3.5 Open vs Closed Day Mix\nCapture day-of-week activity patterns. Open vs. closed days are mutually exclusive and sum to 1 per contact.\n\n\nCode\nday_mix = (\n    df.groupby(\"contact_id\", as_index=False)\n      .agg(\n          share_open_day=(\"is_open_day\", \"mean\"),\n          share_closed_day=(\"is_closed_day\", \"mean\")\n      )\n)\n\n# Sanity check\nday_mix[\"_sum\"] = day_mix[\"share_open_day\"] + day_mix[\"share_closed_day\"]\nassert np.allclose(day_mix[\"_sum\"], 1.0, atol=1e-9), \"Open+Closed shares not summing to 1.\"\nday_mix = day_mix.drop(columns=[\"_sum\"])\n\n\n\n\n\n3.6 Category Mix (Optional)\nFor interpretation, compute the top-K category proportions per contact, preserving sparsity and numeric representation.\n\n\nCode\nTOPK = 8\ntop_categories = df[\"category\"].value_counts().head(TOPK).index.tolist()\n\ncat_tab = (\n    pd.crosstab(df[\"contact_id\"], df[\"category\"])\n      .reindex(columns=top_categories, fill_value=0)\n)\n\n# Convert to proportions\ncat_mix = (cat_tab.div(cat_tab.sum(axis=1).replace(0, 1), axis=0)\n                 .reset_index()\n                 .rename(columns={c: f\"share_cat_{c}\" for c in top_categories}))\n\n\n\n\n\n3.7 Merge All Features\nCombine base, long-call, time-band, day-mix, and category features into a single contact-level table.\n\n\nCode\ncontact_features = (\n    base.merge(long_share, on=\"contact_id\", how=\"left\")\n        .merge(band_mix, on=\"contact_id\", how=\"left\")\n        .merge(day_mix, on=\"contact_id\", how=\"left\")\n        .merge(cat_mix, on=\"contact_id\", how=\"left\")\n)\n\n# Fill any leftover NaNs\nnum_cols = contact_features.select_dtypes(include=[np.number]).columns\ncontact_features[num_cols] = contact_features[num_cols].fillna(0.0)\n\n# Preview\nprint(\"contact_features.shape =\", contact_features.shape)\nprint(\"Long-call threshold (95th pct, seconds) =\", LONG_CALL_THRESHOLD)\n\ncontact_features.head(5).T\n\n\ncontact_features.shape = (2091, 28)\nLong-call threshold (95th pct, seconds) = 396.0\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\ncontact_id\n00047e187745\n000d71073f3e\n001c36642ecb\n00ad9a0b1291\n00ed0d18f9e3\n\n\nn_calls\n1\n905\n6\n1\n2\n\n\nn_days_active\n1\n384\n4\n1\n2\n\n\nfirst_call_ts\n2024-03-22 17:01:05\n2022-01-03 08:26:21\n2022-07-24 20:33:20\n2023-06-01 15:28:13\n2023-12-08 19:38:04\n\n\nlast_call_ts\n2024-03-22 17:01:05\n2024-09-28 15:01:13\n2024-10-01 14:11:06\n2023-06-01 15:28:13\n2023-12-09 07:36:06\n\n\ntotal_dur_sec\n43\n44383\n564\n239\n232\n\n\nmean_dur_sec\n43.0\n49.041989\n94.0\n239.0\n116.0\n\n\nmedian_dur_sec\n43.0\n22.0\n59.0\n239.0\n116.0\n\n\np90_dur_sec\n43.0\n99.0\n201.0\n239.0\n139.2\n\n\nrecency_days\n196.13485\n6.21809\n3.252894\n491.19934\n300.527199\n\n\ntenure_days\n0.0\n999.274213\n799.73456\n0.0\n0.498634\n\n\ncalls_per_active_day\n1.0\n2.356771\n1.5\n1.0\n1.0\n\n\ncalls_per_tenure_day\n1.0\n0.904752\n0.007493\n1.0\n1.334548\n\n\nlog_n_calls\n0.693147\n6.809039\n1.94591\n0.693147\n1.098612\n\n\nlog_total_dur\n3.78419\n10.700634\n6.336826\n5.480639\n5.451038\n\n\nlog_median_dur\n3.78419\n3.135494\n4.094345\n5.480639\n4.762174\n\n\nlong_call_share\n0.0\n0.009945\n0.0\n0.0\n0.0\n\n\nshare_business_hours\n1.0\n0.923757\n0.833333\n1.0\n0.0\n\n\nshare_early_morning\n0.0\n0.028729\n0.0\n0.0\n0.5\n\n\nshare_evening\n0.0\n0.047514\n0.166667\n0.0\n0.5\n\n\nshare_late_night\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nshare_open_day\n1.0\n0.967956\n0.666667\n1.0\n1.0\n\n\nshare_closed_day\n0.0\n0.032044\n0.333333\n0.0\n0.0\n\n\nshare_cat_Unknown\n1.0\n1.0\n1.0\n0.0\n1.0\n\n\nshare_cat_Family\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nshare_cat_Supplier\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\nshare_cat_Important Contacts\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nshare_cat_Service Provider\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n\n3.8 Quick Feature Audit\nSanity check feature completeness and distribution.\n\n\nCode\nfeature_audit = pd.DataFrame({\n    \"n_contacts\": [int(contact_features.shape[0])],\n    \"n_features_total\": [int(contact_features.shape[1])],\n    \"any_missing_numeric\": [bool(contact_features.select_dtypes(include=[np.number]).isna().any().any())],\n    \"min_calls\": [int(contact_features[\"n_calls\"].min())],\n    \"median_calls\": [float(contact_features[\"n_calls\"].median())],\n    \"max_calls\": [int(contact_features[\"n_calls\"].max())],\n    \"median_recency_days\": [float(contact_features[\"recency_days\"].median())],\n})\nfeature_audit\n\n\n\n\n\n\n\n\n\nn_contacts\nn_features_total\nany_missing_numeric\nmin_calls\nmedian_calls\nmax_calls\nmedian_recency_days\n\n\n\n\n0\n2091\n28\nFalse\n1\n2.0\n2413\n445.260324\n\n\n\n\n\n\n\n\n\n\n3.9 Top Activity Contacts\nInspect most active contacts without exposing raw identifiers. This helps validate engineered features and distribution of behavioral metrics.\n\n\nCode\ntop_activity = contact_features.sort_values(\"n_calls\", ascending=False).head(10)[\n    [\"contact_id\",\"n_calls\",\"n_days_active\",\"total_dur_sec\",\"median_dur_sec\",\"recency_days\",\n     \"share_business_hours\",\"share_evening\",\"share_early_morning\",\"share_late_night\",\n     \"share_open_day\",\"share_closed_day\",\"long_call_share\"]\n]\ntop_activity.T\n\n\n\n\n\n\n\n\n\n900\n1026\n572\n1103\n1959\n1\n791\n1885\n675\n1147\n\n\n\n\ncontact_id\n70ac660de5a1\n8080cfddc014\n497ce042ecb8\n88d17d0474e5\nf2f949ce03e7\n000d71073f3e\n632b8c0f82f4\neb0aeed50ff6\n550412a925c4\n8d6e4e8282d5\n\n\nn_calls\n2413\n1473\n1272\n1077\n917\n905\n756\n477\n421\n369\n\n\nn_days_active\n606\n592\n433\n447\n398\n384\n307\n97\n180\n186\n\n\ntotal_dur_sec\n120519\n192181\n74136\n222060\n186933\n44383\n77638\n25168\n23209\n102636\n\n\nmedian_dur_sec\n28.0\n48.0\n12.0\n76.0\n32.0\n22.0\n32.0\n36.0\n40.0\n75.0\n\n\nrecency_days\n59.330243\n4.232801\n0.029317\n0.124444\n158.600069\n6.21809\n1.081655\n59.142512\n2.317778\n63.253345\n\n\nshare_business_hours\n0.822213\n0.680244\n0.672956\n0.733519\n0.72301\n0.923757\n0.727513\n0.953878\n0.973872\n0.731707\n\n\nshare_evening\n0.116453\n0.170401\n0.279088\n0.163417\n0.202835\n0.047514\n0.187831\n0.033543\n0.021378\n0.186992\n\n\nshare_early_morning\n0.033154\n0.127631\n0.038522\n0.077994\n0.061069\n0.028729\n0.064815\n0.0\n0.002375\n0.04878\n\n\nshare_late_night\n0.028181\n0.021724\n0.009434\n0.02507\n0.013086\n0.0\n0.019841\n0.012579\n0.002375\n0.03252\n\n\nshare_open_day\n0.930792\n0.901561\n0.956761\n0.889508\n0.876772\n0.967956\n0.915344\n0.960168\n0.980998\n0.864499\n\n\nshare_closed_day\n0.069208\n0.098439\n0.043239\n0.110492\n0.123228\n0.032044\n0.084656\n0.039832\n0.019002\n0.135501\n\n\nlong_call_share\n0.008288\n0.076035\n0.017296\n0.138347\n0.139586\n0.009945\n0.047619\n0.002096\n0.004751\n0.195122\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePhase 2 — Stakeholder-ready summary (what we learned)\n\n\n\nWe successfully converted 24,952 calls into a contact-level modeling table with 2,091 contacts and 28 features.\n\n3.10 What the Phase 2 feature table represents (business meaning)\nEach row is a contact (hashed ID), and the features capture:\n\nVolume / intensity: n_calls, n_days_active, calls_per_active_day, calls_per_tenure_day\nRelationship depth / effort proxy: total_dur_sec, median_dur_sec, p90_dur_sec, plus log versions\nRecency / dormancy risk: recency_days (how long since last call)\nTime preference / accessibility: shares by time band (business/evening/early/late)\nOpen vs closed day behavior: share_open_day, share_closed_day\nTail behavior: long_call_share where “long” is ≥ 396s (95th percentile)\nCategory mix (interpretation aid): proportions of top-8 categories per contact\n\n\n\n3.11 Sanity checks passed (safe to proceed)\n\nNo missing numeric values in engineered features.\nOpen + closed day shares sum to 1 per contact (validated).\nTime-band shares exist for every contact (columns ensured even if 0).\n\n\n\n3.12 What the results already hint at (without clustering yet)\n\nHighly skewed activity: median contact has 2 calls, but the top contact has 2,413 calls → we must scale/transform before K-means.\nRecency is large for many contacts: median recency_days ≈ 445 → many contacts are dormant; segmentation must separate “active” vs “inactive”.\nTop activity contacts show diverse patterns:\n\nsome are high-volume but mostly business hours\nsome show meaningful evening/early activity\nsome have high long_call_share (deep conversations or issue resolution)\n\n\n\n\n3.13 Modeling implication\nAt this point we have a contact-level feature table with strong business meaning (volume, duration, recency, timing).\nBefore clustering, we must build a clean clustering matrix \\(X\\) so distance-based algorithms behave correctly.\nWe will build \\(X\\) such that it:\n\nExcludes non-model columns\n\nNo timestamps (first_call_ts, last_call_ts) and no raw identifiers.\n\nCategory mix is kept for interpretation, but excluded from default training to avoid “baking in labels”.\n\nControls heavy tails and near-constant features\n\nPhone-call behavior is highly skewed (a few contacts dominate calls/duration).\n\nWe apply robust transforms already engineered (log features) and use a robust preprocessing pipeline.\n\nWe also drop near-constant (IQR≈0) features to avoid unstable scaling and distance domination.\n\nScales fairly for distance-based clustering\n\nWe use median imputation (robust, future-proof if new features introduce missingness).\n\nWe use RobustScaler (median/IQR) instead of StandardScaler to reduce outlier influence.\n\n\n\n\n\n3.14 Transition\nPhase 3 converts contact_features into clustering-ready inputs and produces:\n\na feature-block dictionary for later sensitivity tests (volume vs duration vs timing vs recency),\na cleaned, scaled matrix X and aligned feature_names for reproducibility and interpretation.",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#phase-3-preprocess-bullet-proof-distance-based-clustering-ready",
    "href": "walsoft_contact_segmentation.html#phase-3-preprocess-bullet-proof-distance-based-clustering-ready",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "4 Phase 3 — Preprocess (bullet-proof, distance-based clustering ready)",
    "text": "4 Phase 3 — Preprocess (bullet-proof, distance-based clustering ready)\nThis phase prepares the contact-level feature table for distance-based clustering by producing a stable numeric matrix X (rows = contacts, columns = standardized features).\nWe do four things:\n\nDefine feature blocks (auditable and reusable for sensitivity tests later)\n\nBuild the default model matrix (exclude timestamps and category mix by default)\n\nStabilize heavy tails (clip extreme values in raw space)\n\nImpute + robust-scale (median imputation + RobustScaler) with diagnostics\n\n\n\n\n\n\n\nTipWhy RobustScaler (not StandardScaler)\n\n\n\nContact behaviour is heavy-tailed (a few contacts dominate activity).\nDistance-based clustering is scale-sensitive. RobustScaler uses median and IQR, reducing outlier influence and producing more stable distances.\n\n\n\n\n4.1 Freeze an interpretation copy (read-only view)\n\n\nCode\ncontact_features_view = contact_features.copy()\n\n\n\n\n\n4.2 Define feature blocks (for interpretation and sensitivity analysis)\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n\n\n\n\nCode\n# -----------------------------\n# Feature blocks (business meaning)\n# -----------------------------\nvolume_features = [\n    \"n_calls\", \"n_days_active\", \"calls_per_active_day\", \"calls_per_tenure_day\", \"log_n_calls\"\n]\n\nduration_features = [\n    \"total_dur_sec\", \"mean_dur_sec\", \"median_dur_sec\", \"p90_dur_sec\",\n    \"log_total_dur\", \"log_median_dur\", \"long_call_share\"\n]\n\nrecency_features = [\n    \"recency_days\", \"tenure_days\"\n]\n\ntiming_features = [\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\n# Category mix is useful for interpretation, but excluded from default model training\ncategory_features = [c for c in contact_features.columns if c.startswith(\"share_cat_\")]\n\nFEATURE_BLOCKS = {\n    \"volume\": volume_features,\n    \"duration\": duration_features,\n    \"recency\": recency_features,\n    \"timing\": timing_features,\n    \"category\": category_features\n}\n\n# Validate engineered feature presence (hard gate)\nmissing = {k: [f for f in v if f not in contact_features.columns] for k, v in FEATURE_BLOCKS.items()}\nmissing = {k: v for k, v in missing.items() if len(v) &gt; 0}\nassert len(missing) == 0, f\"Missing engineered features: {missing}\"\n\nFEATURE_BLOCKS\n\n\n{'volume': ['n_calls',\n  'n_days_active',\n  'calls_per_active_day',\n  'calls_per_tenure_day',\n  'log_n_calls'],\n 'duration': ['total_dur_sec',\n  'mean_dur_sec',\n  'median_dur_sec',\n  'p90_dur_sec',\n  'log_total_dur',\n  'log_median_dur',\n  'long_call_share'],\n 'recency': ['recency_days', 'tenure_days'],\n 'timing': ['share_business_hours',\n  'share_evening',\n  'share_early_morning',\n  'share_late_night',\n  'share_open_day',\n  'share_closed_day'],\n 'category': ['share_cat_Unknown',\n  'share_cat_Family',\n  'share_cat_Supplier',\n  'share_cat_Important Contacts',\n  'share_cat_Service Provider']}\n\n\n\n\n\n4.3 Select default model features (no timestamps, no category mix by default)\nWe train clustering on behaviour (volume, duration, recency, timing). Category mix stays available for interpretation and later sensitivity checks.\n\n\nCode\nMODEL_FEATURES = (\n    FEATURE_BLOCKS[\"volume\"]\n    + FEATURE_BLOCKS[\"duration\"]\n    + FEATURE_BLOCKS[\"recency\"]\n    + FEATURE_BLOCKS[\"timing\"]\n)\n\nX_raw = contact_features[MODEL_FEATURES].copy()\nprint(\"X_raw.shape =\", X_raw.shape)\n\n\nX_raw.shape = (2091, 20)\n\n\n\n\n\n4.4 Diagnostics before preprocessing (missingness + inf safety)\n\n\nCode\n# Missingness inspection (future-proof if new features introduce NaNs)\nmiss = X_raw.isna().sum().sort_values(ascending=False)\nmiss_table = miss[miss &gt; 0].to_frame(\"n_missing\")\nmiss_table\n\n\n\n\n\n\n\n\n\nn_missing\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Inf / -Inf inspection (must not exist)\nhas_inf = np.isinf(X_raw.to_numpy(dtype=float)).any()\nprint(\"Any inf in X_raw:\", bool(has_inf))\nassert not has_inf, \"Infinite values detected in X_raw (must fix before scaling).\"\n\n\nAny inf in X_raw: False\n\n\n\n\n\n4.5 Stability strategy (recommended): drop only truly constant columns + clip extremes\nImportant correction: Dropping “low IQR” features is too aggressive in contact data (many contacts have 1–2 calls, which makes shares look constant). Instead we:\n\ndrop only truly constant columns (no information),\nclip each feature to p01–p99 in raw units (winsorization),\nthen apply RobustScaler.\n\n\n\nCode\n# A) Drop truly constant columns only (no information)\nnunique = X_raw.nunique(dropna=False)\nconstant_cols = nunique[nunique &lt;= 1].index.tolist()\nprint(\"Truly-constant cols (drop):\", constant_cols)\n\nX_raw2 = X_raw.drop(columns=constant_cols) if constant_cols else X_raw.copy()\n\n# B) Clip extremes in RAW space to stabilize heavy tails\nCLIP_LO = 0.01\nCLIP_HI = 0.99\n\nclip_info = []\nX_clip = X_raw2.copy()\n\nfor col in X_clip.columns:\n    lo = float(X_clip[col].quantile(CLIP_LO))\n    hi = float(X_clip[col].quantile(CLIP_HI))\n    X_clip[col] = X_clip[col].clip(lower=lo, upper=hi)\n    clip_info.append({\"feature\": col, \"p01\": lo, \"p99\": hi})\n\nclip_table = pd.DataFrame(clip_info).sort_values(\"feature\").reset_index(drop=True)\n\nX_clip.shape, clip_table.head(10)\n\n\nTruly-constant cols (drop): []\n\n\n((2091, 20),\n                 feature       p01         p99\n 0  calls_per_active_day  1.000000    5.000000\n 1  calls_per_tenure_day  0.005871    4.028542\n 2        log_median_dur  1.098612    6.634896\n 3           log_n_calls  0.693147    5.153539\n 4         log_total_dur  1.098612    9.842541\n 5       long_call_share  0.000000    1.000000\n 6          mean_dur_sec  2.000000  771.900000\n 7        median_dur_sec  2.000000  760.200000\n 8               n_calls  1.000000  172.300000\n 9         n_days_active  1.000000   96.100000)\n\n\n\n\n\n4.6 Median imputation + RobustScaler\n\n\nCode\n# Median imputation is robust (and future-proof for features that may introduce NaNs later)\nimputer = SimpleImputer(strategy=\"median\")\n\n# RobustScaler uses median/IQR to reduce outlier influence\nscaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n\nX_imputed = imputer.fit_transform(X_clip)\nX = scaler.fit_transform(X_imputed)\n\nprint(\"X.shape =\", X.shape)\nprint(\"Any NaN in X:\", bool(np.isnan(X).any()))\nassert not np.isnan(X).any(), \"NaNs remain after preprocessing (must fix).\"\n\nprint(\"max_abs_after_scaling =\", float(np.max(np.abs(X))))\n\n\nX.shape = (2091, 20)\nAny NaN in X: False\nmax_abs_after_scaling = 95.09999999999991\n\n\n\n\n\n4.7 Diagnostics: identify any dominating feature in scaled space\nIf one feature still dominates distances, we detect it explicitly.\n\n\nCode\nfeature_names = list(X_clip.columns)\nmax_abs_by_feature = np.max(np.abs(X), axis=0)\n\ndominance = (\n    pd.DataFrame({\n        \"feature\": feature_names,\n        \"max_abs_scaled\": max_abs_by_feature,\n        \"raw_p01\": clip_table.set_index(\"feature\").loc[feature_names, \"p01\"].values,\n        \"raw_p99\": clip_table.set_index(\"feature\").loc[feature_names, \"p99\"].values,\n        \"raw_iqr\": (X_raw2.quantile(0.75) - X_raw2.quantile(0.25)).reindex(feature_names).values,\n        \"n_unique\": X_raw2.nunique().reindex(feature_names).values\n    })\n    .sort_values(\"max_abs_scaled\", ascending=False)\n    .reset_index(drop=True)\n)\n\ndominance.head(12)\n\n\n\n\n\n\n\n\n\nfeature\nmax_abs_scaled\nraw_p01\nraw_p99\nraw_iqr\nn_unique\n\n\n\n\n0\nn_days_active\n95.100000\n1.000000\n96.100000\n1.000000\n80\n\n\n1\ntotal_dur_sec\n78.405858\n2.000000\n18830.000000\n239.000000\n706\n\n\n2\nn_calls\n56.766667\n1.000000\n172.300000\n3.000000\n108\n\n\n3\ntenure_days\n25.858679\n0.000000\n941.093146\n36.393634\n1108\n\n\n4\np90_dur_sec\n13.405819\n2.000000\n1375.100000\n97.950000\n1070\n\n\n5\nmedian_dur_sec\n11.060536\n2.000000\n760.200000\n65.250000\n424\n\n\n6\nmean_dur_sec\n10.365775\n2.000000\n771.900000\n70.125000\n920\n\n\n7\nlog_n_calls\n4.425372\n0.693147\n5.153539\n0.916291\n108\n\n\n8\ncalls_per_active_day\n4.000000\n1.000000\n5.000000\n1.000000\n170\n\n\n9\ncalls_per_tenure_day\n3.649338\n0.005871\n4.028542\n0.829888\n1116\n\n\n10\nlog_total_dur\n2.554233\n1.098612\n9.842541\n2.083111\n706\n\n\n11\nlog_median_dur\n1.706673\n1.098612\n6.634896\n1.733545\n424\n\n\n\n\n\n\n\n\n\n\n4.8 Preprocess audit (compact, decision-useful)\n\n\nCode\npreprocess_audit = pd.DataFrame({\n    \"n_contacts\": [int(X.shape[0])],\n    \"n_model_features\": [int(X.shape[1])],\n    \"n_constant_dropped\": [int(len(constant_cols))],\n    \"constant_dropped\": [\", \".join(constant_cols) if constant_cols else \"None\"],\n    \"clip_low_quantile\": [CLIP_LO],\n    \"clip_high_quantile\": [CLIP_HI],\n    \"n_features_with_missing_before\": [int((X_raw.isna().sum() &gt; 0).sum())],\n    \"total_missing_cells_before\": [int(X_raw.isna().sum().sum())],\n    \"max_abs_after_scaling\": [float(np.max(np.abs(X)))],\n})\n\npreprocess_audit\n\n\n\n\n\n\n\n\n\nn_contacts\nn_model_features\nn_constant_dropped\nconstant_dropped\nclip_low_quantile\nclip_high_quantile\nn_features_with_missing_before\ntotal_missing_cells_before\nmax_abs_after_scaling\n\n\n\n\n0\n2091\n20\n0\nNone\n0.01\n0.99\n0\n0\n95.1\n\n\n\n\n\n\n\n\n\n\n4.9 Store preprocessing artifacts (in-memory, safe by default)\n\n\nCode\nprep = {\n    \"MODEL_FEATURES\": MODEL_FEATURES,\n    \"FEATURE_BLOCKS\": FEATURE_BLOCKS,\n    \"feature_names\": feature_names,\n    \"imputer\": imputer,\n    \"scaler\": scaler,\n    \"X_raw\": X_raw2,      # before clipping (audit)\n    \"X_clip\": X_clip,     # clipped raw matrix (audit)\n    \"X\": X,               # final clustering matrix\n    \"clip_table\": clip_table,\n    \"dominance\": dominance,\n    \"preprocess_audit\": preprocess_audit\n}\n\n# Raw-unit summary (post-clip = what the scaler actually sees)\nraw_summary = X_clip.describe(percentiles=[0.5, 0.9, 0.95, 0.99]).T\nraw_summary = raw_summary[[\"count\", \"mean\", \"std\", \"min\", \"50%\", \"90%\", \"95%\", \"99%\", \"max\"]]\nraw_summary.head(12)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n50%\n90%\n95%\n99%\nmax\n\n\n\n\nn_calls\n2091.0\n7.560641\n22.535264\n1.000000\n2.000000\n12.000000\n28.000000\n169.330000\n172.300000\n\n\nn_days_active\n2091.0\n4.503156\n12.474802\n1.000000\n1.000000\n7.000000\n17.000000\n96.010000\n96.100000\n\n\ncalls_per_active_day\n2091.0\n1.490754\n0.784284\n1.000000\n1.000000\n2.333333\n3.000000\n5.000000\n5.000000\n\n\ncalls_per_tenure_day\n2091.0\n0.987043\n0.853444\n0.005871\n1.000000\n1.998381\n2.894101\n4.005407\n4.028542\n\n\nlog_n_calls\n2091.0\n1.337442\n0.932739\n0.693147\n1.098612\n2.564949\n3.367296\n5.137573\n5.153539\n\n\ntotal_dur_sec\n2091.0\n711.445720\n2477.954937\n2.000000\n91.000000\n1107.000000\n3019.500000\n18608.600000\n18830.000000\n\n\nmean_dur_sec\n2091.0\n84.096441\n126.033557\n2.000000\n45.000000\n184.000000\n310.800000\n767.490000\n771.900000\n\n\nmedian_dur_sec\n2091.0\n72.592396\n115.864389\n2.000000\n38.500000\n153.500000\n276.500000\n760.020000\n760.200000\n\n\np90_dur_sec\n2091.0\n126.747585\n215.263200\n2.000000\n62.000000\n268.600000\n494.400000\n1375.010000\n1375.100000\n\n\nlog_total_dur\n2091.0\n4.617053\n1.825819\n1.098612\n4.521789\n7.010312\n8.013169\n9.831353\n9.842541\n\n\nlog_median_dur\n2091.0\n3.546764\n1.254063\n1.098612\n3.676301\n5.040194\n5.625780\n6.634660\n6.634896\n\n\nlong_call_share\n2091.0\n0.038448\n0.156275\n0.000000\n0.000000\n0.008288\n0.285714\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n4.10 A Final numeric stability guardrail\nEven after robust scaling, distance-based clustering can still be dominated by a small number of extreme points.\nTo make the pipeline bullet-proof, we apply a final cap in standardized space so no single feature can overwhelm Euclidean distances.\n\n\nCode\n# Final stability cap in scaled space (portfolio-grade guardrail)\nSCALED_CAP = 10.0\n\nX_uncapped = prep[\"X\"]\nX_capped = np.clip(X_uncapped, -SCALED_CAP, SCALED_CAP)\n\ncap_audit = pd.DataFrame({\n    \"scaled_cap\": [SCALED_CAP],\n    \"max_abs_before\": [float(np.max(np.abs(X_uncapped)))],\n    \"max_abs_after\": [float(np.max(np.abs(X_capped)))],\n})\n\n# Update the matrix used downstream (keep uncapped for audit)\nprep[\"X_uncapped\"] = X_uncapped\nprep[\"X\"] = X_capped\ncap_audit\n\n\n\n\n\n\n\n\n\nscaled_cap\nmax_abs_before\nmax_abs_after\n\n\n\n\n0\n10.0\n95.1\n10.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePhase 3 — Sign-off (ready for K-selection)\n\n\n\nWe produced a stable clustering matrix prep[\"X\"] for 2,091 contacts × 20 features.\nRobustness guarantees: - No missing values or infinities - Heavy tails controlled by p01–p99 clipping - Distance stability guaranteed by post-scaling cap (max absolute value ≤ 10)\nNext: Phase 4 chooses the number of clusters \\(K\\) using a multi-metric selection protocol (inertia, silhouette, Davies–Bouldin) on prep[\"X\"].",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#phase-4-choose-k-model-selection-for-clustering",
    "href": "walsoft_contact_segmentation.html#phase-4-choose-k-model-selection-for-clustering",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "5 Phase 4 — Choose \\(K\\) (model selection for clustering)",
    "text": "5 Phase 4 — Choose \\(K\\) (model selection for clustering)\nChoosing \\(K\\) is a model selection problem: different values of \\(K\\) define different segmentations. We evaluate candidate \\(K\\) values using three complementary criteria:\n\nInertia (SSE): decreases with \\(K\\); we look for an “elbow” (diminishing returns).\nSilhouette: higher is better; measures separation vs cohesion.\nDavies–Bouldin: lower is better; penalizes overlapping clusters.\n\n\n\n\n\n\n\nImportant\n\n\n\nWe compute metrics on the preprocessed matrix prep[\"X\"] (robust-scaled and capped).\nThis ensures the \\(K\\) decision reflects behavioural structure rather than raw-unit dominance.\n\n\n\n\n5.1 Compute \\(K\\)-search metrics\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\n\nX = prep[\"X\"]  # capped, clustering-ready matrix\n\n# Candidate K range (employer-ready default)\nK_MIN, K_MAX = 2, 12\n\nrows = []\nfor k in range(K_MIN, K_MAX + 1):\n    km = KMeans(\n        n_clusters=k,\n        n_init=50,         # more restarts = more reliable\n        random_state=42\n    )\n    labels = km.fit_predict(X)\n\n    inertia = float(km.inertia_)\n    sil = float(silhouette_score(X, labels))\n    db = float(davies_bouldin_score(X, labels))\n\n    rows.append({\n        \"k\": k,\n        \"inertia\": inertia,\n        \"silhouette\": sil,\n        \"davies_bouldin\": db\n    })\n\nk_metrics = pd.DataFrame(rows)\n\n# Add a simple \"elbow help\": marginal gain in inertia\nk_metrics[\"inertia_drop\"] = k_metrics[\"inertia\"].shift(1) - k_metrics[\"inertia\"]\nk_metrics[\"inertia_drop_pct\"] = (k_metrics[\"inertia_drop\"] / k_metrics[\"inertia\"].shift(1)) * 100\n\nk_metrics\n\n\n\n\n\n\n\n\n\nk\ninertia\nsilhouette\ndavies_bouldin\ninertia_drop\ninertia_drop_pct\n\n\n\n\n0\n2\n50974.583857\n0.651267\n0.746627\nNaN\nNaN\n\n\n1\n3\n37194.004774\n0.646105\n0.828924\n13780.579083\n27.034216\n\n\n2\n4\n26086.981574\n0.589088\n0.771859\n11107.023200\n29.862402\n\n\n3\n5\n22856.984343\n0.482657\n0.926567\n3229.997231\n12.381644\n\n\n4\n6\n20323.525176\n0.457261\n0.999610\n2533.459166\n11.083961\n\n\n5\n7\n18523.891058\n0.430309\n1.050928\n1799.634118\n8.854931\n\n\n6\n8\n16933.390393\n0.396590\n1.021011\n1590.500665\n8.586213\n\n\n7\n9\n15403.105156\n0.375016\n1.019315\n1530.285237\n9.037087\n\n\n8\n10\n14055.290862\n0.320327\n0.997488\n1347.814294\n8.750277\n\n\n9\n11\n13118.789180\n0.319340\n1.022192\n936.501682\n6.662983\n\n\n10\n12\n12285.369653\n0.321039\n1.040977\n833.419528\n6.352869\n\n\n\n\n\n\n\n\n\n\n5.2 Visualize metrics (elbow + quality trade-offs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3 Candidate shortlist (data-driven)\nWe shortlist \\(K\\) values that are plausible trade-offs:\n\namong the top silhouette scores,\namong the lowest Davies–Bouldin scores,\nand not far beyond the elbow (where inertia gains flatten).\n\n\n\nCode\n# ----------------------------------------\n# Candidate shortlist — modern, auditable\n# ----------------------------------------\n\n# Identify top-K by each metric\ntop_sil = k_metrics.nlargest(5, \"silhouette\").copy()\ntop_sil[\"reason\"] = \"top_silhouette\"\n\nlow_db = k_metrics.nsmallest(5, \"davies_bouldin\").copy()\nlow_db[\"reason\"] = \"low_davies_bouldin\"\n\n# Combine and mark duplicates\nshortlist = pd.concat([top_sil, low_db]).reset_index(drop=True)\n\n# If a K appears in both, update reason\nshortlist = (\n    shortlist.groupby(\"k\", as_index=False)\n    .agg({\n        \"silhouette\": \"first\",\n        \"davies_bouldin\": \"first\",\n        \"inertia\": \"first\",\n        \"reason\": lambda x: \" & \".join(sorted(set(x)))\n    })\n    .sort_values(\"k\")\n    .reset_index(drop=True)\n)\n\n# Round metrics for readability\nshortlist[[\"silhouette\", \"davies_bouldin\", \"inertia\"]] = shortlist[\n    [\"silhouette\", \"davies_bouldin\", \"inertia\"]\n].round(4)\n\nshortlist\n\n\n\n\n\n\n\n\n\nk\nsilhouette\ndavies_bouldin\ninertia\nreason\n\n\n\n\n0\n2\n0.6513\n0.7466\n50974.5839\nlow_davies_bouldin & top_silhouette\n\n\n1\n3\n0.6461\n0.8289\n37194.0048\nlow_davies_bouldin & top_silhouette\n\n\n2\n4\n0.5891\n0.7719\n26086.9816\nlow_davies_bouldin & top_silhouette\n\n\n3\n5\n0.4827\n0.9266\n22856.9843\nlow_davies_bouldin & top_silhouette\n\n\n4\n6\n0.4573\n0.9996\n20323.5252\ntop_silhouette\n\n\n5\n10\n0.3203\n0.9975\n14055.2909\nlow_davies_bouldin\n\n\n\n\n\n\n\n\n\n\n5.4 What the metrics say (from first principles)\n\n5.4.1 Inertia (elbow)\nHuge drops from K=2 → 3 (~27%) and 3 → 4 (~30%).\nAfter K=4, the drop collapses to ~12% and then &lt;~11% thereafter.\nThat’s a classic elbow at K≈4.\n\n\n5.4.2 Silhouette (separation vs cohesion)\n\nK=2: 0.651 (very strong)\n\nK=3: 0.646 (very strong)\n\nK=4: 0.589 (still strong)\n\nThen it falls sharply (e.g., K=5 = 0.483 and continues down).\nStructure is strongest for small K, especially 2–4.\n\n\n5.4.3 Davies–Bouldin (overlap; lower is better)\n\nBest is K=2 (0.747), then K=4 (0.772), then K=3 (0.829).\nAfter that, the metric worsens notably.\n\n\n\n\n\n5.5 Decision\nChoose K=4 (recommended).\n\nK=2: too coarse for actionability (“low vs high activity”)\n\nK=3: better but still merges distinct behaviours (“steady frequent” vs “bursty intense”)\n\nK=4: lands at the elbow, with strong silhouette & Davies–Bouldin — best balance of interpretability, actionability, and metric support.\n\n\n\n\n\n\n\nImportantDecision: choose \\(K=4\\)\n\n\n\nWe select \\(K=4\\) as the operational segmentation because it balances:\n\nStructure quality: strong silhouette (0.589) and low Davies–Bouldin (0.772)\n\nParsimony: clear elbow around K=4 (large inertia drops up to 4, then diminishing returns)\n\nActionability: more useful than K=2 while avoiding over-fragmentation at higher K\n\nNext: fit K-means with K=4 and interpret clusters using feature-block summaries (volume, duration, recency, timing).\n\n\n\n\n\n5.6 Optional: why we may explore \\(K=5\\) (exploratory only)\n\n\n\n\n\n\nWarning\n\n\n\n\\(K=5\\) is not recommended as the default operational segmentation in this project.\nCompared to \\(K=4\\), separation quality declines noticeably:\n\nSilhouette: 0.589 → 0.483 (weaker cohesion/separation)\nDavies–Bouldin: 0.772 → 0.927 (more overlap between clusters)\n\nWe may still explore \\(K=5\\) only as an exploratory lens when stakeholders want finer distinctions inside high-activity groups (e.g., separating “steady frequent” from “burst-intense”). Any \\(K=5\\) results should be presented as additional insight without changing the primary framework.\n\n\n\nStakeholder takeaway: K=4 remains the primary segmentation. K=5 can be used selectively to provide additional insights where fine distinctions are important, without altering the main operational framework.",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#phase-5-fit-and-interpret-clusters-k-means-k4",
    "href": "walsoft_contact_segmentation.html#phase-5-fit-and-interpret-clusters-k-means-k4",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "6 Phase 5 — Fit and interpret clusters (K-means, \\(K=4\\))",
    "text": "6 Phase 5 — Fit and interpret clusters (K-means, \\(K=4\\))\nWe fit K-means on the clustering matrix prep[\"X\"] (robust-scaled and capped for numeric stability).\nFor interpretation, we summarise clusters back in business units using contact_features_view (raw-unit features) and translate clusters into stakeholder-ready segments with actions.\n\n\n\n\n\n\nImportantTraining vs interpretation (do not mix these)\n\n\n\n\nTraining: uses prep[\"X\"] (scaled + capped) so Euclidean distances are fair and stable.\nInterpretation: uses raw-unit features (calls, seconds, days, shares) so segments can be explained and acted on.\nPrivacy: we keep contact_id hashed and do not merge any name/number fields into report outputs.\n\n\n\n\n\n6.1 Fit K-means (\\(K=4\\)) and store labels\n\n\nCode\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\n\nX = prep[\"X\"]   # final clustering matrix (scaled + capped)\nK_FINAL = 4\n\nkmeans = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42)\nlabels = kmeans.fit_predict(X)\n\n# One label per contact (hashed ID only)\ncontact_clusters = pd.DataFrame({\n    \"contact_id\": contact_features_view[\"contact_id\"].values,\n    \"cluster\": labels\n})\n\ncluster_sizes = (\n    contact_clusters[\"cluster\"]\n    .value_counts()\n    .sort_index()\n    .to_frame(\"n_contacts\")\n)\n\ncluster_sizes\n\n\n\n\n\n\n\n\n\nn_contacts\n\n\ncluster\n\n\n\n\n\n0\n91\n\n\n1\n176\n\n\n2\n1599\n\n\n3\n225\n\n\n\n\n\n\n\n\n\n\n6.2 Join clusters back to contact-level table (raw units)\n\n\nCode\ncontact_labeled = contact_features_view.merge(contact_clusters, on=\"contact_id\", how=\"left\")\n\n# Hard gate: every contact must get a cluster label\nassert contact_labeled[\"cluster\"].isna().sum() == 0\n\ncontact_labeled[[\"contact_id\", \"cluster\"]].head()\n\n\n\n\n\n\n\n\n\ncontact_id\ncluster\n\n\n\n\n0\n00047e187745\n2\n\n\n1\n000d71073f3e\n1\n\n\n2\n001c36642ecb\n3\n\n\n3\n00ad9a0b1291\n2\n\n\n4\n00ed0d18f9e3\n2\n\n\n\n\n\n\n\n\n\n\n6.3 Cluster “cards” (robust medians + size)\nThese cards are stakeholder-ready: one row per cluster, using medians (robust to heavy tails).\n\n\nCode\ncard_features = [\n    # volume / intensity\n    \"n_calls\", \"n_days_active\", \"calls_per_active_day\",\n\n    # duration / relationship depth proxy\n    \"total_dur_sec\", \"median_dur_sec\", \"long_call_share\",\n\n    # recency / tenure\n    \"recency_days\", \"tenure_days\",\n\n    # timing mix\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\nmissing_cards = [c for c in card_features if c not in contact_labeled.columns]\nassert len(missing_cards) == 0, f\"Missing card features: {missing_cards}\"\n\ncards = (\n    contact_labeled\n    .groupby(\"cluster\")[card_features]\n    .median()\n    .merge(cluster_sizes, left_index=True, right_index=True)\n    .reset_index()\n)\n\ncards = cards[[\"cluster\", \"n_contacts\"] + card_features]\ncards.T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\ncluster\n0.000000\n1.000000\n2.000000\n3.000000\n\n\nn_contacts\n91.000000\n176.000000\n1599.000000\n225.000000\n\n\nn_calls\n2.000000\n35.500000\n1.000000\n5.000000\n\n\nn_days_active\n2.000000\n21.000000\n1.000000\n4.000000\n\n\ncalls_per_active_day\n1.200000\n1.707143\n1.000000\n1.333333\n\n\ntotal_dur_sec\n1553.000000\n2776.000000\n61.000000\n270.000000\n\n\nmedian_dur_sec\n494.000000\n35.500000\n37.000000\n34.000000\n\n\nlong_call_share\n0.600000\n0.000000\n0.000000\n0.000000\n\n\nrecency_days\n621.220833\n84.802164\n501.331516\n259.384988\n\n\ntenure_days\n0.813924\n676.594664\n0.000000\n312.908426\n\n\nshare_business_hours\n1.000000\n0.911879\n1.000000\n1.000000\n\n\nshare_evening\n0.000000\n0.047619\n0.000000\n0.000000\n\n\nshare_early_morning\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nshare_late_night\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nshare_open_day\n1.000000\n0.983416\n1.000000\n1.000000\n\n\nshare_closed_day\n0.000000\n0.016584\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\n6.4 Within-cluster spread check (to avoid misleading medians)\nWe report 25/50/75% for a few “anchor” variables.\n\n\nCode\nspread_features = [\"n_calls\", \"total_dur_sec\", \"recency_days\", \"median_dur_sec\"]\n\nspread = (\n    contact_labeled\n    .groupby(\"cluster\")[spread_features]\n    .quantile([0.25, 0.50, 0.75])\n    .unstack(level=1)\n)\n\nspread.columns = [f\"{feat}_q{int(q*100)}\" for feat, q in spread.columns]\nspread = spread.reset_index()\n\nspread.T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\ncluster\n0.000000\n1.000000\n2.000000\n3.000000\n\n\nn_calls_q25\n1.000000\n23.000000\n1.000000\n3.000000\n\n\nn_calls_q50\n2.000000\n35.500000\n1.000000\n5.000000\n\n\nn_calls_q75\n4.000000\n85.250000\n2.000000\n9.000000\n\n\ntotal_dur_sec_q25\n819.500000\n1156.750000\n22.000000\n109.000000\n\n\ntotal_dur_sec_q50\n1553.000000\n2776.000000\n61.000000\n270.000000\n\n\ntotal_dur_sec_q75\n2971.500000\n8409.500000\n134.000000\n507.000000\n\n\nrecency_days_q25\n273.385498\n59.149308\n259.746244\n94.075741\n\n\nrecency_days_q50\n621.220833\n84.802164\n501.331516\n259.384988\n\n\nrecency_days_q75\n823.246152\n255.288302\n781.281389\n515.196192\n\n\nmedian_dur_sec_q25\n355.000000\n22.500000\n11.000000\n12.000000\n\n\nmedian_dur_sec_q50\n494.000000\n35.500000\n37.000000\n34.000000\n\n\nmedian_dur_sec_q75\n748.500000\n59.125000\n72.500000\n60.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipWhy timing-share medians often look like 0 or 1\n\n\n\nMany contacts have only 1–2 calls. With tiny denominators, share features become extreme. So for timing behaviour we also report means and % of contacts with any activity in each time band.\n\n\n\n\n\n6.5 Timing patterns (mean shares + % with any non-business-hours / Sunday activity)\n\n\nCode\nshare_cols = [\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\ntiming_mean = (\n    contact_labeled\n    .groupby(\"cluster\")[share_cols]\n    .mean()\n    .add_suffix(\"_mean\")\n    .reset_index()\n)\n\ntiming_any = (\n    contact_labeled\n    .assign(\n        any_evening=lambda x: (x[\"share_evening\"] &gt; 0).astype(int),\n        any_early_morning=lambda x: (x[\"share_early_morning\"] &gt; 0).astype(int),\n        any_late_night=lambda x: (x[\"share_late_night\"] &gt; 0).astype(int),\n        any_sunday=lambda x: (x[\"share_closed_day\"] &gt; 0).astype(int),\n    )\n    .groupby(\"cluster\")[[\"any_evening\", \"any_early_morning\", \"any_late_night\", \"any_sunday\"]]\n    .mean()\n    .mul(100)\n    .add_suffix(\"_pct_contacts\")\n    .reset_index()\n)\n\ntiming_summary = (\n    timing_mean\n    .merge(timing_any, on=\"cluster\", how=\"left\")\n    .merge(cluster_sizes.reset_index(), on=\"cluster\", how=\"left\")\n    .sort_values(\"cluster\")\n    .reset_index(drop=True)\n)\n\ntiming_summary.T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\ncluster\n0.000000\n1.000000\n2.000000\n3.000000\n\n\nshare_business_hours_mean\n0.874036\n0.837563\n0.905810\n0.874097\n\n\nshare_evening_mean\n0.077054\n0.108877\n0.065602\n0.094157\n\n\nshare_early_morning_mean\n0.033106\n0.040734\n0.020386\n0.022898\n\n\nshare_late_night_mean\n0.015803\n0.012827\n0.008203\n0.008848\n\n\nshare_open_day_mean\n0.917799\n0.935881\n0.957316\n0.950137\n\n\nshare_closed_day_mean\n0.082201\n0.064119\n0.042684\n0.049863\n\n\nany_evening_pct_contacts\n15.384615\n67.045455\n9.130707\n31.555556\n\n\nany_early_morning_pct_contacts\n8.791209\n48.295455\n3.189493\n12.000000\n\n\nany_late_night_pct_contacts\n5.494505\n25.568182\n1.438399\n3.111111\n\n\nany_sunday_pct_contacts\n15.384615\n54.545455\n5.753596\n19.111111\n\n\nn_contacts\n91.000000\n176.000000\n1599.000000\n225.000000\n\n\n\n\n\n\n\n\n\n\n6.6 Segment names + action policy (stakeholder-ready layer)\nCluster IDs are arbitrary. We map clusters to segment labels using behavioural signatures from cards.\n\n\nCode\nsig = cards[[\n    \"cluster\", \"n_contacts\",\n    \"n_calls\", \"total_dur_sec\", \"median_dur_sec\", \"long_call_share\",\n    \"recency_days\", \"tenure_days\"\n]].copy()\n\n# Identify clusters by signatures (robust against label permutation)\none_off_cluster = int(sig.sort_values([\"n_calls\", \"total_dur_sec\"], ascending=[True, True]).iloc[0][\"cluster\"])\ncore_active_cluster = int(sig.sort_values([\"n_calls\", \"n_contacts\"], ascending=[False, False]).iloc[0][\"cluster\"])\nrare_deep_cluster = int(sig.sort_values([\"median_dur_sec\", \"long_call_share\"], ascending=[False, False]).iloc[0][\"cluster\"])\n\nremaining = sorted(set(sig[\"cluster\"]) - {one_off_cluster, core_active_cluster, rare_deep_cluster})\nassert len(remaining) == 1, \"Expected exactly one remaining cluster for 'warm occasional'.\"\nwarm_cluster = int(remaining[0])\n\nsegment_map = {\n    one_off_cluster: {\n        \"label\": \"One-off / low-engagement contacts\",\n        \"primary_action\": \"Deprioritize by default; automate nurture; reactivate only if new activity appears\"\n    },\n    core_active_cluster: {\n        \"label\": \"Core active relationships\",\n        \"primary_action\": \"Retention + priority servicing; assign owner; proactive follow-ups\"\n    },\n    warm_cluster: {\n        \"label\": \"Warm occasional contacts\",\n        \"primary_action\": \"Nurture cadence (monthly/quarterly); structured check-ins to increase engagement\"\n    },\n    rare_deep_cluster: {\n        \"label\": \"Rare deep conversations\",\n        \"primary_action\": \"High-touch when active; preserve context; personalized re-engagement when dormant\"\n    }\n}\n\ncluster_labels = (\n    pd.DataFrame([\n        {\"cluster\": k, \"segment_label\": v[\"label\"], \"primary_action\": v[\"primary_action\"]}\n        for k, v in segment_map.items()\n    ])\n    .merge(cluster_sizes.reset_index(), on=\"cluster\", how=\"left\")\n    .sort_values(\"cluster\")\n    .reset_index(drop=True)\n)\n\ncluster_labels\n\n\n\n\n\n\n\n\n\ncluster\nsegment_label\nprimary_action\nn_contacts\n\n\n\n\n0\n0\nRare deep conversations\nHigh-touch when active; preserve context; pers...\n91\n\n\n1\n1\nCore active relationships\nRetention + priority servicing; assign owner; ...\n176\n\n\n2\n2\nOne-off / low-engagement contacts\nDeprioritize by default; automate nurture; rea...\n1599\n\n\n3\n3\nWarm occasional contacts\nNurture cadence (monthly/quarterly); structure...\n225\n\n\n\n\n\n\n\n\n\n\n6.7 Cluster narrative (auto-generated from the cards)\nShort and operational: “what it looks like” + “what to do”.\n\n\nCode\ndef fmt(x, nd=1):\n    if pd.isna(x):\n        return \"NA\"\n    return f\"{float(x):.{nd}f}\"\n\ncards_idx = cards.set_index(\"cluster\")\nlabels_idx = cluster_labels.set_index(\"cluster\")\n\nnarr_rows = []\nfor c in sorted(cards_idx.index):\n    row = cards_idx.loc[c]\n    seg = labels_idx.loc[c, \"segment_label\"]\n    act = labels_idx.loc[c, \"primary_action\"]\n\n    narrative = (\n        f\"{seg}: median calls={fmt(row['n_calls'],1)}, \"\n        f\"median active days={fmt(row['n_days_active'],1)}, \"\n        f\"median total duration (sec)={fmt(row['total_dur_sec'],0)}, \"\n        f\"median call duration (sec)={fmt(row['median_dur_sec'],0)}, \"\n        f\"median recency (days)={fmt(row['recency_days'],0)}.\"\n    )\n\n    narr_rows.append({\n        \"cluster\": int(c),\n        \"segment_label\": seg,\n        \"n_contacts\": int(row[\"n_contacts\"]),\n        \"cluster_story\": narrative,\n        \"default_action\": act\n    })\n\ncluster_story = pd.DataFrame(narr_rows).sort_values(\"cluster\").reset_index(drop=True)\ncluster_story\n\n\n\n\n\n\n\n\n\ncluster\nsegment_label\nn_contacts\ncluster_story\ndefault_action\n\n\n\n\n0\n0\nRare deep conversations\n91\nRare deep conversations: median calls=2.0, med...\nHigh-touch when active; preserve context; pers...\n\n\n1\n1\nCore active relationships\n176\nCore active relationships: median calls=35.5, ...\nRetention + priority servicing; assign owner; ...\n\n\n2\n2\nOne-off / low-engagement contacts\n1599\nOne-off / low-engagement contacts: median call...\nDeprioritize by default; automate nurture; rea...\n\n\n3\n3\nWarm occasional contacts\n225\nWarm occasional contacts: median calls=5.0, me...\nNurture cadence (monthly/quarterly); structure...\n\n\n\n\n\n\n\n\n\n\n6.8 “What decisions can the business make with these segments?”\nPolicy rules we can implement in a CRM without exposing identifiers.\n\n\nCode\nrules = pd.DataFrame([\n    {\n        \"segment_label\": \"Core active relationships\",\n        \"decision_use\": \"Prioritization and retention\",\n        \"operational_rule\": \"Assign owner and SLA; proactive check-ins\",\n        \"typical_pattern\": \"High calls, many active days, low recency\"\n    },\n    {\n        \"segment_label\": \"Warm occasional contacts\",\n        \"decision_use\": \"Nurture and growth\",\n        \"operational_rule\": \"Monthly/quarterly outreach; reminders; targeted offers\",\n        \"typical_pattern\": \"Moderate repeat calls, moderate tenure, mid recency\"\n    },\n    {\n        \"segment_label\": \"Rare deep conversations\",\n        \"decision_use\": \"High-touch exceptions and win-back\",\n        \"operational_rule\": \"Personalized follow-up; preserve context; targeted reactivation if dormant\",\n        \"typical_pattern\": \"Few calls but long conversations (high median duration / long-call share)\"\n    },\n    {\n        \"segment_label\": \"One-off / low-engagement contacts\",\n        \"decision_use\": \"Noise filtering and automation\",\n        \"operational_rule\": \"Exclude from priority lists; automate nurture only\",\n        \"typical_pattern\": \"One call, one day, small total duration, often old\"\n    },\n])\n\nrules\n\n\n\n\n\n\n\n\n\nsegment_label\ndecision_use\noperational_rule\ntypical_pattern\n\n\n\n\n0\nCore active relationships\nPrioritization and retention\nAssign owner and SLA; proactive check-ins\nHigh calls, many active days, low recency\n\n\n1\nWarm occasional contacts\nNurture and growth\nMonthly/quarterly outreach; reminders; targete...\nModerate repeat calls, moderate tenure, mid re...\n\n\n2\nRare deep conversations\nHigh-touch exceptions and win-back\nPersonalized follow-up; preserve context; targ...\nFew calls but long conversations (high median ...\n\n\n3\nOne-off / low-engagement contacts\nNoise filtering and automation\nExclude from priority lists; automate nurture ...\nOne call, one day, small total duration, often...\n\n\n\n\n\n\n\n\n\n\n6.9 Store artifacts for later phases (robustness, sensitivity, PCA)\n\n\nCode\nprep[\"K_FINAL\"] = K_FINAL\nprep[\"kmeans\"] = kmeans\nprep[\"labels\"] = labels\n\nprep[\"contact_clusters\"] = contact_clusters\nprep[\"cluster_sizes\"] = cluster_sizes\n\nprep[\"cards\"] = cards\nprep[\"spread\"] = spread\nprep[\"timing_summary\"] = timing_summary\nprep[\"cluster_labels\"] = cluster_labels\nprep[\"cluster_story\"] = cluster_story\nprep[\"rules\"] = rules\n\nlist(prep.keys())[:15]\n\n\n['MODEL_FEATURES',\n 'FEATURE_BLOCKS',\n 'feature_names',\n 'imputer',\n 'scaler',\n 'X_raw',\n 'X_clip',\n 'X',\n 'clip_table',\n 'dominance',\n 'preprocess_audit',\n 'X_uncapped',\n 'K_FINAL',\n 'kmeans',\n 'labels']",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#phase-6-robustness-and-sensitivity-do-the-clusters-hold-up",
    "href": "walsoft_contact_segmentation.html#phase-6-robustness-and-sensitivity-do-the-clusters-hold-up",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "7 Phase 6 — Robustness and sensitivity (do the clusters “hold up”?)",
    "text": "7 Phase 6 — Robustness and sensitivity (do the clusters “hold up”?)\nA good clustering result is not just “a nice plot.” It should be stable when we rerun the algorithm and reasonably consistent when we change feature blocks.\nIn this phase we test two kinds of robustness:\n\nSeed stability: If we change the random seed (different K-means initializations), do we get essentially the same segmentation?\nFeature-block sensitivity: If we drop one block (volume, duration, recency, timing), do the segments remain broadly similar?\n\nWe quantify stability using Adjusted Rand Index (ARI):\n\n\\(ARI = 1\\) means two clusterings are identical up to label permutation.\n\\(ARI \\approx 0\\) means agreement is no better than random.\nNegative \\(ARI\\) can happen (worse than random agreement), usually a warning sign.\n\n\n\n\n\n\n\nImportantTraining matrix\n\n\n\nAll robustness tests use prep[\"X\"] (robust-scaled + capped) so distances are fair and numerically stable.\n\n7.1 Privacy\nWe do not merge names/phone numbers into any robustness outputs. We only evaluate labels as arrays.\n\n\n\n\n\n7.2 Seed stability test (ARI across many random seeds)\nWe run K-means many times with different random_state, always using \\(K = 4\\), and compare each run to a baseline labeling.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# -------------------------\n# Setup\n# -------------------------\nX = prep[\"X\"]\nK_FINAL = int(prep.get(\"K_FINAL\", 4))\n\n# Baseline labels (use Phase 5 if available)\nif \"labels\" in prep and prep.get(\"labels\", None) is not None:\n    base_labels = np.asarray(prep[\"labels\"])\nelse:\n    km0 = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42)\n    base_labels = km0.fit_predict(X)\n\n# -------------------------\n# Run many seeds\n# -------------------------\nSEEDS = list(range(0, 50))  # employer-ready default\nrows = []\n\nfor s in SEEDS:\n    km = KMeans(n_clusters=K_FINAL, n_init=50, random_state=s)\n    lab = km.fit_predict(X)\n    ari = float(adjusted_rand_score(base_labels, lab))\n    rows.append({\"seed\": s, \"ari_vs_baseline\": ari, \"inertia\": float(km.inertia_)})\n\nseed_stability = (\n    pd.DataFrame(rows)\n    .sort_values(\"ari_vs_baseline\", ascending=True)\n    .reset_index(drop=True)\n)\n\nseed_audit = pd.DataFrame({\n    \"K_FINAL\": [K_FINAL],\n    \"n_seeds_tested\": [len(SEEDS)],\n    \"ari_min\": [float(seed_stability[\"ari_vs_baseline\"].min())],\n    \"ari_p05\": [float(seed_stability[\"ari_vs_baseline\"].quantile(0.05))],\n    \"ari_median\": [float(seed_stability[\"ari_vs_baseline\"].median())],\n    \"ari_p95\": [float(seed_stability[\"ari_vs_baseline\"].quantile(0.95))],\n    \"ari_max\": [float(seed_stability[\"ari_vs_baseline\"].max())],\n})\n\nseed_audit, seed_stability.head(10)\n\n\n(   K_FINAL  n_seeds_tested  ari_min  ari_p05  ari_median  ari_p95  ari_max\n 0        4              50      1.0      1.0         1.0      1.0      1.0,\n    seed  ari_vs_baseline       inertia\n 0     0              1.0  26086.981574\n 1     1              1.0  26086.981574\n 2     2              1.0  26086.981574\n 3     3              1.0  26086.981574\n 4     4              1.0  26086.981574\n 5     5              1.0  26086.981574\n 6     6              1.0  26086.981574\n 7     7              1.0  26086.981574\n 8     8              1.0  26086.981574\n 9     9              1.0  26086.981574)\n\n\n\n\n\n\n\n\nTipHow to interpret seed ARI\n\n\n\nA practical rule of thumb:\n\nVery stable: most ARI values \\(\\ge 0.90\\)\nReasonably stable: most ARI values \\(\\ge 0.75\\)\nUnstable: many ARI values \\(&lt; 0.60\\)\n\nIf stability is weak, we usually revisit preprocessing, \\(K\\), or consider a different clustering method.\n\n\n\n\n\n7.3 “Worst-case” rerun inspection (label-aligned)\nEven when ARI is \\(1.0\\), K-means can output the same partition with different numeric labels (label permutation). So we align labels before comparing cluster sizes.\n\n\nCode\n# Identify worst seed run (lowest ARI vs baseline)\nworst = seed_stability.iloc[0]\nworst_seed = int(worst[\"seed\"])\n\nkm_worst = KMeans(n_clusters=K_FINAL, n_init=50, random_state=worst_seed)\nlabels_worst = km_worst.fit_predict(X)\n\nari_worst = float(adjusted_rand_score(base_labels, labels_worst))\n\n# Contingency table: baseline cluster IDs (rows) vs worst run IDs (cols)\nct = pd.crosstab(\n    pd.Series(base_labels, name=\"baseline\"),\n    pd.Series(labels_worst, name=\"worst\")\n)\n\n# Map each worst-cluster to the baseline cluster it overlaps with most\nmapping = ct.idxmax(axis=0).to_dict()\n\nlabels_worst_aligned = np.vectorize(mapping.get)(labels_worst)\n\n# Compare cluster sizes after alignment\nbase_sizes = pd.Series(base_labels).value_counts().sort_index()\nworst_sizes = pd.Series(labels_worst_aligned).value_counts().sort_index()\n\nsize_compare = pd.DataFrame({\n    \"cluster_id\": sorted(set(base_sizes.index) | set(worst_sizes.index)),\n}).set_index(\"cluster_id\")\n\nsize_compare[\"baseline_n\"] = base_sizes.reindex(size_compare.index).fillna(0).astype(int)\nsize_compare[\"worst_n_aligned\"] = worst_sizes.reindex(size_compare.index).fillna(0).astype(int)\nsize_compare[\"baseline_pct\"] = (size_compare[\"baseline_n\"] / size_compare[\"baseline_n\"].sum() * 100).round(2)\nsize_compare[\"worst_pct_aligned\"] = (size_compare[\"worst_n_aligned\"] / size_compare[\"worst_n_aligned\"].sum() * 100).round(2)\n\nworst_seed, float(worst[\"ari_vs_baseline\"]), ari_worst, mapping, size_compare.reset_index()\n\n\n(0,\n 1.0,\n 1.0,\n {0: 0, 1: 1, 2: 3, 3: 2},\n    cluster_id  baseline_n  worst_n_aligned  baseline_pct  worst_pct_aligned\n 0           0          91               91          4.35               4.35\n 1           1         176              176          8.42               8.42\n 2           2        1599             1599         76.47              76.47\n 3           3         225              225         10.76              10.76)\n\n\n\n\n\n7.4 Feature-block sensitivity (drop-one-block tests)\nNow we test whether the segmentation depends too heavily on a single block. We refit K-means on reduced matrices and compare to baseline using ARI.\nWe do “drop one block at a time” on:\n\nvolume\nduration\nrecency\ntiming\n\n\n\n\n\n\n\nNoteWhy ARI is valid here\n\n\n\nCluster labels are arbitrary (cluster 0 in one run is not “the same” as cluster 0 in another). ARI is invariant to label permutation, so it compares structure, not label IDs.\n\n\n\n\nCode\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n\n# Rebuild X for each variant using the SAME preprocessing recipe as Phase 3:\n# - drop constant cols\n# - clip p01–p99 in raw space\n# - median impute + RobustScaler\n# - cap in scaled space\n\nSCALED_CAP = float(prep.get(\"scaled_cap\", 10.0))  # safe default\n\nFEATURE_BLOCKS = prep[\"FEATURE_BLOCKS\"]\nMODEL_FEATURES = prep[\"MODEL_FEATURES\"]\ndf_view = contact_features_view  # raw-unit feature table\n\ndef build_X_from_features(df, features, clip_lo=0.01, clip_hi=0.99, scaled_cap=10.0):\n    X_raw = df[features].copy()\n\n    # Drop truly constant columns\n    nunique = X_raw.nunique(dropna=False)\n    constant_cols = nunique[nunique &lt;= 1].index.tolist()\n    X_raw2 = X_raw.drop(columns=constant_cols) if constant_cols else X_raw.copy()\n\n    # Clip extremes in raw space\n    X_clip = X_raw2.copy()\n    for col in X_clip.columns:\n        lo = float(X_clip[col].quantile(clip_lo))\n        hi = float(X_clip[col].quantile(clip_hi))\n        X_clip[col] = X_clip[col].clip(lower=lo, upper=hi)\n\n    # Impute + robust scale\n    imputer = SimpleImputer(strategy=\"median\")\n    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n    X_imp = imputer.fit_transform(X_clip)\n    X_scaled = scaler.fit_transform(X_imp)\n\n    # Final cap in scaled space\n    X_final = np.clip(X_scaled, -scaled_cap, scaled_cap)\n\n    return X_final, list(X_clip.columns)\n\n# Baseline rebuild for consistent comparisons\nX_base, base_feature_names = build_X_from_features(\n    df_view, MODEL_FEATURES, clip_lo=0.01, clip_hi=0.99, scaled_cap=SCALED_CAP\n)\nbase_labels_for_sens = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42).fit_predict(X_base)\n\n# Define variants: drop one block at a time (excluding \"category\")\nvariants = {}\nfor block in [\"volume\", \"duration\", \"recency\", \"timing\"]:\n    keep = []\n    for b, feats in FEATURE_BLOCKS.items():\n        if b == \"category\":\n            continue\n        if b != block:\n            keep += feats\n    variants[f\"drop_{block}\"] = keep\n\nrows = []\nfor name, feats in variants.items():\n    X_var, feat_names_var = build_X_from_features(\n        df_view, feats, clip_lo=0.01, clip_hi=0.99, scaled_cap=SCALED_CAP\n    )\n    lab = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42).fit_predict(X_var)\n    ari = float(adjusted_rand_score(base_labels_for_sens, lab))\n    rows.append({\n        \"variant\": name,\n        \"n_features\": int(X_var.shape[1]),\n        \"ari_vs_baseline\": ari\n    })\n\nblock_sensitivity = (\n    pd.DataFrame(rows)\n    .sort_values(\"ari_vs_baseline\", ascending=False)\n    .reset_index(drop=True)\n)\n\nblock_sensitivity\n\n\n\n\n\n\n\n\n\nvariant\nn_features\nari_vs_baseline\n\n\n\n\n0\ndrop_timing\n14\n1.000000\n\n\n1\ndrop_volume\n15\n0.943619\n\n\n2\ndrop_recency\n18\n0.738307\n\n\n3\ndrop_duration\n13\n0.698273\n\n\n\n\n\n\n\n\n\n\n7.5 Sensitivity summary (decision-useful)\n\n\nCode\nsens_audit = pd.DataFrame({\n    \"K_FINAL\": [K_FINAL],\n    \"seed_stability_ari_median\": [float(seed_audit[\"ari_median\"].iloc[0])],\n    \"seed_stability_ari_p05\": [float(seed_audit[\"ari_p05\"].iloc[0])],\n    \"seed_stability_ari_min\": [float(seed_audit[\"ari_min\"].iloc[0])],\n    \"block_sensitivity_ari_min\": [float(block_sensitivity[\"ari_vs_baseline\"].min())],\n    \"block_sensitivity_ari_median\": [float(block_sensitivity[\"ari_vs_baseline\"].median())],\n})\n\nsens_audit.T\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\nK_FINAL\n4.000000\n\n\nseed_stability_ari_median\n1.000000\n\n\nseed_stability_ari_p05\n1.000000\n\n\nseed_stability_ari_min\n1.000000\n\n\nblock_sensitivity_ari_min\n0.698273\n\n\nblock_sensitivity_ari_median\n0.840963\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantWhat we conclude here (based on your results)\n\n\n\n\nSeed stability: ARI is \\(1.0\\) for all tested seeds \\(\\Rightarrow\\) K-means solution is fully stable.\nBlock sensitivity: dropping duration or recency reduces agreement (ARI \\(\\approx 0.70\\)–\\(0.74\\)), which implies these blocks carry meaningful segmentation signal.\n\nThis is robust enough for stakeholder-facing use, with a clear explanation of what drives the segments.\n\n\n\n\n\n7.6 Store robustness artifacts (for later reporting)\n\n\nCode\nprep[\"seed_stability\"] = seed_stability\nprep[\"seed_audit\"] = seed_audit\n\nprep[\"labels_worst_seed\"] = labels_worst\nprep[\"labels_worst_seed_aligned\"] = labels_worst_aligned\nprep[\"size_compare_worst_seed\"] = size_compare.reset_index()\nprep[\"worst_seed_label_mapping\"] = mapping\n\nprep[\"block_sensitivity\"] = block_sensitivity\nprep[\"sens_audit\"] = sens_audit\n\nlist(prep.keys())[:25]\n\n\n['MODEL_FEATURES',\n 'FEATURE_BLOCKS',\n 'feature_names',\n 'imputer',\n 'scaler',\n 'X_raw',\n 'X_clip',\n 'X',\n 'clip_table',\n 'dominance',\n 'preprocess_audit',\n 'X_uncapped',\n 'K_FINAL',\n 'kmeans',\n 'labels',\n 'contact_clusters',\n 'cluster_sizes',\n 'cards',\n 'spread',\n 'timing_summary',\n 'cluster_labels',\n 'cluster_story',\n 'rules',\n 'seed_stability',\n 'seed_audit']\n\n\n\n\n\n\n\n\n\nImportantPhase 6 summary (robustness verdict)\n\n\n\nBased on your results:\n\nSeed stability (random restarts): \\(ARI = 1.00\\) for all tested seeds \\(\\Rightarrow\\) the \\(K=4\\) K-means solution is fully stable on prep[\"X\"].\nFeature-block sensitivity (drop-one-block):\n\nDrop timing: \\(ARI = 1.00\\) (timing is not driving the segmentation)\nDrop volume: \\(ARI \\approx 0.94\\) (volume contributes, but is not the sole driver)\nDrop recency: \\(ARI \\approx 0.74\\) (recency contains meaningful signal)\nDrop duration: \\(ARI \\approx 0.70\\) (duration contains meaningful signal)\n\n\nDecision-useful takeaway: The segmentation is stable and not an artifact of random initialization.\nThe clusters are primarily supported by duration and recency behaviour (with volume contributing), so these are the main behavioural levers to emphasize in stakeholder explanations.",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#phase-7-pca-visualization-for-interpretation-only",
    "href": "walsoft_contact_segmentation.html#phase-7-pca-visualization-for-interpretation-only",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "8 Phase 7 — PCA visualization (for interpretation only)",
    "text": "8 Phase 7 — PCA visualization (for interpretation only)\nPCA is not used to train clustering.\nWe use PCA only to create a 2D “map” of contacts for storytelling, sanity-checking, and explaining separation.\n\n\n\n\n\n\nWarningPCA is not the clustering model\n\n\n\n\nClustering was trained on: prep[\"X\"] (robust-scaled + capped)\nPCA is used for: visualization only (2D projection) If PCA looks messy, it does not automatically mean clustering is wrong. PCA compresses information into 2 dimensions.\n\n\n\n\n\n\n\n\n\nImportantPrivacy\n\n\n\nWe plot only:\n\nPCA coordinates\ncluster labels We do not plot names or phone numbers, and we keep contact_id hashed.\n\n\n\n\n\n8.1 Fit PCA (\\(2\\) components) on the clustering matrix\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nX = prep[\"X\"]  # scaled + capped matrix used for clustering\nlabels = np.asarray(prep[\"labels\"])\nK_FINAL = int(prep.get(\"K_FINAL\", 4))\n\npca = PCA(n_components=2, random_state=42)\nZ = pca.fit_transform(X)\n\npca_audit = pd.DataFrame({\n    \"n_components\": [2],\n    \"explained_var_ratio_pc1\": [float(pca.explained_variance_ratio_[0])],\n    \"explained_var_ratio_pc2\": [float(pca.explained_variance_ratio_[1])],\n    \"explained_var_ratio_total_2pc\": [float(pca.explained_variance_ratio_[:2].sum())],\n})\n\npca_audit\n\n\n\n\n\n\n\n\n\nn_components\nexplained_var_ratio_pc1\nexplained_var_ratio_pc2\nexplained_var_ratio_total_2pc\n\n\n\n\n0\n2\n0.628668\n0.215786\n0.844454\n\n\n\n\n\n\n\n\n\n\n8.2 Build a plotting table (hashed id + PCA coordinates + cluster)\n\n\nCode\npca_view = pd.DataFrame({\n    \"contact_id\": contact_features_view[\"contact_id\"].values,  # hashed only\n    \"pc1\": Z[:, 0],\n    \"pc2\": Z[:, 1],\n    \"cluster\": labels\n})\n\npca_view.head()\n\n\n\n\n\n\n\n\n\ncontact_id\npc1\npc2\ncluster\n\n\n\n\n0\n00047e187745\n-3.211533\n-0.439848\n2\n\n\n1\n000d71073f3e\n17.283469\n-2.797619\n1\n\n\n2\n001c36642ecb\n6.364608\n-1.535155\n3\n\n\n3\n00ad9a0b1291\n-2.066420\n3.971317\n2\n\n\n4\n00ed0d18f9e3\n-1.794263\n1.314460\n2\n\n\n\n\n\n\n\n\n\n\n8.3 PCA scatter plot (clusters on the 2D map)\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot each cluster separately for a clear legend\nfor c in sorted(pca_view[\"cluster\"].unique()):\n    sub = pca_view[pca_view[\"cluster\"] == c]\n    ax.scatter(sub[\"pc1\"], sub[\"pc2\"], s=12, alpha=0.7, label=f\"Cluster {c}\")\n\nax.set_title(f\"PCA map of contacts (K-means clusters, $K={K_FINAL}$)\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.legend(title=\"Cluster\", fontsize=9)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n8.4 Optional: cluster centroids projected into PCA space\nThis helps stakeholders see “where the centers sit” on the map.\n\n\nCode\n# K-means centroids exist in scaled feature space; PCA was fit on the same space.\ncentroids = prep[\"kmeans\"].cluster_centers_\ncentroids_2d = pca.transform(centroids)\n\ncent = pd.DataFrame({\n    \"cluster\": list(range(K_FINAL)),\n    \"pc1_centroid\": centroids_2d[:, 0],\n    \"pc2_centroid\": centroids_2d[:, 1],\n})\n\ncent\n\n\n\n\n\n\n\n\n\ncluster\npc1_centroid\npc2_centroid\n\n\n\n\n0\n0\n4.245796\n11.770443\n\n\n1\n1\n15.100966\n-1.621873\n\n\n2\n2\n-2.582659\n-0.175937\n\n\n3\n3\n4.824594\n-2.241496\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor c in sorted(pca_view[\"cluster\"].unique()):\n    sub = pca_view[pca_view[\"cluster\"] == c]\n    ax.scatter(sub[\"pc1\"], sub[\"pc2\"], s=10, alpha=0.45, label=f\"Cluster {c}\")\n\n# Centroids\nax.scatter(cent[\"pc1_centroid\"], cent[\"pc2_centroid\"], s=120, marker=\"X\", label=\"Centroids\")\n\nax.set_title(f\"PCA map with centroids (K={K_FINAL})\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.legend(fontsize=9)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n8.5 Interpretation guidance (what we conclude from PCA)\n\n\n\n\n\n\nNote\n\n\n\nPCA is a compression from many dimensions down to 2.\nUse this plot to check:\n\nwhether clusters are roughly separated (good sign),\nwhether some clusters overlap heavily (could be fine; segmentation can still be valid),\nwhether a small cluster looks like extreme outliers (could indicate a “special handling” segment).\n\nDo not use PCA to choose \\(K\\) or to claim “the true number of clusters.”\n\n\n\n\n\n8.6 Store PCA artifacts (for the final report)\n\n\nCode\nprep[\"pca_model\"] = pca\nprep[\"pca_audit\"] = pca_audit\nprep[\"pca_view\"] = pca_view\nprep[\"pca_centroids_2d\"] = cent\n\nlist(prep.keys())[:30]\n\n\n['MODEL_FEATURES',\n 'FEATURE_BLOCKS',\n 'feature_names',\n 'imputer',\n 'scaler',\n 'X_raw',\n 'X_clip',\n 'X',\n 'clip_table',\n 'dominance',\n 'preprocess_audit',\n 'X_uncapped',\n 'K_FINAL',\n 'kmeans',\n 'labels',\n 'contact_clusters',\n 'cluster_sizes',\n 'cards',\n 'spread',\n 'timing_summary',\n 'cluster_labels',\n 'cluster_story',\n 'rules',\n 'seed_stability',\n 'seed_audit',\n 'labels_worst_seed',\n 'labels_worst_seed_aligned',\n 'size_compare_worst_seed',\n 'worst_seed_label_mapping',\n 'block_sensitivity']",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#what-pca-tells-us-interpretation-only",
    "href": "walsoft_contact_segmentation.html#what-pca-tells-us-interpretation-only",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "9 What PCA tells us (interpretation only)",
    "text": "9 What PCA tells us (interpretation only)\nPCA is a 2D projection of the same scaled training matrix used for clustering, prep[\"X\"]. It does not change the model; it only helps us see separation.\n\n9.1 Variance captured\nFrom pca_audit, the first two components explain:\n\n\\(PC1 \\approx 0.629\\)\n\\(PC2 \\approx 0.216\\)\nTotal (2D) \\(\\approx 0.844\\)\n\nSo about 84.4% of the variance in the scaled feature space is visible in the 2D map. That is high enough that the plot is a meaningful sanity-check.\n\n\n9.2 Visual separation (sanity-check)\nThe PCA map shows clear structure:\n\nOne cluster sits far to the right on \\(PC1\\) (strong separation in the main direction of variance).\nOne cluster is concentrated far to the left (tight wedge near low \\(PC1\\) values).\nTwo clusters occupy the middle/bottom region with partial overlap, suggesting they differ in multiple dimensions (not fully separable in 2D, which is normal).\n\nThis supports the earlier interpretation: the segmentation is not an artifact of one random run (Phase 6 already proved stability), and the clusters correspond to genuinely different behavioral regimes in feature space.\n\n\n9.3 Centroids on the PCA map\nProjected centroids help explain “where the centers sit”:\n\nThe centroid that is far right on \\(PC1\\) corresponds to a high-intensity / high-activity regime.\nThe centroid that is far left corresponds to low-engagement / near-baseline contacts.\nThe centroid that is high on \\(PC2\\) indicates a behavior pattern that is not just “more or less activity,” but a different mix (often duration/recency-related structure).\n\nImportant: overlap in PCA does not invalidate clusters. K-means was fit in the full feature space; PCA compresses information into 2 dimensions.",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#phase-8-final-deliverables-stakeholder-ready-outputs",
    "href": "walsoft_contact_segmentation.html#phase-8-final-deliverables-stakeholder-ready-outputs",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "10 Phase 8 — Final deliverables (stakeholder-ready outputs)",
    "text": "10 Phase 8 — Final deliverables (stakeholder-ready outputs)\nIn this phase we package the work into decision-ready artifacts that can be used in a CRM or reporting workflow without exposing identities.\n\n\n\n\n\n\nImportantPrivacy and operational use\n\n\n\n\nWe do not join names or phone numbers into any outputs.\nAll contact identifiers remain hashed.\nDefault is no file export. You can enable export if you want portfolio artifacts.\n\n\n\n\n\n10.1 Assemble the “final segment table” (labels + key medians)\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Guardrails\nrequired = [\"cards\", \"cluster_labels\", \"rules\", \"sens_audit\", \"pca_audit\"]\nmissing = [k for k in required if k not in prep]\nassert len(missing) == 0, f\"Missing Phase artifacts in prep: {missing}\"\n\ncards = prep[\"cards\"].copy()\ncluster_labels = prep[\"cluster_labels\"].copy()\nrules = prep[\"rules\"].copy()\n\n# Join: cluster -&gt; segment label + action\nfinal_segments = (\n    cards.merge(cluster_labels[[\"cluster\", \"segment_label\", \"primary_action\"]], on=\"cluster\", how=\"left\")\n    .sort_values(\"n_contacts\", ascending=False)\n    .reset_index(drop=True)\n)\n\n# Reorder columns for a stakeholder view\nfront = [\"cluster\", \"segment_label\", \"n_contacts\", \"primary_action\"]\nrest = [c for c in final_segments.columns if c not in front]\nfinal_segments = final_segments[front + rest]\n\nfinal_segments\n\n\n\n\n\n\n\n\n\ncluster\nsegment_label\nn_contacts\nprimary_action\nn_calls\nn_days_active\ncalls_per_active_day\ntotal_dur_sec\nmedian_dur_sec\nlong_call_share\nrecency_days\ntenure_days\nshare_business_hours\nshare_evening\nshare_early_morning\nshare_late_night\nshare_open_day\nshare_closed_day\n\n\n\n\n0\n2\nOne-off / low-engagement contacts\n1599\nDeprioritize by default; automate nurture; rea...\n1.0\n1.0\n1.000000\n61.0\n37.0\n0.0\n501.331516\n0.000000\n1.000000\n0.000000\n0.0\n0.0\n1.000000\n0.000000\n\n\n1\n3\nWarm occasional contacts\n225\nNurture cadence (monthly/quarterly); structure...\n5.0\n4.0\n1.333333\n270.0\n34.0\n0.0\n259.384988\n312.908426\n1.000000\n0.000000\n0.0\n0.0\n1.000000\n0.000000\n\n\n2\n1\nCore active relationships\n176\nRetention + priority servicing; assign owner; ...\n35.5\n21.0\n1.707143\n2776.0\n35.5\n0.0\n84.802164\n676.594664\n0.911879\n0.047619\n0.0\n0.0\n0.983416\n0.016584\n\n\n3\n0\nRare deep conversations\n91\nHigh-touch when active; preserve context; pers...\n2.0\n2.0\n1.200000\n1553.0\n494.0\n0.6\n621.220833\n0.813924\n1.000000\n0.000000\n0.0\n0.0\n1.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\n10.2 Decision rules (CRM / outreach policy)\n\n\nCode\nrules\n\n\n\n\n\n\n\n\n\nsegment_label\ndecision_use\noperational_rule\ntypical_pattern\n\n\n\n\n0\nCore active relationships\nPrioritization and retention\nAssign owner and SLA; proactive check-ins\nHigh calls, many active days, low recency\n\n\n1\nWarm occasional contacts\nNurture and growth\nMonthly/quarterly outreach; reminders; targete...\nModerate repeat calls, moderate tenure, mid re...\n\n\n2\nRare deep conversations\nHigh-touch exceptions and win-back\nPersonalized follow-up; preserve context; targ...\nFew calls but long conversations (high median ...\n\n\n3\nOne-off / low-engagement contacts\nNoise filtering and automation\nExclude from priority lists; automate nurture ...\nOne call, one day, small total duration, often...\n\n\n\n\n\n\n\n\n\n\n10.3 “Executive summary” (one screen)\n\n\nCode\nsens_audit = prep[\"sens_audit\"].copy()\npca_audit = prep[\"pca_audit\"].copy()\n\n# Key stats\nn_contacts_total = int(prep[\"contact_clusters\"].shape[0])\nk_final = int(prep.get(\"K_FINAL\", 4))\n\nseed_median = float(sens_audit[\"seed_stability_ari_median\"].iloc[0])\nseed_min = float(sens_audit[\"seed_stability_ari_min\"].iloc[0])\nblock_min = float(sens_audit[\"block_sensitivity_ari_min\"].iloc[0])\n\npca_total2 = float(pca_audit[\"explained_var_ratio_total_2pc\"].iloc[0])\n\nexec_summary = pd.DataFrame([{\n    \"n_contacts_clustered\": n_contacts_total,\n    \"K_FINAL\": k_final,\n    \"seed_stability_ARI_median\": seed_median,\n    \"seed_stability_ARI_min\": seed_min,\n    \"block_sensitivity_ARI_min\": block_min,\n    \"pca_variance_explained_2PC\": pca_total2,\n}])\n\nexec_summary\n\n\n\n\n\n\n\n\n\nn_contacts_clustered\nK_FINAL\nseed_stability_ARI_median\nseed_stability_ARI_min\nblock_sensitivity_ARI_min\npca_variance_explained_2PC\n\n\n\n\n0\n2091\n4\n1.0\n1.0\n0.698273\n0.844454\n\n\n\n\n\n\n\n\n\n\n10.4 Findings (plain-English, decision-first)\n\n\nCode\n# Convert the segment table into a short narrative summary\nseg_counts = final_segments[[\"segment_label\", \"n_contacts\"]].copy()\nseg_counts[\"share_pct\"] = (seg_counts[\"n_contacts\"] / seg_counts[\"n_contacts\"].sum() * 100).round(2)\n\nseg_counts\n\n\n\n\n\n\n\n\n\nsegment_label\nn_contacts\nshare_pct\n\n\n\n\n0\nOne-off / low-engagement contacts\n1599\n76.47\n\n\n1\nWarm occasional contacts\n225\n10.76\n\n\n2\nCore active relationships\n176\n8.42\n\n\n3\nRare deep conversations\n91\n4.35\n\n\n\n\n\n\n\n\n\nCode\n# Build a compact “what it is + what to do” list (no IDs)\nlabels_idx = cluster_labels.set_index(\"segment_label\")\n\nrows = []\nfor _, r in seg_counts.iterrows():\n    seg = r[\"segment_label\"]\n    n = int(r[\"n_contacts\"])\n    pct = float(r[\"share_pct\"])\n    action = labels_idx.loc[seg, \"primary_action\"] if seg in labels_idx.index else \"TBD\"\n    rows.append({\"segment_label\": seg, \"n_contacts\": n, \"share_pct\": pct, \"default_action\": action})\n\nfindings_table = pd.DataFrame(rows).sort_values(\"n_contacts\", ascending=False).reset_index(drop=True)\nfindings_table\n\n\n\n\n\n\n\n\n\nsegment_label\nn_contacts\nshare_pct\ndefault_action\n\n\n\n\n0\nOne-off / low-engagement contacts\n1599\n76.47\nDeprioritize by default; automate nurture; rea...\n\n\n1\nWarm occasional contacts\n225\n10.76\nNurture cadence (monthly/quarterly); structure...\n\n\n2\nCore active relationships\n176\n8.42\nRetention + priority servicing; assign owner; ...\n\n\n3\nRare deep conversations\n91\n4.35\nHigh-touch when active; preserve context; pers...\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat we learned from your results\n\n\n\n\nThe solution is fully stable to random initialization (seed ARI \\(\\approx 1\\) across tested seeds), so you can rerun K-means and get the same segmentation (up to label permutation).\nRecency and duration matter: dropping either block reduces agreement (lower ARI), so those blocks carry meaningful segmentation signal.\nPCA is supportive, not decisive: the first two PCs explain a large share of variance, and the 2D map shows visible separation patterns that align with the segmentation.\n\n\n\n\n\n\n10.5 Limitations and “how not to misuse this”\n\n\n\n\n\n\nWarningLimitations\n\n\n\n\nThis is behavior-based segmentation, not a causal model. It supports prioritization and outreach strategy, not “ground truth identities.”\nK-means assumes roughly spherical clusters in the feature space. If your business needs non-spherical shapes, consider alternative methods later (e.g., GMM, HDBSCAN).\nVery sparse contacts (1–2 calls) can make share features extreme; we handled this by using medians + mean/%-any checks for timing.\n\n\n\n\n\n\n10.6 Optional: export portfolio artifacts (OFF by default)\n\n\nCode\nSAVE_FILES = False  # keep False unless you explicitly want exports\n\nif SAVE_FILES:\n    # These contain NO names/phone numbers. contact_id stays hashed.\n    final_segments.to_csv(\"final_segments_cluster_cards.csv\", index=False)\n    rules.to_csv(\"segment_policy_rules.csv\", index=False)\n    findings_table.to_csv(\"segment_findings_summary.csv\", index=False)\n    exec_summary.to_csv(\"executive_summary_metrics.csv\", index=False)\n\n    print(\"Saved CSV files (hashed IDs only, no raw identifiers).\")\nelse:\n    print(\"SAVE_FILES=False (no files written).\")\n\n\nSAVE_FILES=False (no files written).",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "walsoft_contact_segmentation.html#phase-10-final-wrap-up-competition-framing-decision-narrative",
    "href": "walsoft_contact_segmentation.html#phase-10-final-wrap-up-competition-framing-decision-narrative",
    "title": "Walsoft Decision-driven contact segmentation",
    "section": "11 Phase 10 — Final wrap-up (competition framing + decision narrative)",
    "text": "11 Phase 10 — Final wrap-up (competition framing + decision narrative)\n\n11.1 Competition prompt (what we were given)\nWe were given a historical phone-call behaviour dataset where each record represents a call event.\nThe dataset includes timestamps, call duration, and categorised call context, but no business labels such as “good customer” or “bad customer”.\nTask: Without supervision (no labels), we had to build a contact-level segmentation that a business could actually use for prioritisation, outreach cadence, and relationship management.\nHard constraints:\n\nPrivacy-first: do not use names or phone numbers; keep identifiers hashed.\nOperational: output must translate into clear segment actions, not just clusters.\nProfessional robustness: results must be stable, not a “nice plot”.\n\n\n\n\n11.2 What we had to produce (deliverable definition)\nWe had to deliver:\n\nA clean contact-level feature table (one row per contact).\nA distance-ready clustering matrix (robust preprocessing).\nA final segmentation using K-means with \\(K=4\\).\nStakeholder-ready interpretation (cluster “cards” + segment labels + default actions).\nRobustness evidence (seed stability and feature sensitivity).\nInterpretation-only visual sanity-check (PCA map).\n\n\n\n\n11.3 What we built (end-to-end pipeline)\n\nAudit the raw call-event data (types, missingness, constraints).\nAggregate to contacts (the clustering unit), producing features that capture:\n\nvolume/intensity (calls, active days),\nrelationship depth proxy (duration metrics, long-call share),\nrecency/tenure (dormancy vs continuity),\ntiming mix (business hours vs off-hours).\n\nPreprocess for K-means:\n\nclip extreme values in raw space,\nmedian imputation,\nrobust scaling,\ncap scaled values for numerical stability.\n\nFit K-means with \\(K=4\\) and attach labels to contacts (hashed IDs only).\nInterpret clusters using raw units and translate into segments + actions.\nValidate robustness using ARI across seeds and drop-one-block sensitivity.\nUse PCA only for visualization (interpretation, not training).\n\n\n\n\n11.4 The segments (what we found + what to do)\nFrom the cluster cards and behaviour-based mapping, we obtained four operational segments:\n\nOne-off / low-engagement contacts (largest share)\nAction: deprioritize by default; automate nurture; reactivate only if new activity appears.\nWarm occasional contacts\nAction: nurture cadence (monthly/quarterly); structured check-ins to increase engagement.\nCore active relationships\nAction: retention and priority servicing; assign owner; proactive follow-ups.\nRare deep conversations\nAction: high-touch when active; preserve context; personalized re-engagement when dormant.\n\n\n\n\n11.5 Proof the solution “holds up” (robustness)\nSeed stability: ARI was \\(1.00\\) across all tested seeds.\nSo the segmentation is fully stable to random initialization (up to label permutation).\nFeature sensitivity: dropping duration or recency reduces agreement (ARI drops to about \\(0.70\\)–\\(0.74\\)).\nInterpretation: duration and recency carry meaningful segmentation signal; timing contributes less to the final partition.\n\n\n\n11.6 What PCA contributed (interpretation only)\nPCA is a 2D projection of prep[\"X\"] for storytelling and sanity-checking.\n\nThe first two components explain about \\(84.45\\%\\) of variance.\nThe map shows visible structure consistent with the segmentation.\nPCA overlap does not invalidate clusters because K-means was trained in the full feature space.\n\n\n\n\n11.7 Decision use: what a business can do with these segments\n\nRouting: prioritize “Core active” + “Rare deep” to high-touch handling.\nCadence: automate outreach for “Warm occasional”; suppress manual effort for “One-off”.\nReactivation: trigger win-back campaigns for dormant “Rare deep” and “Warm occasional”.\nService levels: define SLA tiers using segment label and recency.\n\n\n\n\n11.8 Limitations (how not to misuse this)\n\nThis is behaviour segmentation, not identity inference and not causal.\nK-means prefers spherical separation in feature space; alternative methods can be tested later.\nVery sparse contacts can make timing shares extreme; we mitigated via robust summaries and mean/%-any checks.\n\n\n\n\n\n\n\nNote\n\n\n\nFinal deliverables:\n\nA stable \\(K=4\\) segmentation (hashed IDs only).\nStakeholder-ready cluster cards + action policy rules.\nRobustness evidence (seed ARI + block sensitivity).\nPCA map used only for interpretation and storytelling.",
    "crumbs": [
      "Walsoft Contact Segmentation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Walsoft Contact-Level Clustering",
    "section": "",
    "text": "Segment contacts into actionable behavioral groups to support outreach planning, CRM prioritization, and relationship management.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#business-objective",
    "href": "index.html#business-objective",
    "title": "Walsoft Contact-Level Clustering",
    "section": "",
    "text": "Segment contacts into actionable behavioral groups to support outreach planning, CRM prioritization, and relationship management.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#what-changes-after-you-deploy-this-pipeline",
    "href": "index.html#what-changes-after-you-deploy-this-pipeline",
    "title": "Walsoft Contact-Level Clustering",
    "section": "2 What changes after you deploy this pipeline",
    "text": "2 What changes after you deploy this pipeline\n\nYou stop treating all contacts the same.\nYou get stable, interpretable contact segments that drive outreach cadence, prioritization, and risk flags.\nYou can justify communication decisions using measurable behavioral signals (call volume, duration, timing, recency, and gaps).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#how-this-project-is-structured",
    "href": "index.html#how-this-project-is-structured",
    "title": "Walsoft Contact-Level Clustering",
    "section": "3 How this project is structured",
    "text": "3 How this project is structured\nThis project is deliberately split into two layers:\n\nDecision layer (this site) — explains why the segmentation exists and how it supports real business decisions.\nAnalytical layer (next chapter) — shows the full, reproducible unsupervised learning pipeline, from raw call logs to validated contact clusters.\n\nOpen the chapter: Walsoft Contact Segmentation.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Walsoft Contact-Level Clustering",
    "section": "4 About the author",
    "text": "4 About the author\nPhilip Tambiti Leo Walekhwa\nData Scientist | Business Strategist | Entrepreneur | Educator\nThis project is part of a broader applied research and portfolio initiative focused on practical machine learning, analytics, and decision-support systems.\n\nProfile & research context: https://ai.walsoftcomputers.com/",
    "crumbs": [
      "Home"
    ]
  }
]