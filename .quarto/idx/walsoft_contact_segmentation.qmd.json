{"title":"Walsoft Decision-driven contact segmentation","markdown":{"yaml":{"title":"Walsoft Decision-driven contact segmentation","format":"html","execute":{"echo":true,"warning":false,"message":false}},"headingText":"Business objective","containsRefs":false,"markdown":"\n\n\nSegment contacts into **actionable behavioral groups** to support **outreach planning, CRM prioritization, and relationship management**.\n\n::: callout-important\n### Hard constraints enforced in this project\n\n**Time bands (exact):**\n\n- `business_hours`: 08:00–17:59  \n- `evening`: 18:00–20:59  \n- `late_night`: 21:00–23:59  \n- `early_morning`: 00:00–07:59  \n\n**Working days:**\n\n- Open days: Monday–Saturday  \n- Closed day: Sunday only\n:::\n\n::: callout-tip\n### Workflow (phases)\nPhase 1 Audit → Phase 2 Contact-level features → Phase 3 Preprocess → Phase 4 Choose K → Phase 5 Fit/Interpret → then stability/ARI → sensitivity (feature blocks) → PCA viz → final recommendations.\n:::\n\n## Phase 1 — Audit (call-level)\n\n### Setup (imports + toggles)\n\n```{python}\n#| echo: false\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport hashlib\nfrom collections import OrderedDict\n\n# -----------------------\n# GitHub safety toggles\n# -----------------------\nSAVE_FILES = False  # keep OFF by default\n\nDATA_URL = (\n    \"https://learn.walsoftcomputers.com/machine_learning/walsoft_phonecall/\"\n    \"original_dataset_with_instructions/walsoft_semi_categorized_phone_dataset.csv\"\n)\n\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 140)\n\n# -----------------------\n# Hard constraints (exact)\n# -----------------------\nTIME_BANDS = {\n    \"business_hours\": (8, 17),   # 08:00–17:59\n    \"evening\":        (18, 20),  # 18:00–20:59\n    \"late_night\":     (21, 23),  # 21:00–23:59\n    \"early_morning\":  (0, 7),    # 00:00–07:59\n}\n\nOPEN_DOW = set([0, 1, 2, 3, 4, 5])  # Mon..Sat\nCLOSED_DOW = set([6])              # Sun\n```\n\n### Load the dataset\n\n\n#### Load data\n\n```{python}\n# Load dataset (no assumptions, no output)\ndf = pd.read_csv(DATA_URL)\n```\n\n#### Dataset overview\n\n```{python}\n# Dataset overview (executive-level structural metrics)\n\ndataset_overview = pd.DataFrame({\n    \"Metric\": [\n        \"Rows\",\n        \"Columns\",\n        \"Total cells\",\n        \"Memory usage (MB)\"\n    ],\n    \"Value\": [\n        int(df.shape[0]),\n        int(df.shape[1]),\n        int(df.shape[0] * df.shape[1]),\n        f\"{df.memory_usage(deep=True).sum() / 1e6:.2f}\"\n    ]\n})\n\ndataset_overview\n```\n\n#### Column inventory (schema inspection)\n\n```{python}\n# Column inventory (schema audit table)\n\ncolumn_inventory = (\n    pd.DataFrame({\n        \"Column name\": df.columns,\n        \"Data type\": df.dtypes.astype(str),\n        \"Non-null count\": df.notna().sum().values,\n        \"Non-null rate\": (df.notna().mean()).round(3).values\n    })\n    .sort_values(\"Column name\")\n    .reset_index(drop=True)\n)\n\ncolumn_inventory\n```\n\n#### Sample records (visual sanity check)\n\n```{python}\n# Sample records (human sanity check only)\ndf.head(3)\n```\n\n\n\n---\n\n### Identify key columns (robust, from the data itself)\n\n```{python}\n# Identify key columns (schema audit, no assumptions)\n\n\n# ------------------------------------------------------------------\n# Normalise column names once (single source of truth)\n# ------------------------------------------------------------------\ncol_map = {c: c.lower().strip() for c in df.columns}\n\n# ------------------------------------------------------------------\n# Detection rules (explicit, auditable, extensible)\n# ------------------------------------------------------------------\nDETECTION_RULES = OrderedDict({\n    \"Date\": {\n        \"description\": \"Calendar date component\",\n        \"match_any\": [\"date\"]\n    },\n    \"Time\": {\n        \"description\": \"Clock time component\",\n        \"match_exact_or_contains\": [\"time\"]\n    },\n    \"Duration\": {\n        \"description\": \"Call duration (numeric, any unit)\",\n        \"match_any\": [\"duration\", \"seconds\", \"secs\", \"sec\", \"minutes\", \"mins\"]\n    },\n    \"Identifier\": {\n        \"description\": \"Contact or phone identifier\",\n        \"match_any\": [\"dialled\", \"dialed\", \"phone\", \"number\", \"contact\"]\n    },\n    \"Category\": {\n        \"description\": \"Human or system call classification\",\n        \"match_any\": [\"category\"]\n    }\n})\n\n# ------------------------------------------------------------------\n# Apply detection rules\n# ------------------------------------------------------------------\nrecords = []\n\nfor field, rule in DETECTION_RULES.items():\n    detected = []\n\n    for col, lc in col_map.items():\n        if \"match_exact_or_contains\" in rule:\n            if any(lc == k or k in lc for k in rule[\"match_exact_or_contains\"]):\n                detected.append(col)\n        elif \"match_any\" in rule:\n            if any(k in lc for k in rule[\"match_any\"]):\n                detected.append(col)\n\n    records.append({\n        \"Field type\": field,\n        \"Purpose\": rule[\"description\"],\n        \"Detected columns\": \", \".join(detected) if detected else \"—\",\n        \"Count\": len(detected),\n        \"Status\": \"OK\" if detected else \"Missing\"\n    })\n\n# ------------------------------------------------------------------\n# Canonical schema audit table (Quarto renders automatically)\n# ------------------------------------------------------------------\nschema_audit = pd.DataFrame(records)\n\nschema_audit\n```\n\n### Build canonical timestamp and numeric duration (audit-quality checks)\n\n\n#### Validate required schema\n\n```{python}\n# Validate required columns exist (hard audit gate)\n\nrequired_cols = [\n    \"date_stamp\",\n    \"time\",\n    \"duration_in_seconds\",\n    \"dialled_phone_number\"\n]\n\nmissing_required = [c for c in required_cols if c not in df.columns]\n\nschema_validation = pd.DataFrame({\n    \"Required column\": required_cols,\n    \"Present in dataset\": [c in df.columns for c in required_cols]\n})\n\nschema_validation\n```\n\n```{python}\n# Stop execution if schema is invalid\nassert len(missing_required) == 0, f\"Missing required columns: {missing_required}\"\n```\n\n---\n\n#### Build canonical fields\n\n```{python}\n# Canonical timestamp\ndf[\"call_ts\"] = pd.to_datetime(\n    df[\"date_stamp\"].astype(str).str.strip() + \" \" +\n    df[\"time\"].astype(str).str.strip(),\n    errors=\"coerce\"\n)\n\n# Canonical duration (seconds)\ndf[\"dur_sec\"] = pd.to_numeric(\n    df[\"duration_in_seconds\"],\n    errors=\"coerce\"\n)\n```\n\n---\n\n#### Data integrity audit\n\n```{python}\nbad_ts = int(df[\"call_ts\"].isna().sum())\nbad_dur = int(df[\"dur_sec\"].isna().sum())\nnegative_dur = int((df[\"dur_sec\"] < 0).sum())\n\ntimestamp_min = df[\"call_ts\"].min()\ntimestamp_max = df[\"call_ts\"].max()\n\nintegrity_audit = pd.DataFrame({\n    \"Check\": [\n        \"Timestamp parse failures\",\n        \"Duration numeric failures\",\n        \"Negative durations\",\n        \"Timestamp range (min)\",\n        \"Timestamp range (max)\",\n        \"Timestamp success rate\",\n        \"Duration success rate\"\n    ],\n    \"Value\": [\n        bad_ts,\n        bad_dur,\n        negative_dur,\n        str(timestamp_min),\n        str(timestamp_max),\n        f\"{df['call_ts'].notna().mean():.3f}\",\n        f\"{df['dur_sec'].notna().mean():.3f}\"\n    ]\n})\n\nintegrity_audit\n```\n\n---\n\n#### Hard integrity gates\n\n```{python}\nassert bad_ts == 0, \"Timestamp parsing failed for some rows.\"\nassert bad_dur == 0, \"Duration numeric conversion failed for some rows.\"\nassert negative_dur == 0, \"Negative durations detected.\"\n```\n\n\n\n### Hash a stable `contact_id` (do not export raw identifiers)\n\n```{python}\ndef stable_contact_id(x: int | str) -> str:\n    s = str(x).encode(\"utf-8\")\n    return hashlib.sha256(s).hexdigest()[:12]\n\ndf[\"contact_id\"] = df[\"dialled_phone_number\"].map(stable_contact_id)\n\nprint(\"Unique contacts (hashed):\", int(df[\"contact_id\"].nunique()))\nassert df[\"contact_id\"].isna().sum() == 0\n```\n\n### Derive time-band + open/closed-day flags (hard constraints)\n\n```{python}\ndf[\"hour\"] = df[\"call_ts\"].dt.hour\ndf[\"dow\"] = df[\"call_ts\"].dt.dayofweek  # Mon=0..Sun=6\n\ndef assign_time_band(hour: int) -> str:\n    if TIME_BANDS[\"early_morning\"][0] <= hour <= TIME_BANDS[\"early_morning\"][1]:\n        return \"early_morning\"\n    if TIME_BANDS[\"business_hours\"][0] <= hour <= TIME_BANDS[\"business_hours\"][1]:\n        return \"business_hours\"\n    if TIME_BANDS[\"evening\"][0] <= hour <= TIME_BANDS[\"evening\"][1]:\n        return \"evening\"\n    if TIME_BANDS[\"late_night\"][0] <= hour <= TIME_BANDS[\"late_night\"][1]:\n        return \"late_night\"\n    raise ValueError(f\"Hour out of range: {hour}\")\n\ndf[\"time_band\"] = df[\"hour\"].map(assign_time_band)\n\ndf[\"is_open_day\"] = df[\"dow\"].isin(OPEN_DOW).astype(int)\ndf[\"is_closed_day\"] = df[\"dow\"].isin(CLOSED_DOW).astype(int)\n\n# sanity checks: mutually exclusive and exhaustive\nassert set(df[\"time_band\"].unique()) == {\"early_morning\", \"business_hours\", \"evening\", \"late_night\"}\nassert int((df[\"is_open_day\"] + df[\"is_closed_day\"]).min()) == 1\nassert int((df[\"is_open_day\"] + df[\"is_closed_day\"]).max()) == 1\n\ndf[[\"call_ts\",\"hour\",\"dow\",\"time_band\",\"is_open_day\",\"is_closed_day\"]].head(5)\n```\n\n### Executive summary table (stakeholder-ready)\n\n```{python}\ncat_col = \"category\" if \"category\" in df.columns else None\n\nsummary = pd.DataFrame({\n    \"Metric\": [\n        \"Rows (calls)\",\n        \"Columns\",\n        \"Unique contacts (hashed)\",\n        \"Timestamp parsable rate\",\n        \"Timestamp range (min)\",\n        \"Timestamp range (max)\",\n        \"Duration numeric rate\",\n        \"Duplicate rows\",\n        \"Category column present?\",\n        \"Category non-null rate (if present)\",\n        \"Data readiness score (ts & dur)\"\n    ],\n    \"Value\": [\n        int(df.shape[0]),\n        int(df.shape[1]),\n        int(df[\"contact_id\"].nunique()),\n        f\"{df['call_ts'].notna().mean():.3f}\",\n        str(df[\"call_ts\"].min()),\n        str(df[\"call_ts\"].max()),\n        f\"{df['dur_sec'].notna().mean():.3f}\",\n        int(df.duplicated().sum()),\n        bool(cat_col is not None),\n        f\"{df[cat_col].notna().mean():.3f}\" if cat_col else \"N/A\",\n        f\"{(df['call_ts'].notna() & df['dur_sec'].notna()).mean():.3f}\",\n    ]\n})\nsummary\n```\n\n### Visual 1 — Monthly call volume (coverage)\n\n```{python}\n#| echo: false\nmonthly = (\n    df.assign(month_id=lambda x: x[\"call_ts\"].dt.to_period(\"M\").astype(str))\n      .groupby(\"month_id\")\n      .size()\n      .reset_index(name=\"n_calls\")\n)\n\nplt.figure()\nplt.plot(monthly[\"month_id\"], monthly[\"n_calls\"])\nplt.xticks(rotation=90)\nplt.title(\"Monthly call volume (data coverage)\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Number of calls\")\nplt.tight_layout()\nplt.show()\n\nmonthly.tail(12)\n```\n\n### Visual 2 — Duration distribution (outlier awareness)\n\n```{python}\n#| echo: false\nplt.figure()\nplt.hist(df[\"dur_sec\"], bins=60)\nplt.title(\"Call duration distribution (seconds)\")\nplt.xlabel(\"Duration (seconds)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\nplt.figure()\nplt.boxplot(df[\"dur_sec\"], vert=True)\nplt.title(\"Call duration boxplot (seconds)\")\nplt.ylabel(\"Duration (seconds)\")\nplt.tight_layout()\nplt.show()\n\ndf[\"dur_sec\"].quantile([0, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1.0]).to_frame(\"duration_seconds\")\n```\n\n### Visual 3 — Time-band mix + open/closed-day mix (hard-constraint diagnostics)\n\n```{python}\n#| echo: false\nband_counts = df[\"time_band\"].value_counts().sort_index()\nday_counts = df[\"is_closed_day\"].map({0: \"Open day (Mon–Sat)\", 1: \"Closed day (Sun)\"}).value_counts()\n\nplt.figure()\nplt.bar(band_counts.index.astype(str), band_counts.values)\nplt.title(\"Calls by time band (hard constraints)\")\nplt.xlabel(\"Time band\")\nplt.ylabel(\"Number of calls\")\nplt.tight_layout()\nplt.show()\n\nplt.figure()\nplt.bar(day_counts.index.astype(str), day_counts.values)\nplt.title(\"Calls on open vs closed days (hard constraint)\")\nplt.xlabel(\"Day type\")\nplt.ylabel(\"Number of calls\")\nplt.tight_layout()\nplt.show()\n\nband_counts.to_frame(\"n_calls\"), day_counts.to_frame(\"n_calls\")\n```\n\n---\n\n\n::: callout-note\n## Phase 1 — Audit summary (stakeholder-ready)\n\n**Dataset health (ready for segmentation):**\n\n- **24,952 call records** across **2,091 unique contacts** (hashed).\n- **Time coverage:** 2022-01-01 → 2024-10-04 (timestamps parse perfectly; durations fully numeric).\n- **No duplicates** detected; **category is fully populated**.\n- **Data readiness score = 1.000** (timestamp + duration fully usable).\n\n**Behavioral signals visible already:**\n\n- Calls concentrate in **business hours** (20,624 / 24,952 ≈ 82.7%).\n- Smaller but meaningful activity in **evening** (2,877 ≈ 11.5%) and **early morning** (1,071 ≈ 4.3%); **late night** is rare (380 ≈ 1.5%).\n- Most calls happen on **open days (Mon–Sat)** (23,300 ≈ 93.4%); **Sunday** exists but is limited (1,652 ≈ 6.6%).\n\n**Duration profile (important for feature design):**\n\n- Typical call is short: median **34s**, 75th percentile **84s**.\n- Heavy tail: 99th percentile **~1,274s**, max **7,200s** (2 hours) → we will use **robust statistics** (median, IQR, log transforms, and long-call share) instead of relying only on means.\n\n**Coverage caution (modeling hygiene):**\n\n- Monthly volume is broadly stable until mid-2024, then shows sharp drops in some late months.\n- We will treat this as **coverage/behavior regime change** and rely on **contact-level aggregates + recency** (not month-level trends) for clustering.\n\n### Transition to Phase 2\nNext, we convert the call-level table into a **contact-level feature table** (1 row per contact).  \nThis is the modeling dataset for clustering.\n:::\n\n---\n\n## Phase 2 — Contact-level features\n\nThis phase transforms the **call-level dataset** into a **contact-level feature table**, producing **one row per contact**. These features capture activity intensity, temporal patterns, and category distributions, forming the basis for clustering and behavioral segmentation.\n\n---\n\n### Guardrails & Setup\n\nWe first ensure all required columns from Phase 1 exist. This prevents accidental execution on incomplete data.\n\n```{python}\n# Guardrails: ensure critical columns exist\nassert \"contact_id\" in df.columns, \"contact_id missing (run Phase 1 first).\"\nassert \"call_ts\" in df.columns, \"call_ts missing (run Phase 1 first).\"\nassert \"dur_sec\" in df.columns, \"dur_sec missing (run Phase 1 first).\"\nassert \"time_band\" in df.columns, \"time_band missing (run Phase 1 first).\"\nassert \"is_open_day\" in df.columns and \"is_closed_day\" in df.columns, \"open/closed flags missing.\"\n```\n\nHelper for safe division (avoids division by zero):\n\n```{python}\ndef safe_div(a, b):\n    return np.where(b == 0, 0.0, a / b)\n```\n\nReference timestamp for recency calculations:\n\n```{python}\nASOF_TS = df[\"call_ts\"].max()\n```\n\n---\n\n### Base Aggregations\n\nCompute **core contact-level metrics**: total calls, active days, duration statistics, and recency/tenure measures.\n\n```{python}\ng = df.groupby(\"contact_id\", as_index=False)\n\nbase = g.agg(\n    n_calls=(\"call_ts\", \"size\"),\n    n_days_active=(\"call_ts\", lambda x: x.dt.date.nunique()),\n    first_call_ts=(\"call_ts\", \"min\"),\n    last_call_ts=(\"call_ts\", \"max\"),\n    total_dur_sec=(\"dur_sec\", \"sum\"),\n    mean_dur_sec=(\"dur_sec\", \"mean\"),\n    median_dur_sec=(\"dur_sec\", \"median\"),\n    p90_dur_sec=(\"dur_sec\", lambda x: x.quantile(0.90)),\n)\n\n# Recency and tenure (days)\nbase[\"recency_days\"] = (ASOF_TS - base[\"last_call_ts\"]).dt.total_seconds() / (24 * 3600)\nbase[\"tenure_days\"] = (base[\"last_call_ts\"] - base[\"first_call_ts\"]).dt.total_seconds() / (24 * 3600)\n\n# Call rates\nbase[\"calls_per_active_day\"] = safe_div(base[\"n_calls\"], base[\"n_days_active\"])\nbase[\"calls_per_tenure_day\"] = safe_div(base[\"n_calls\"], (base[\"tenure_days\"] + 1.0))\n\n# Log transforms to stabilize heavy-tailed distributions\nbase[\"log_n_calls\"] = np.log1p(base[\"n_calls\"])\nbase[\"log_total_dur\"] = np.log1p(base[\"total_dur_sec\"])\nbase[\"log_median_dur\"] = np.log1p(base[\"median_dur_sec\"])\n```\n\n---\n\n### Long-call Share\n\nDefine **long calls** as those exceeding the 95th percentile. Compute each contact's share of long calls to capture **tail behavior**.\n\n```{python}\nLONG_CALL_THRESHOLD = float(df[\"dur_sec\"].quantile(0.95))\ndf[\"_is_long_call\"] = (df[\"dur_sec\"] >= LONG_CALL_THRESHOLD).astype(int)\n\nlong_share = (\n    df.groupby(\"contact_id\", as_index=False)\n      .agg(long_call_share=(\"_is_long_call\", \"mean\"))\n)\n```\n\n---\n\n### Time-Band Mix\n\nContacts may have different temporal activity patterns. We compute **proportions of calls per time band** (business hours, evening, late night, early morning).\n\n```{python}\nband_mix = (\n    pd.crosstab(df[\"contact_id\"], df[\"time_band\"], normalize=\"index\")\n      .reset_index()\n      .rename(columns={\n          \"business_hours\": \"share_business_hours\",\n          \"evening\": \"share_evening\",\n          \"late_night\": \"share_late_night\",\n          \"early_morning\": \"share_early_morning\",\n      })\n)\n\n# Ensure all expected columns exist\nfor col in [\"share_business_hours\",\"share_evening\",\"share_late_night\",\"share_early_morning\"]:\n    if col not in band_mix.columns:\n        band_mix[col] = 0.0\n```\n\n---\n\n### Open vs Closed Day Mix\n\nCapture **day-of-week activity patterns**. Open vs. closed days are mutually exclusive and sum to 1 per contact.\n\n```{python}\nday_mix = (\n    df.groupby(\"contact_id\", as_index=False)\n      .agg(\n          share_open_day=(\"is_open_day\", \"mean\"),\n          share_closed_day=(\"is_closed_day\", \"mean\")\n      )\n)\n\n# Sanity check\nday_mix[\"_sum\"] = day_mix[\"share_open_day\"] + day_mix[\"share_closed_day\"]\nassert np.allclose(day_mix[\"_sum\"], 1.0, atol=1e-9), \"Open+Closed shares not summing to 1.\"\nday_mix = day_mix.drop(columns=[\"_sum\"])\n```\n\n---\n\n### Category Mix (Optional)\n\nFor interpretation, compute the **top-K category proportions per contact**, preserving sparsity and numeric representation.\n\n```{python}\nTOPK = 8\ntop_categories = df[\"category\"].value_counts().head(TOPK).index.tolist()\n\ncat_tab = (\n    pd.crosstab(df[\"contact_id\"], df[\"category\"])\n      .reindex(columns=top_categories, fill_value=0)\n)\n\n# Convert to proportions\ncat_mix = (cat_tab.div(cat_tab.sum(axis=1).replace(0, 1), axis=0)\n                 .reset_index()\n                 .rename(columns={c: f\"share_cat_{c}\" for c in top_categories}))\n```\n\n---\n\n### Merge All Features\n\nCombine **base, long-call, time-band, day-mix, and category features** into a single contact-level table.\n\n```{python}\ncontact_features = (\n    base.merge(long_share, on=\"contact_id\", how=\"left\")\n        .merge(band_mix, on=\"contact_id\", how=\"left\")\n        .merge(day_mix, on=\"contact_id\", how=\"left\")\n        .merge(cat_mix, on=\"contact_id\", how=\"left\")\n)\n\n# Fill any leftover NaNs\nnum_cols = contact_features.select_dtypes(include=[np.number]).columns\ncontact_features[num_cols] = contact_features[num_cols].fillna(0.0)\n\n# Preview\nprint(\"contact_features.shape =\", contact_features.shape)\nprint(\"Long-call threshold (95th pct, seconds) =\", LONG_CALL_THRESHOLD)\n\ncontact_features.head(5).T\n```\n\n---\n\n### Quick Feature Audit\n\nSanity check **feature completeness and distribution**.\n\n```{python}\nfeature_audit = pd.DataFrame({\n    \"n_contacts\": [int(contact_features.shape[0])],\n    \"n_features_total\": [int(contact_features.shape[1])],\n    \"any_missing_numeric\": [bool(contact_features.select_dtypes(include=[np.number]).isna().any().any())],\n    \"min_calls\": [int(contact_features[\"n_calls\"].min())],\n    \"median_calls\": [float(contact_features[\"n_calls\"].median())],\n    \"max_calls\": [int(contact_features[\"n_calls\"].max())],\n    \"median_recency_days\": [float(contact_features[\"recency_days\"].median())],\n})\nfeature_audit\n```\n\n---\n\n### Top Activity Contacts\n\nInspect **most active contacts** without exposing raw identifiers. This helps validate engineered features and distribution of behavioral metrics.\n\n```{python}\ntop_activity = contact_features.sort_values(\"n_calls\", ascending=False).head(10)[\n    [\"contact_id\",\"n_calls\",\"n_days_active\",\"total_dur_sec\",\"median_dur_sec\",\"recency_days\",\n     \"share_business_hours\",\"share_evening\",\"share_early_morning\",\"share_late_night\",\n     \"share_open_day\",\"share_closed_day\",\"long_call_share\"]\n]\ntop_activity.T\n```\n\n---\n\n::: callout-note\n## Phase 2 — Stakeholder-ready summary (what we learned)\n\nWe successfully converted **24,952 calls** into a **contact-level modeling table** with **2,091 contacts** and **28 features**.\n\n### What the Phase 2 feature table represents (business meaning)\nEach row is a **contact** (hashed ID), and the features capture:\n\n- **Volume / intensity:** `n_calls`, `n_days_active`, `calls_per_active_day`, `calls_per_tenure_day`\n- **Relationship depth / effort proxy:** `total_dur_sec`, `median_dur_sec`, `p90_dur_sec`, plus log versions\n- **Recency / dormancy risk:** `recency_days` (how long since last call)\n- **Time preference / accessibility:** shares by time band (business/evening/early/late)\n- **Open vs closed day behavior:** `share_open_day`, `share_closed_day`\n- **Tail behavior:** `long_call_share` where “long” is **≥ 396s** (95th percentile)\n- **Category mix (interpretation aid):** proportions of top-8 categories per contact\n\n### Sanity checks passed (safe to proceed)\n- No missing numeric values in engineered features.\n- Open + closed day shares sum to 1 per contact (validated).\n- Time-band shares exist for every contact (columns ensured even if 0).\n\n### What the results already hint at (without clustering yet)\n- **Highly skewed activity:** median contact has **2 calls**, but the top contact has **2,413** calls → we must **scale/transform** before K-means.\n- **Recency is large for many contacts:** median `recency_days ≈ 445` → many contacts are dormant; segmentation must separate “active” vs “inactive”.\n- Top activity contacts show diverse patterns:\n  - some are high-volume but mostly business hours\n  - some show meaningful evening/early activity\n  - some have high `long_call_share` (deep conversations or issue resolution)\n\n### Modeling implication\n\nAt this point we have a **contact-level feature table** with strong business meaning (volume, duration, recency, timing).  \nBefore clustering, we must build a **clean clustering matrix** $X$ so distance-based algorithms behave correctly.\n\nWe will build $X$ such that it:\n\n1) **Excludes non-model columns**  \n   - No timestamps (`first_call_ts`, `last_call_ts`) and no raw identifiers.  \n   - Category mix is kept for interpretation, but excluded from default training to avoid “baking in labels”.\n\n2) **Controls heavy tails and near-constant features**  \n   - Phone-call behavior is highly skewed (a few contacts dominate calls/duration).  \n   - We apply robust transforms already engineered (log features) and use a *robust preprocessing pipeline*.  \n   - We also drop **near-constant (IQR≈0) features** to avoid unstable scaling and distance domination.\n\n3) **Scales fairly for distance-based clustering**  \n   - We use **median imputation** (robust, future-proof if new features introduce missingness).  \n   - We use **RobustScaler (median/IQR)** instead of StandardScaler to reduce outlier influence.\n\n---\n\n### Transition\n\nPhase 3 converts `contact_features` into **clustering-ready inputs** and produces:\n\n- a **feature-block dictionary** for later sensitivity tests (volume vs duration vs timing vs recency),\n- a cleaned, scaled matrix **`X`** and aligned **`feature_names`** for reproducibility and interpretation.\n:::\n\n---\n\n\n## Phase 3 — Preprocess (bullet-proof, distance-based clustering ready)\n\nThis phase prepares the **contact-level feature table** for distance-based clustering by producing a stable numeric matrix **`X`** (rows = contacts, columns = standardized features).\n\nWe do four things:\n\n1) **Define feature blocks** (auditable and reusable for sensitivity tests later)  \n2) **Build the default model matrix** (exclude timestamps and category mix by default)  \n3) **Stabilize heavy tails** (clip extreme values in raw space)  \n4) **Impute + robust-scale** (median imputation + RobustScaler) with diagnostics\n\n::: callout-tip\n### Why RobustScaler (not StandardScaler)\nContact behaviour is heavy-tailed (a few contacts dominate activity).  \nDistance-based clustering is scale-sensitive. **RobustScaler uses median and IQR**, reducing outlier influence and producing more stable distances.\n:::\n\n---\n\n### Freeze an interpretation copy (read-only view)\n\n```{python}\ncontact_features_view = contact_features.copy()\n```\n\n---\n\n### Define feature blocks (for interpretation and sensitivity analysis)\n\n```{python}\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n```\n\n```{python}\n# -----------------------------\n# Feature blocks (business meaning)\n# -----------------------------\nvolume_features = [\n    \"n_calls\", \"n_days_active\", \"calls_per_active_day\", \"calls_per_tenure_day\", \"log_n_calls\"\n]\n\nduration_features = [\n    \"total_dur_sec\", \"mean_dur_sec\", \"median_dur_sec\", \"p90_dur_sec\",\n    \"log_total_dur\", \"log_median_dur\", \"long_call_share\"\n]\n\nrecency_features = [\n    \"recency_days\", \"tenure_days\"\n]\n\ntiming_features = [\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\n# Category mix is useful for interpretation, but excluded from default model training\ncategory_features = [c for c in contact_features.columns if c.startswith(\"share_cat_\")]\n\nFEATURE_BLOCKS = {\n    \"volume\": volume_features,\n    \"duration\": duration_features,\n    \"recency\": recency_features,\n    \"timing\": timing_features,\n    \"category\": category_features\n}\n\n# Validate engineered feature presence (hard gate)\nmissing = {k: [f for f in v if f not in contact_features.columns] for k, v in FEATURE_BLOCKS.items()}\nmissing = {k: v for k, v in missing.items() if len(v) > 0}\nassert len(missing) == 0, f\"Missing engineered features: {missing}\"\n\nFEATURE_BLOCKS\n```\n\n---\n\n### Select default model features (no timestamps, no category mix by default)\n\nWe train clustering on behaviour (volume, duration, recency, timing).\nCategory mix stays available for interpretation and later sensitivity checks.\n\n```{python}\nMODEL_FEATURES = (\n    FEATURE_BLOCKS[\"volume\"]\n    + FEATURE_BLOCKS[\"duration\"]\n    + FEATURE_BLOCKS[\"recency\"]\n    + FEATURE_BLOCKS[\"timing\"]\n)\n\nX_raw = contact_features[MODEL_FEATURES].copy()\nprint(\"X_raw.shape =\", X_raw.shape)\n```\n\n---\n\n### Diagnostics before preprocessing (missingness + inf safety)\n\n```{python}\n# Missingness inspection (future-proof if new features introduce NaNs)\nmiss = X_raw.isna().sum().sort_values(ascending=False)\nmiss_table = miss[miss > 0].to_frame(\"n_missing\")\nmiss_table\n```\n\n```{python}\n# Inf / -Inf inspection (must not exist)\nhas_inf = np.isinf(X_raw.to_numpy(dtype=float)).any()\nprint(\"Any inf in X_raw:\", bool(has_inf))\nassert not has_inf, \"Infinite values detected in X_raw (must fix before scaling).\"\n```\n\n---\n\n### Stability strategy (recommended): drop only truly constant columns + clip extremes\n\n**Important correction:**\nDropping “low IQR” features is too aggressive in contact data (many contacts have 1–2 calls, which makes shares look constant).\nInstead we:\n\n* drop only **truly constant** columns (no information),\n* clip each feature to **p01–p99** in raw units (winsorization),\n* then apply RobustScaler.\n\n```{python}\n# A) Drop truly constant columns only (no information)\nnunique = X_raw.nunique(dropna=False)\nconstant_cols = nunique[nunique <= 1].index.tolist()\nprint(\"Truly-constant cols (drop):\", constant_cols)\n\nX_raw2 = X_raw.drop(columns=constant_cols) if constant_cols else X_raw.copy()\n\n# B) Clip extremes in RAW space to stabilize heavy tails\nCLIP_LO = 0.01\nCLIP_HI = 0.99\n\nclip_info = []\nX_clip = X_raw2.copy()\n\nfor col in X_clip.columns:\n    lo = float(X_clip[col].quantile(CLIP_LO))\n    hi = float(X_clip[col].quantile(CLIP_HI))\n    X_clip[col] = X_clip[col].clip(lower=lo, upper=hi)\n    clip_info.append({\"feature\": col, \"p01\": lo, \"p99\": hi})\n\nclip_table = pd.DataFrame(clip_info).sort_values(\"feature\").reset_index(drop=True)\n\nX_clip.shape, clip_table.head(10)\n```\n\n---\n\n### Median imputation + RobustScaler\n\n```{python}\n# Median imputation is robust (and future-proof for features that may introduce NaNs later)\nimputer = SimpleImputer(strategy=\"median\")\n\n# RobustScaler uses median/IQR to reduce outlier influence\nscaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n\nX_imputed = imputer.fit_transform(X_clip)\nX = scaler.fit_transform(X_imputed)\n\nprint(\"X.shape =\", X.shape)\nprint(\"Any NaN in X:\", bool(np.isnan(X).any()))\nassert not np.isnan(X).any(), \"NaNs remain after preprocessing (must fix).\"\n\nprint(\"max_abs_after_scaling =\", float(np.max(np.abs(X))))\n```\n\n---\n\n### Diagnostics: identify any dominating feature in scaled space\n\nIf one feature still dominates distances, we detect it explicitly.\n\n```{python}\nfeature_names = list(X_clip.columns)\nmax_abs_by_feature = np.max(np.abs(X), axis=0)\n\ndominance = (\n    pd.DataFrame({\n        \"feature\": feature_names,\n        \"max_abs_scaled\": max_abs_by_feature,\n        \"raw_p01\": clip_table.set_index(\"feature\").loc[feature_names, \"p01\"].values,\n        \"raw_p99\": clip_table.set_index(\"feature\").loc[feature_names, \"p99\"].values,\n        \"raw_iqr\": (X_raw2.quantile(0.75) - X_raw2.quantile(0.25)).reindex(feature_names).values,\n        \"n_unique\": X_raw2.nunique().reindex(feature_names).values\n    })\n    .sort_values(\"max_abs_scaled\", ascending=False)\n    .reset_index(drop=True)\n)\n\ndominance.head(12)\n```\n\n---\n\n### Preprocess audit (compact, decision-useful)\n\n```{python}\npreprocess_audit = pd.DataFrame({\n    \"n_contacts\": [int(X.shape[0])],\n    \"n_model_features\": [int(X.shape[1])],\n    \"n_constant_dropped\": [int(len(constant_cols))],\n    \"constant_dropped\": [\", \".join(constant_cols) if constant_cols else \"None\"],\n    \"clip_low_quantile\": [CLIP_LO],\n    \"clip_high_quantile\": [CLIP_HI],\n    \"n_features_with_missing_before\": [int((X_raw.isna().sum() > 0).sum())],\n    \"total_missing_cells_before\": [int(X_raw.isna().sum().sum())],\n    \"max_abs_after_scaling\": [float(np.max(np.abs(X)))],\n})\n\npreprocess_audit\n```\n\n---\n\n### Store preprocessing artifacts (in-memory, safe by default)\n\n```{python}\nprep = {\n    \"MODEL_FEATURES\": MODEL_FEATURES,\n    \"FEATURE_BLOCKS\": FEATURE_BLOCKS,\n    \"feature_names\": feature_names,\n    \"imputer\": imputer,\n    \"scaler\": scaler,\n    \"X_raw\": X_raw2,      # before clipping (audit)\n    \"X_clip\": X_clip,     # clipped raw matrix (audit)\n    \"X\": X,               # final clustering matrix\n    \"clip_table\": clip_table,\n    \"dominance\": dominance,\n    \"preprocess_audit\": preprocess_audit\n}\n\n# Raw-unit summary (post-clip = what the scaler actually sees)\nraw_summary = X_clip.describe(percentiles=[0.5, 0.9, 0.95, 0.99]).T\nraw_summary = raw_summary[[\"count\", \"mean\", \"std\", \"min\", \"50%\", \"90%\", \"95%\", \"99%\", \"max\"]]\nraw_summary.head(12)\n```\n\n\n\n---\n\n\n### A Final numeric stability guardrail\n\nEven after robust scaling, distance-based clustering can still be dominated by a small number of extreme points.  \nTo make the pipeline **bullet-proof**, we apply a final cap in standardized space so no single feature can overwhelm Euclidean distances.\n\n```{python}\n# Final stability cap in scaled space (portfolio-grade guardrail)\nSCALED_CAP = 10.0\n\nX_uncapped = prep[\"X\"]\nX_capped = np.clip(X_uncapped, -SCALED_CAP, SCALED_CAP)\n\ncap_audit = pd.DataFrame({\n    \"scaled_cap\": [SCALED_CAP],\n    \"max_abs_before\": [float(np.max(np.abs(X_uncapped)))],\n    \"max_abs_after\": [float(np.max(np.abs(X_capped)))],\n})\n\n# Update the matrix used downstream (keep uncapped for audit)\nprep[\"X_uncapped\"] = X_uncapped\nprep[\"X\"] = X_capped\ncap_audit\n```\n\n---\n\n\n::: callout-note\n## Phase 3 — Sign-off (ready for K-selection)\n\nWe produced a stable clustering matrix `prep[\"X\"]` for **2,091 contacts × 20 features**.\n\n**Robustness guarantees:**\n- No missing values or infinities\n- Heavy tails controlled by **p01–p99 clipping**\n- Distance stability guaranteed by **post-scaling cap** (max absolute value ≤ 10)\n\n**Next:** Phase 4 chooses the number of clusters $K$ using a multi-metric selection protocol (inertia, silhouette, Davies–Bouldin) on `prep[\"X\"]`.\n:::\n\n\n\n## Phase 4 — Choose $K$ (model selection for clustering)\n\nChoosing $K$ is a **model selection** problem: different values of $K$ define different segmentations.\nWe evaluate candidate $K$ values using three complementary criteria:\n\n- **Inertia (SSE)**: decreases with $K$; we look for an “elbow” (diminishing returns).\n- **Silhouette**: higher is better; measures separation vs cohesion.\n- **Davies–Bouldin**: lower is better; penalizes overlapping clusters.\n\n::: callout-important\nWe compute metrics on the **preprocessed matrix** `prep[\"X\"]` (robust-scaled and capped).  \nThis ensures the $K$ decision reflects behavioural structure rather than raw-unit dominance.\n:::\n\n---\n\n### Compute $K$-search metrics\n\n```{python}\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\n\nX = prep[\"X\"]  # capped, clustering-ready matrix\n\n# Candidate K range (employer-ready default)\nK_MIN, K_MAX = 2, 12\n\nrows = []\nfor k in range(K_MIN, K_MAX + 1):\n    km = KMeans(\n        n_clusters=k,\n        n_init=50,         # more restarts = more reliable\n        random_state=42\n    )\n    labels = km.fit_predict(X)\n\n    inertia = float(km.inertia_)\n    sil = float(silhouette_score(X, labels))\n    db = float(davies_bouldin_score(X, labels))\n\n    rows.append({\n        \"k\": k,\n        \"inertia\": inertia,\n        \"silhouette\": sil,\n        \"davies_bouldin\": db\n    })\n\nk_metrics = pd.DataFrame(rows)\n\n# Add a simple \"elbow help\": marginal gain in inertia\nk_metrics[\"inertia_drop\"] = k_metrics[\"inertia\"].shift(1) - k_metrics[\"inertia\"]\nk_metrics[\"inertia_drop_pct\"] = (k_metrics[\"inertia_drop\"] / k_metrics[\"inertia\"].shift(1)) * 100\n\nk_metrics\n```\n\n---\n\n### Visualize metrics (elbow + quality trade-offs)\n\n```{python}\n#| echo: false\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.plot(k_metrics[\"k\"], k_metrics[\"inertia\"], marker=\"o\")\nplt.title(\"Elbow plot: inertia vs K\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Inertia (SSE)\")\nplt.tight_layout()\nplt.show()\n\nplt.figure()\nplt.plot(k_metrics[\"k\"], k_metrics[\"silhouette\"], marker=\"o\")\nplt.title(\"Silhouette vs K (higher is better)\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Silhouette\")\nplt.tight_layout()\nplt.show()\n\nplt.figure()\nplt.plot(k_metrics[\"k\"], k_metrics[\"davies_bouldin\"], marker=\"o\")\nplt.title(\"Davies–Bouldin vs K (lower is better)\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Davies–Bouldin\")\nplt.tight_layout()\nplt.show()\n```\n\n---\n\n### Candidate shortlist (data-driven)\n\nWe shortlist $K$ values that are plausible trade-offs:\n\n* among the **top silhouette** scores,\n* among the **lowest Davies–Bouldin** scores,\n* and not far beyond the elbow (where inertia gains flatten).\n\n\n```{python}\n# ----------------------------------------\n# Candidate shortlist — modern, auditable\n# ----------------------------------------\n\n# Identify top-K by each metric\ntop_sil = k_metrics.nlargest(5, \"silhouette\").copy()\ntop_sil[\"reason\"] = \"top_silhouette\"\n\nlow_db = k_metrics.nsmallest(5, \"davies_bouldin\").copy()\nlow_db[\"reason\"] = \"low_davies_bouldin\"\n\n# Combine and mark duplicates\nshortlist = pd.concat([top_sil, low_db]).reset_index(drop=True)\n\n# If a K appears in both, update reason\nshortlist = (\n    shortlist.groupby(\"k\", as_index=False)\n    .agg({\n        \"silhouette\": \"first\",\n        \"davies_bouldin\": \"first\",\n        \"inertia\": \"first\",\n        \"reason\": lambda x: \" & \".join(sorted(set(x)))\n    })\n    .sort_values(\"k\")\n    .reset_index(drop=True)\n)\n\n# Round metrics for readability\nshortlist[[\"silhouette\", \"davies_bouldin\", \"inertia\"]] = shortlist[\n    [\"silhouette\", \"davies_bouldin\", \"inertia\"]\n].round(4)\n\nshortlist\n\n```\n\n\n---\n\n\n\n### What the metrics say (from first principles)\n\n#### Inertia (elbow)\nHuge drops from **K=2 → 3** (~27%) and **3 → 4** (~30%).  \nAfter **K=4**, the drop collapses to ~12% and then <~11% thereafter.  \nThat’s a classic **elbow at K≈4**.\n\n#### Silhouette (separation vs cohesion)\n- **K=2:** 0.651 (very strong)  \n- **K=3:** 0.646 (very strong)  \n- **K=4:** 0.589 (still strong)  \n\nThen it falls sharply (e.g., **K=5** = 0.483 and continues down).  \nStructure is strongest for small K, especially **2–4**.\n\n#### Davies–Bouldin (overlap; lower is better)\n- Best is **K=2** (0.747), then **K=4** (0.772), then **K=3** (0.829).  \nAfter that, the metric worsens notably.\n\n---\n\n### Decision\nChoose **K=4** (recommended).  \n\n- **K=2:** too coarse for actionability (“low vs high activity”)  \n- **K=3:** better but still merges distinct behaviours (“steady frequent” vs “bursty intense”)  \n- **K=4:** lands at the elbow, with strong silhouette & Davies–Bouldin — best balance of interpretability, actionability, and metric support.\n\n::: callout-important\n## Decision: choose $K=4$\n\nWe select **$K=4$** as the operational segmentation because it balances:\n\n- **Structure quality:** strong silhouette (0.589) and low Davies–Bouldin (0.772)  \n- **Parsimony:** clear elbow around **K=4** (large inertia drops up to 4, then diminishing returns)  \n- **Actionability:** more useful than K=2 while avoiding over-fragmentation at higher K\n\n**Next:** fit K-means with **K=4** and interpret clusters using feature-block summaries (volume, duration, recency, timing).\n:::\n\n---\n\n### Optional: why we may explore $K=5$ (exploratory only)\n\n::: callout-warning\n$K=5$ is **not** recommended as the default operational segmentation in this project.\n\nCompared to $K=4$, separation quality declines noticeably:\n\n- Silhouette: **0.589 → 0.483** (weaker cohesion/separation)\n- Davies–Bouldin: **0.772 → 0.927** (more overlap between clusters)\n\nWe may still explore $K=5$ **only as an exploratory lens** when stakeholders want finer distinctions inside high-activity groups (e.g., separating “steady frequent” from “burst-intense”). Any $K=5$ results should be presented as *additional insight* without changing the primary framework.\n:::\n\n> **Stakeholder takeaway:** K=4 remains the primary segmentation. K=5 can be used **selectively** to provide additional insights where fine distinctions are important, without altering the main operational framework.\n\n---\n\n## Phase 5 — Fit and interpret clusters (K-means, $K=4$)\n\nWe fit K-means on the **clustering matrix** `prep[\"X\"]` (robust-scaled and capped for numeric stability).  \nFor interpretation, we summarise clusters back in **business units** using `contact_features_view` (raw-unit features) and translate clusters into **stakeholder-ready segments with actions**.\n\n::: callout-important\n### Training vs interpretation (do not mix these)\n- **Training:** uses `prep[\"X\"]` (scaled + capped) so Euclidean distances are fair and stable.\n- **Interpretation:** uses raw-unit features (calls, seconds, days, shares) so segments can be explained and acted on.\n- **Privacy:** we keep `contact_id` hashed and do not merge any name/number fields into report outputs.\n:::\n\n---\n\n### Fit K-means ($K=4$) and store labels\n\n```{python}\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\n\nX = prep[\"X\"]   # final clustering matrix (scaled + capped)\nK_FINAL = 4\n\nkmeans = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42)\nlabels = kmeans.fit_predict(X)\n\n# One label per contact (hashed ID only)\ncontact_clusters = pd.DataFrame({\n    \"contact_id\": contact_features_view[\"contact_id\"].values,\n    \"cluster\": labels\n})\n\ncluster_sizes = (\n    contact_clusters[\"cluster\"]\n    .value_counts()\n    .sort_index()\n    .to_frame(\"n_contacts\")\n)\n\ncluster_sizes\n```\n\n---\n\n### Join clusters back to contact-level table (raw units)\n\n```{python}\ncontact_labeled = contact_features_view.merge(contact_clusters, on=\"contact_id\", how=\"left\")\n\n# Hard gate: every contact must get a cluster label\nassert contact_labeled[\"cluster\"].isna().sum() == 0\n\ncontact_labeled[[\"contact_id\", \"cluster\"]].head()\n```\n\n---\n\n### Cluster “cards” (robust medians + size)\n\nThese cards are stakeholder-ready: **one row per cluster**, using **medians** (robust to heavy tails).\n\n```{python}\ncard_features = [\n    # volume / intensity\n    \"n_calls\", \"n_days_active\", \"calls_per_active_day\",\n\n    # duration / relationship depth proxy\n    \"total_dur_sec\", \"median_dur_sec\", \"long_call_share\",\n\n    # recency / tenure\n    \"recency_days\", \"tenure_days\",\n\n    # timing mix\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\nmissing_cards = [c for c in card_features if c not in contact_labeled.columns]\nassert len(missing_cards) == 0, f\"Missing card features: {missing_cards}\"\n\ncards = (\n    contact_labeled\n    .groupby(\"cluster\")[card_features]\n    .median()\n    .merge(cluster_sizes, left_index=True, right_index=True)\n    .reset_index()\n)\n\ncards = cards[[\"cluster\", \"n_contacts\"] + card_features]\ncards.T\n```\n\n---\n\n### Within-cluster spread check (to avoid misleading medians)\n\nWe report 25/50/75% for a few “anchor” variables.\n\n```{python}\nspread_features = [\"n_calls\", \"total_dur_sec\", \"recency_days\", \"median_dur_sec\"]\n\nspread = (\n    contact_labeled\n    .groupby(\"cluster\")[spread_features]\n    .quantile([0.25, 0.50, 0.75])\n    .unstack(level=1)\n)\n\nspread.columns = [f\"{feat}_q{int(q*100)}\" for feat, q in spread.columns]\nspread = spread.reset_index()\n\nspread.T\n```\n\n---\n\n::: callout-tip\n\n### Why timing-share medians often look like 0 or 1\n\nMany contacts have only 1–2 calls. With tiny denominators, share features become extreme.\nSo for timing behaviour we also report **means** and **% of contacts with any activity** in each time band.\n:::\n\n---\n\n### Timing patterns (mean shares + % with any non-business-hours / Sunday activity)\n\n```{python}\nshare_cols = [\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\ntiming_mean = (\n    contact_labeled\n    .groupby(\"cluster\")[share_cols]\n    .mean()\n    .add_suffix(\"_mean\")\n    .reset_index()\n)\n\ntiming_any = (\n    contact_labeled\n    .assign(\n        any_evening=lambda x: (x[\"share_evening\"] > 0).astype(int),\n        any_early_morning=lambda x: (x[\"share_early_morning\"] > 0).astype(int),\n        any_late_night=lambda x: (x[\"share_late_night\"] > 0).astype(int),\n        any_sunday=lambda x: (x[\"share_closed_day\"] > 0).astype(int),\n    )\n    .groupby(\"cluster\")[[\"any_evening\", \"any_early_morning\", \"any_late_night\", \"any_sunday\"]]\n    .mean()\n    .mul(100)\n    .add_suffix(\"_pct_contacts\")\n    .reset_index()\n)\n\ntiming_summary = (\n    timing_mean\n    .merge(timing_any, on=\"cluster\", how=\"left\")\n    .merge(cluster_sizes.reset_index(), on=\"cluster\", how=\"left\")\n    .sort_values(\"cluster\")\n    .reset_index(drop=True)\n)\n\ntiming_summary.T\n```\n\n---\n\n### Segment names + action policy (stakeholder-ready layer)\n\nCluster IDs are arbitrary. We map clusters to segment labels using **behavioural signatures** from `cards`.\n\n```{python}\nsig = cards[[\n    \"cluster\", \"n_contacts\",\n    \"n_calls\", \"total_dur_sec\", \"median_dur_sec\", \"long_call_share\",\n    \"recency_days\", \"tenure_days\"\n]].copy()\n\n# Identify clusters by signatures (robust against label permutation)\none_off_cluster = int(sig.sort_values([\"n_calls\", \"total_dur_sec\"], ascending=[True, True]).iloc[0][\"cluster\"])\ncore_active_cluster = int(sig.sort_values([\"n_calls\", \"n_contacts\"], ascending=[False, False]).iloc[0][\"cluster\"])\nrare_deep_cluster = int(sig.sort_values([\"median_dur_sec\", \"long_call_share\"], ascending=[False, False]).iloc[0][\"cluster\"])\n\nremaining = sorted(set(sig[\"cluster\"]) - {one_off_cluster, core_active_cluster, rare_deep_cluster})\nassert len(remaining) == 1, \"Expected exactly one remaining cluster for 'warm occasional'.\"\nwarm_cluster = int(remaining[0])\n\nsegment_map = {\n    one_off_cluster: {\n        \"label\": \"One-off / low-engagement contacts\",\n        \"primary_action\": \"Deprioritize by default; automate nurture; reactivate only if new activity appears\"\n    },\n    core_active_cluster: {\n        \"label\": \"Core active relationships\",\n        \"primary_action\": \"Retention + priority servicing; assign owner; proactive follow-ups\"\n    },\n    warm_cluster: {\n        \"label\": \"Warm occasional contacts\",\n        \"primary_action\": \"Nurture cadence (monthly/quarterly); structured check-ins to increase engagement\"\n    },\n    rare_deep_cluster: {\n        \"label\": \"Rare deep conversations\",\n        \"primary_action\": \"High-touch when active; preserve context; personalized re-engagement when dormant\"\n    }\n}\n\ncluster_labels = (\n    pd.DataFrame([\n        {\"cluster\": k, \"segment_label\": v[\"label\"], \"primary_action\": v[\"primary_action\"]}\n        for k, v in segment_map.items()\n    ])\n    .merge(cluster_sizes.reset_index(), on=\"cluster\", how=\"left\")\n    .sort_values(\"cluster\")\n    .reset_index(drop=True)\n)\n\ncluster_labels\n```\n\n---\n\n### Cluster narrative (auto-generated from the cards)\n\nShort and operational: “what it looks like” + “what to do”.\n\n```{python}\ndef fmt(x, nd=1):\n    if pd.isna(x):\n        return \"NA\"\n    return f\"{float(x):.{nd}f}\"\n\ncards_idx = cards.set_index(\"cluster\")\nlabels_idx = cluster_labels.set_index(\"cluster\")\n\nnarr_rows = []\nfor c in sorted(cards_idx.index):\n    row = cards_idx.loc[c]\n    seg = labels_idx.loc[c, \"segment_label\"]\n    act = labels_idx.loc[c, \"primary_action\"]\n\n    narrative = (\n        f\"{seg}: median calls={fmt(row['n_calls'],1)}, \"\n        f\"median active days={fmt(row['n_days_active'],1)}, \"\n        f\"median total duration (sec)={fmt(row['total_dur_sec'],0)}, \"\n        f\"median call duration (sec)={fmt(row['median_dur_sec'],0)}, \"\n        f\"median recency (days)={fmt(row['recency_days'],0)}.\"\n    )\n\n    narr_rows.append({\n        \"cluster\": int(c),\n        \"segment_label\": seg,\n        \"n_contacts\": int(row[\"n_contacts\"]),\n        \"cluster_story\": narrative,\n        \"default_action\": act\n    })\n\ncluster_story = pd.DataFrame(narr_rows).sort_values(\"cluster\").reset_index(drop=True)\ncluster_story\n```\n\n---\n\n### “What decisions can the business make with these segments?”\n\nPolicy rules we can implement in a CRM without exposing identifiers.\n\n```{python}\nrules = pd.DataFrame([\n    {\n        \"segment_label\": \"Core active relationships\",\n        \"decision_use\": \"Prioritization and retention\",\n        \"operational_rule\": \"Assign owner and SLA; proactive check-ins\",\n        \"typical_pattern\": \"High calls, many active days, low recency\"\n    },\n    {\n        \"segment_label\": \"Warm occasional contacts\",\n        \"decision_use\": \"Nurture and growth\",\n        \"operational_rule\": \"Monthly/quarterly outreach; reminders; targeted offers\",\n        \"typical_pattern\": \"Moderate repeat calls, moderate tenure, mid recency\"\n    },\n    {\n        \"segment_label\": \"Rare deep conversations\",\n        \"decision_use\": \"High-touch exceptions and win-back\",\n        \"operational_rule\": \"Personalized follow-up; preserve context; targeted reactivation if dormant\",\n        \"typical_pattern\": \"Few calls but long conversations (high median duration / long-call share)\"\n    },\n    {\n        \"segment_label\": \"One-off / low-engagement contacts\",\n        \"decision_use\": \"Noise filtering and automation\",\n        \"operational_rule\": \"Exclude from priority lists; automate nurture only\",\n        \"typical_pattern\": \"One call, one day, small total duration, often old\"\n    },\n])\n\nrules\n```\n\n---\n\n### Store artifacts for later phases (robustness, sensitivity, PCA)\n\n```{python}\nprep[\"K_FINAL\"] = K_FINAL\nprep[\"kmeans\"] = kmeans\nprep[\"labels\"] = labels\n\nprep[\"contact_clusters\"] = contact_clusters\nprep[\"cluster_sizes\"] = cluster_sizes\n\nprep[\"cards\"] = cards\nprep[\"spread\"] = spread\nprep[\"timing_summary\"] = timing_summary\nprep[\"cluster_labels\"] = cluster_labels\nprep[\"cluster_story\"] = cluster_story\nprep[\"rules\"] = rules\n\nlist(prep.keys())[:15]\n```\n\n\n\n---\n\n\n## Phase 6 — Robustness and sensitivity (do the clusters “hold up”?)\n\nA good clustering result is not just “a nice plot.” It should be **stable** when we rerun the algorithm and **reasonably consistent** when we change *feature blocks*.\n\nIn this phase we test two kinds of robustness:\n\n1. **Seed stability:** If we change the random seed (different K-means initializations), do we get essentially the same segmentation?\n2. **Feature-block sensitivity:** If we drop one block (volume, duration, recency, timing), do the segments remain broadly similar?\n\nWe quantify stability using **Adjusted Rand Index (ARI)**:\n\n\n- $ARI = 1$ means two clusterings are identical up to label permutation.\n- $ARI \\approx 0$ means agreement is no better than random.\n- Negative $ARI$ can happen (worse than random agreement), usually a warning sign.\n\n::: callout-important\n### Training matrix\nAll robustness tests use `prep[\"X\"]` (robust-scaled + capped) so distances are fair and numerically stable.\n\n### Privacy\nWe do not merge names/phone numbers into any robustness outputs. We only evaluate labels as arrays.\n:::\n\n---\n\n### Seed stability test (ARI across many random seeds)\n\nWe run K-means many times with different `random_state`, always using $K = 4$, and compare each run to a baseline labeling.\n\n```{python}\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# -------------------------\n# Setup\n# -------------------------\nX = prep[\"X\"]\nK_FINAL = int(prep.get(\"K_FINAL\", 4))\n\n# Baseline labels (use Phase 5 if available)\nif \"labels\" in prep and prep.get(\"labels\", None) is not None:\n    base_labels = np.asarray(prep[\"labels\"])\nelse:\n    km0 = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42)\n    base_labels = km0.fit_predict(X)\n\n# -------------------------\n# Run many seeds\n# -------------------------\nSEEDS = list(range(0, 50))  # employer-ready default\nrows = []\n\nfor s in SEEDS:\n    km = KMeans(n_clusters=K_FINAL, n_init=50, random_state=s)\n    lab = km.fit_predict(X)\n    ari = float(adjusted_rand_score(base_labels, lab))\n    rows.append({\"seed\": s, \"ari_vs_baseline\": ari, \"inertia\": float(km.inertia_)})\n\nseed_stability = (\n    pd.DataFrame(rows)\n    .sort_values(\"ari_vs_baseline\", ascending=True)\n    .reset_index(drop=True)\n)\n\nseed_audit = pd.DataFrame({\n    \"K_FINAL\": [K_FINAL],\n    \"n_seeds_tested\": [len(SEEDS)],\n    \"ari_min\": [float(seed_stability[\"ari_vs_baseline\"].min())],\n    \"ari_p05\": [float(seed_stability[\"ari_vs_baseline\"].quantile(0.05))],\n    \"ari_median\": [float(seed_stability[\"ari_vs_baseline\"].median())],\n    \"ari_p95\": [float(seed_stability[\"ari_vs_baseline\"].quantile(0.95))],\n    \"ari_max\": [float(seed_stability[\"ari_vs_baseline\"].max())],\n})\n\nseed_audit, seed_stability.head(10)\n```\n\n::: callout-tip\n\n### How to interpret seed ARI\n\nA practical rule of thumb:\n\n* **Very stable:** most ARI values $\\ge 0.90$\n* **Reasonably stable:** most ARI values $\\ge 0.75$\n* **Unstable:** many ARI values $< 0.60$\n\nIf stability is weak, we usually revisit preprocessing, $K$, or consider a different clustering method.\n:::\n\n---\n\n### “Worst-case” rerun inspection (label-aligned)\n\nEven when ARI is $1.0$, K-means can output the same partition with different numeric labels (label permutation).\nSo we align labels before comparing cluster sizes.\n\n```{python}\n# Identify worst seed run (lowest ARI vs baseline)\nworst = seed_stability.iloc[0]\nworst_seed = int(worst[\"seed\"])\n\nkm_worst = KMeans(n_clusters=K_FINAL, n_init=50, random_state=worst_seed)\nlabels_worst = km_worst.fit_predict(X)\n\nari_worst = float(adjusted_rand_score(base_labels, labels_worst))\n\n# Contingency table: baseline cluster IDs (rows) vs worst run IDs (cols)\nct = pd.crosstab(\n    pd.Series(base_labels, name=\"baseline\"),\n    pd.Series(labels_worst, name=\"worst\")\n)\n\n# Map each worst-cluster to the baseline cluster it overlaps with most\nmapping = ct.idxmax(axis=0).to_dict()\n\nlabels_worst_aligned = np.vectorize(mapping.get)(labels_worst)\n\n# Compare cluster sizes after alignment\nbase_sizes = pd.Series(base_labels).value_counts().sort_index()\nworst_sizes = pd.Series(labels_worst_aligned).value_counts().sort_index()\n\nsize_compare = pd.DataFrame({\n    \"cluster_id\": sorted(set(base_sizes.index) | set(worst_sizes.index)),\n}).set_index(\"cluster_id\")\n\nsize_compare[\"baseline_n\"] = base_sizes.reindex(size_compare.index).fillna(0).astype(int)\nsize_compare[\"worst_n_aligned\"] = worst_sizes.reindex(size_compare.index).fillna(0).astype(int)\nsize_compare[\"baseline_pct\"] = (size_compare[\"baseline_n\"] / size_compare[\"baseline_n\"].sum() * 100).round(2)\nsize_compare[\"worst_pct_aligned\"] = (size_compare[\"worst_n_aligned\"] / size_compare[\"worst_n_aligned\"].sum() * 100).round(2)\n\nworst_seed, float(worst[\"ari_vs_baseline\"]), ari_worst, mapping, size_compare.reset_index()\n```\n\n---\n\n### Feature-block sensitivity (drop-one-block tests)\n\nNow we test whether the segmentation depends too heavily on a single block.\nWe refit K-means on reduced matrices and compare to baseline using ARI.\n\nWe do “drop one block at a time” on:\n\n* volume\n* duration\n* recency\n* timing\n\n::: callout-note\n\n### Why ARI is valid here\n\nCluster labels are arbitrary (cluster 0 in one run is not “the same” as cluster 0 in another).\nARI is invariant to label permutation, so it compares structure, not label IDs.\n:::\n\n```{python}\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n\n# Rebuild X for each variant using the SAME preprocessing recipe as Phase 3:\n# - drop constant cols\n# - clip p01–p99 in raw space\n# - median impute + RobustScaler\n# - cap in scaled space\n\nSCALED_CAP = float(prep.get(\"scaled_cap\", 10.0))  # safe default\n\nFEATURE_BLOCKS = prep[\"FEATURE_BLOCKS\"]\nMODEL_FEATURES = prep[\"MODEL_FEATURES\"]\ndf_view = contact_features_view  # raw-unit feature table\n\ndef build_X_from_features(df, features, clip_lo=0.01, clip_hi=0.99, scaled_cap=10.0):\n    X_raw = df[features].copy()\n\n    # Drop truly constant columns\n    nunique = X_raw.nunique(dropna=False)\n    constant_cols = nunique[nunique <= 1].index.tolist()\n    X_raw2 = X_raw.drop(columns=constant_cols) if constant_cols else X_raw.copy()\n\n    # Clip extremes in raw space\n    X_clip = X_raw2.copy()\n    for col in X_clip.columns:\n        lo = float(X_clip[col].quantile(clip_lo))\n        hi = float(X_clip[col].quantile(clip_hi))\n        X_clip[col] = X_clip[col].clip(lower=lo, upper=hi)\n\n    # Impute + robust scale\n    imputer = SimpleImputer(strategy=\"median\")\n    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n    X_imp = imputer.fit_transform(X_clip)\n    X_scaled = scaler.fit_transform(X_imp)\n\n    # Final cap in scaled space\n    X_final = np.clip(X_scaled, -scaled_cap, scaled_cap)\n\n    return X_final, list(X_clip.columns)\n\n# Baseline rebuild for consistent comparisons\nX_base, base_feature_names = build_X_from_features(\n    df_view, MODEL_FEATURES, clip_lo=0.01, clip_hi=0.99, scaled_cap=SCALED_CAP\n)\nbase_labels_for_sens = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42).fit_predict(X_base)\n\n# Define variants: drop one block at a time (excluding \"category\")\nvariants = {}\nfor block in [\"volume\", \"duration\", \"recency\", \"timing\"]:\n    keep = []\n    for b, feats in FEATURE_BLOCKS.items():\n        if b == \"category\":\n            continue\n        if b != block:\n            keep += feats\n    variants[f\"drop_{block}\"] = keep\n\nrows = []\nfor name, feats in variants.items():\n    X_var, feat_names_var = build_X_from_features(\n        df_view, feats, clip_lo=0.01, clip_hi=0.99, scaled_cap=SCALED_CAP\n    )\n    lab = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42).fit_predict(X_var)\n    ari = float(adjusted_rand_score(base_labels_for_sens, lab))\n    rows.append({\n        \"variant\": name,\n        \"n_features\": int(X_var.shape[1]),\n        \"ari_vs_baseline\": ari\n    })\n\nblock_sensitivity = (\n    pd.DataFrame(rows)\n    .sort_values(\"ari_vs_baseline\", ascending=False)\n    .reset_index(drop=True)\n)\n\nblock_sensitivity\n```\n\n---\n\n### Sensitivity summary (decision-useful)\n\n```{python}\nsens_audit = pd.DataFrame({\n    \"K_FINAL\": [K_FINAL],\n    \"seed_stability_ari_median\": [float(seed_audit[\"ari_median\"].iloc[0])],\n    \"seed_stability_ari_p05\": [float(seed_audit[\"ari_p05\"].iloc[0])],\n    \"seed_stability_ari_min\": [float(seed_audit[\"ari_min\"].iloc[0])],\n    \"block_sensitivity_ari_min\": [float(block_sensitivity[\"ari_vs_baseline\"].min())],\n    \"block_sensitivity_ari_median\": [float(block_sensitivity[\"ari_vs_baseline\"].median())],\n})\n\nsens_audit.T\n```\n\n::: callout-important\n\n### What we conclude here (based on your results)\n\n* **Seed stability:** ARI is $1.0$ for all tested seeds $\\Rightarrow$ K-means solution is *fully stable*.\n* **Block sensitivity:** dropping **duration** or **recency** reduces agreement (ARI $\\approx 0.70$–$0.74$), which implies these blocks carry meaningful segmentation signal.\n\nThis is robust enough for stakeholder-facing use, with a clear explanation of what drives the segments.\n:::\n\n---\n\n### Store robustness artifacts (for later reporting)\n\n```{python}\nprep[\"seed_stability\"] = seed_stability\nprep[\"seed_audit\"] = seed_audit\n\nprep[\"labels_worst_seed\"] = labels_worst\nprep[\"labels_worst_seed_aligned\"] = labels_worst_aligned\nprep[\"size_compare_worst_seed\"] = size_compare.reset_index()\nprep[\"worst_seed_label_mapping\"] = mapping\n\nprep[\"block_sensitivity\"] = block_sensitivity\nprep[\"sens_audit\"] = sens_audit\n\nlist(prep.keys())[:25]\n```\n\n---\n\n::: callout-important\n## Phase 6 summary (robustness verdict)\n\nBased on your results:\n\n- **Seed stability (random restarts):** $ARI = 1.00$ for all tested seeds $\\Rightarrow$ the $K=4$ K-means solution is **fully stable** on `prep[\"X\"]`.\n- **Feature-block sensitivity (drop-one-block):**\n  - Drop timing: $ARI = 1.00$ (timing is not driving the segmentation)\n  - Drop volume: $ARI \\approx 0.94$ (volume contributes, but is not the sole driver)\n  - Drop recency: $ARI \\approx 0.74$ (recency contains meaningful signal)\n  - Drop duration: $ARI \\approx 0.70$ (duration contains meaningful signal)\n\n**Decision-useful takeaway:** The segmentation is stable and not an artifact of random initialization.  \nThe clusters are primarily supported by **duration** and **recency** behaviour (with volume contributing), so these are the main behavioural levers to emphasize in stakeholder explanations.\n\n:::\n\n\n---\n\n\n\n## Phase 7 — PCA visualization (for interpretation only)\n\nPCA is **not** used to train clustering.  \nWe use PCA only to create a 2D “map” of contacts for storytelling, sanity-checking, and explaining separation.\n\n::: callout-warning\n### PCA is not the clustering model\n- **Clustering was trained on:** `prep[\"X\"]` (robust-scaled + capped)\n- **PCA is used for:** visualization only (2D projection)\nIf PCA looks messy, it does **not** automatically mean clustering is wrong. PCA compresses information into 2 dimensions.\n:::\n\n::: callout-important\n### Privacy\nWe plot only:\n\n- PCA coordinates\n- cluster labels\nWe do **not** plot names or phone numbers, and we keep `contact_id` hashed.\n:::\n\n---\n\n### Fit PCA ($2$ components) on the clustering matrix\n\n```{python}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nX = prep[\"X\"]  # scaled + capped matrix used for clustering\nlabels = np.asarray(prep[\"labels\"])\nK_FINAL = int(prep.get(\"K_FINAL\", 4))\n\npca = PCA(n_components=2, random_state=42)\nZ = pca.fit_transform(X)\n\npca_audit = pd.DataFrame({\n    \"n_components\": [2],\n    \"explained_var_ratio_pc1\": [float(pca.explained_variance_ratio_[0])],\n    \"explained_var_ratio_pc2\": [float(pca.explained_variance_ratio_[1])],\n    \"explained_var_ratio_total_2pc\": [float(pca.explained_variance_ratio_[:2].sum())],\n})\n\npca_audit\n```\n\n---\n\n### Build a plotting table (hashed id + PCA coordinates + cluster)\n\n```{python}\npca_view = pd.DataFrame({\n    \"contact_id\": contact_features_view[\"contact_id\"].values,  # hashed only\n    \"pc1\": Z[:, 0],\n    \"pc2\": Z[:, 1],\n    \"cluster\": labels\n})\n\npca_view.head()\n```\n\n---\n\n### PCA scatter plot (clusters on the 2D map)\n\n```{python}\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot each cluster separately for a clear legend\nfor c in sorted(pca_view[\"cluster\"].unique()):\n    sub = pca_view[pca_view[\"cluster\"] == c]\n    ax.scatter(sub[\"pc1\"], sub[\"pc2\"], s=12, alpha=0.7, label=f\"Cluster {c}\")\n\nax.set_title(f\"PCA map of contacts (K-means clusters, $K={K_FINAL}$)\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.legend(title=\"Cluster\", fontsize=9)\nplt.show()\n```\n\n---\n\n### Optional: cluster centroids projected into PCA space\n\nThis helps stakeholders see “where the centers sit” on the map.\n\n```{python}\n# K-means centroids exist in scaled feature space; PCA was fit on the same space.\ncentroids = prep[\"kmeans\"].cluster_centers_\ncentroids_2d = pca.transform(centroids)\n\ncent = pd.DataFrame({\n    \"cluster\": list(range(K_FINAL)),\n    \"pc1_centroid\": centroids_2d[:, 0],\n    \"pc2_centroid\": centroids_2d[:, 1],\n})\n\ncent\n```\n\n```{python}\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor c in sorted(pca_view[\"cluster\"].unique()):\n    sub = pca_view[pca_view[\"cluster\"] == c]\n    ax.scatter(sub[\"pc1\"], sub[\"pc2\"], s=10, alpha=0.45, label=f\"Cluster {c}\")\n\n# Centroids\nax.scatter(cent[\"pc1_centroid\"], cent[\"pc2_centroid\"], s=120, marker=\"X\", label=\"Centroids\")\n\nax.set_title(f\"PCA map with centroids (K={K_FINAL})\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.legend(fontsize=9)\nplt.show()\n```\n\n---\n\n### Interpretation guidance (what we conclude from PCA)\n\n::: callout-note\nPCA is a **compression** from many dimensions down to 2.\n\nUse this plot to check:\n\n* whether clusters are *roughly separated* (good sign),\n* whether some clusters overlap heavily (could be fine; segmentation can still be valid),\n* whether a small cluster looks like extreme outliers (could indicate a “special handling” segment).\n\nDo **not** use PCA to choose $K$ or to claim “the true number of clusters.”\n:::\n\n---\n\n### Store PCA artifacts (for the final report)\n\n```{python}\nprep[\"pca_model\"] = pca\nprep[\"pca_audit\"] = pca_audit\nprep[\"pca_view\"] = pca_view\nprep[\"pca_centroids_2d\"] = cent\n\nlist(prep.keys())[:30]\n```\n\n---\n\n\n## What PCA tells us (interpretation only)\n\nPCA is a 2D projection of the same scaled training matrix used for clustering, `prep[\"X\"]`.\nIt does **not** change the model; it only helps us *see* separation.\n\n### Variance captured\n\nFrom `pca_audit`, the first two components explain:\n\n- $PC1 \\approx 0.629$\n- $PC2 \\approx 0.216$\n- Total (2D) $\\approx 0.844$\n\nSo about **84.4%** of the variance in the scaled feature space is visible in the 2D map.\nThat is high enough that the plot is a meaningful sanity-check.\n\n### Visual separation (sanity-check)\n\nThe PCA map shows clear structure:\n\n- One cluster sits far to the **right** on $PC1$ (strong separation in the main direction of variance).\n- One cluster is concentrated far to the **left** (tight wedge near low $PC1$ values).\n- Two clusters occupy the middle/bottom region with partial overlap, suggesting they differ in *multiple* dimensions (not fully separable in 2D, which is normal).\n\nThis supports the earlier interpretation: the segmentation is not an artifact of one random run (Phase 6 already proved stability), and the clusters correspond to genuinely different behavioral regimes in feature space.\n\n### Centroids on the PCA map\n\nProjected centroids help explain “where the centers sit”:\n\n- The centroid that is far right on $PC1$ corresponds to a high-intensity / high-activity regime.\n- The centroid that is far left corresponds to low-engagement / near-baseline contacts.\n- The centroid that is high on $PC2$ indicates a behavior pattern that is not just “more or less activity,” but a different mix (often duration/recency-related structure).\n\n**Important:** overlap in PCA does not invalidate clusters. K-means was fit in the full feature space; PCA compresses information into 2 dimensions.\n\n\n---\n\n## Phase 8 — Final deliverables (stakeholder-ready outputs)\n\nIn this phase we package the work into **decision-ready artifacts** that can be used in a CRM or reporting workflow **without exposing identities**.\n\n::: callout-important\n### Privacy and operational use\n- We **do not** join names or phone numbers into any outputs.\n- All contact identifiers remain **hashed**.\n- Default is **no file export**. You can enable export if you want portfolio artifacts.\n:::\n\n---\n\n### Assemble the “final segment table” (labels + key medians)\n\n```{python}\nimport pandas as pd\nimport numpy as np\n\n# Guardrails\nrequired = [\"cards\", \"cluster_labels\", \"rules\", \"sens_audit\", \"pca_audit\"]\nmissing = [k for k in required if k not in prep]\nassert len(missing) == 0, f\"Missing Phase artifacts in prep: {missing}\"\n\ncards = prep[\"cards\"].copy()\ncluster_labels = prep[\"cluster_labels\"].copy()\nrules = prep[\"rules\"].copy()\n\n# Join: cluster -> segment label + action\nfinal_segments = (\n    cards.merge(cluster_labels[[\"cluster\", \"segment_label\", \"primary_action\"]], on=\"cluster\", how=\"left\")\n    .sort_values(\"n_contacts\", ascending=False)\n    .reset_index(drop=True)\n)\n\n# Reorder columns for a stakeholder view\nfront = [\"cluster\", \"segment_label\", \"n_contacts\", \"primary_action\"]\nrest = [c for c in final_segments.columns if c not in front]\nfinal_segments = final_segments[front + rest]\n\nfinal_segments\n```\n\n---\n\n### Decision rules (CRM / outreach policy)\n\n```{python}\nrules\n```\n\n---\n\n### “Executive summary” (one screen)\n\n```{python}\nsens_audit = prep[\"sens_audit\"].copy()\npca_audit = prep[\"pca_audit\"].copy()\n\n# Key stats\nn_contacts_total = int(prep[\"contact_clusters\"].shape[0])\nk_final = int(prep.get(\"K_FINAL\", 4))\n\nseed_median = float(sens_audit[\"seed_stability_ari_median\"].iloc[0])\nseed_min = float(sens_audit[\"seed_stability_ari_min\"].iloc[0])\nblock_min = float(sens_audit[\"block_sensitivity_ari_min\"].iloc[0])\n\npca_total2 = float(pca_audit[\"explained_var_ratio_total_2pc\"].iloc[0])\n\nexec_summary = pd.DataFrame([{\n    \"n_contacts_clustered\": n_contacts_total,\n    \"K_FINAL\": k_final,\n    \"seed_stability_ARI_median\": seed_median,\n    \"seed_stability_ARI_min\": seed_min,\n    \"block_sensitivity_ARI_min\": block_min,\n    \"pca_variance_explained_2PC\": pca_total2,\n}])\n\nexec_summary\n```\n\n---\n\n### Findings (plain-English, decision-first)\n\n```{python}\n# Convert the segment table into a short narrative summary\nseg_counts = final_segments[[\"segment_label\", \"n_contacts\"]].copy()\nseg_counts[\"share_pct\"] = (seg_counts[\"n_contacts\"] / seg_counts[\"n_contacts\"].sum() * 100).round(2)\n\nseg_counts\n```\n\n```{python}\n# Build a compact “what it is + what to do” list (no IDs)\nlabels_idx = cluster_labels.set_index(\"segment_label\")\n\nrows = []\nfor _, r in seg_counts.iterrows():\n    seg = r[\"segment_label\"]\n    n = int(r[\"n_contacts\"])\n    pct = float(r[\"share_pct\"])\n    action = labels_idx.loc[seg, \"primary_action\"] if seg in labels_idx.index else \"TBD\"\n    rows.append({\"segment_label\": seg, \"n_contacts\": n, \"share_pct\": pct, \"default_action\": action})\n\nfindings_table = pd.DataFrame(rows).sort_values(\"n_contacts\", ascending=False).reset_index(drop=True)\nfindings_table\n```\n\n::: callout-note\n\n### What we learned from your results\n\n* **The solution is fully stable to random initialization** (seed ARI $\\approx 1$ across tested seeds), so you can rerun K-means and get the same segmentation (up to label permutation).\n* **Recency and duration matter**: dropping either block reduces agreement (lower ARI), so those blocks carry meaningful segmentation signal.\n* **PCA is supportive, not decisive**: the first two PCs explain a large share of variance, and the 2D map shows visible separation patterns that align with the segmentation.\n:::\n\n---\n\n### Limitations and “how not to misuse this”\n\n::: callout-warning\n\n### Limitations\n\n* This is **behavior-based segmentation**, not a causal model. It supports prioritization and outreach strategy, not “ground truth identities.”\n* K-means assumes roughly spherical clusters in the feature space. If your business needs non-spherical shapes, consider alternative methods later (e.g., GMM, HDBSCAN).\n* Very sparse contacts (1–2 calls) can make share features extreme; we handled this by using medians + mean/%-any checks for timing.\n:::\n\n---\n\n### Optional: export portfolio artifacts (OFF by default)\n\n```{python}\nSAVE_FILES = False  # keep False unless you explicitly want exports\n\nif SAVE_FILES:\n    # These contain NO names/phone numbers. contact_id stays hashed.\n    final_segments.to_csv(\"final_segments_cluster_cards.csv\", index=False)\n    rules.to_csv(\"segment_policy_rules.csv\", index=False)\n    findings_table.to_csv(\"segment_findings_summary.csv\", index=False)\n    exec_summary.to_csv(\"executive_summary_metrics.csv\", index=False)\n\n    print(\"Saved CSV files (hashed IDs only, no raw identifiers).\")\nelse:\n    print(\"SAVE_FILES=False (no files written).\")\n```\n\n---\n\n\n## Phase 10 — Final wrap-up (competition framing + decision narrative)\n\n### Competition prompt (what we were given)\n\n**We were given** a historical phone-call behaviour dataset where each record represents a call event.  \nThe dataset includes **timestamps, call duration, and categorised call context**, but **no business labels** such as “good customer” or “bad customer”.\n\n**Task:** Without supervision (no labels), we had to build a **contact-level segmentation** that a business could actually use for prioritisation, outreach cadence, and relationship management.\n\n**Hard constraints:**\n\n- **Privacy-first:** do not use names or phone numbers; keep identifiers hashed.\n- **Operational:** output must translate into **clear segment actions**, not just clusters.\n- **Professional robustness:** results must be stable, not a “nice plot”.\n\n---\n\n### What we had to produce (deliverable definition)\n\nWe had to deliver:\n\n1. A **clean contact-level feature table** (one row per contact).\n2. A **distance-ready clustering matrix** (robust preprocessing).\n3. A final segmentation using **K-means with $K=4$**.\n4. **Stakeholder-ready interpretation** (cluster “cards” + segment labels + default actions).\n5. **Robustness evidence** (seed stability and feature sensitivity).\n6. **Interpretation-only visual sanity-check** (PCA map).\n\n---\n\n### What we built (end-to-end pipeline)\n\n1. **Audit** the raw call-event data (types, missingness, constraints).\n2. **Aggregate to contacts** (the clustering unit), producing features that capture:\n   - volume/intensity (calls, active days),\n   - relationship depth proxy (duration metrics, long-call share),\n   - recency/tenure (dormancy vs continuity),\n   - timing mix (business hours vs off-hours).\n3. **Preprocess for K-means**:\n   - clip extreme values in raw space,\n   - median imputation,\n   - robust scaling,\n   - cap scaled values for numerical stability.\n4. **Fit K-means** with $K=4$ and attach labels to contacts (hashed IDs only).\n5. **Interpret** clusters using raw units and translate into segments + actions.\n6. **Validate robustness** using ARI across seeds and drop-one-block sensitivity.\n7. **Use PCA only for visualization** (interpretation, not training).\n\n---\n\n### The segments (what we found + what to do)\n\nFrom the cluster cards and behaviour-based mapping, we obtained four operational segments:\n\n- **One-off / low-engagement contacts** (largest share)  \n  **Action:** deprioritize by default; automate nurture; reactivate only if new activity appears.\n\n- **Warm occasional contacts**  \n  **Action:** nurture cadence (monthly/quarterly); structured check-ins to increase engagement.\n\n- **Core active relationships**  \n  **Action:** retention and priority servicing; assign owner; proactive follow-ups.\n\n- **Rare deep conversations**  \n  **Action:** high-touch when active; preserve context; personalized re-engagement when dormant.\n\n---\n\n### Proof the solution “holds up” (robustness)\n\n**Seed stability:** ARI was $1.00$ across all tested seeds.  \nSo the segmentation is fully stable to random initialization (up to label permutation).\n\n**Feature sensitivity:** dropping duration or recency reduces agreement (ARI drops to about $0.70$–$0.74$).  \nInterpretation: **duration and recency carry meaningful segmentation signal**; timing contributes less to the final partition.\n\n---\n\n### What PCA contributed (interpretation only)\n\nPCA is a 2D projection of `prep[\"X\"]` for storytelling and sanity-checking.\n\n- The first two components explain about $84.45\\%$ of variance.\n- The map shows visible structure consistent with the segmentation.\n- PCA overlap does not invalidate clusters because K-means was trained in the full feature space.\n\n---\n\n### Decision use: what a business can do with these segments\n\n- **Routing:** prioritize “Core active” + “Rare deep” to high-touch handling.\n- **Cadence:** automate outreach for “Warm occasional”; suppress manual effort for “One-off”.\n- **Reactivation:** trigger win-back campaigns for dormant “Rare deep” and “Warm occasional”.\n- **Service levels:** define SLA tiers using segment label and recency.\n\n---\n\n### Limitations (how not to misuse this)\n\n- This is behaviour segmentation, not identity inference and not causal.\n- K-means prefers spherical separation in feature space; alternative methods can be tested later.\n- Very sparse contacts can make timing shares extreme; we mitigated via robust summaries and mean/%-any checks.\n\n::: callout-note\n**Final deliverables:**\n\n- A stable $K=4$ segmentation (hashed IDs only).\n- Stakeholder-ready cluster cards + action policy rules.\n- Robustness evidence (seed ARI + block sensitivity).\n- PCA map used only for interpretation and storytelling.\n:::\n\n\n---\n\n","srcMarkdownNoYaml":"\n\n## Business objective\n\nSegment contacts into **actionable behavioral groups** to support **outreach planning, CRM prioritization, and relationship management**.\n\n::: callout-important\n### Hard constraints enforced in this project\n\n**Time bands (exact):**\n\n- `business_hours`: 08:00–17:59  \n- `evening`: 18:00–20:59  \n- `late_night`: 21:00–23:59  \n- `early_morning`: 00:00–07:59  \n\n**Working days:**\n\n- Open days: Monday–Saturday  \n- Closed day: Sunday only\n:::\n\n::: callout-tip\n### Workflow (phases)\nPhase 1 Audit → Phase 2 Contact-level features → Phase 3 Preprocess → Phase 4 Choose K → Phase 5 Fit/Interpret → then stability/ARI → sensitivity (feature blocks) → PCA viz → final recommendations.\n:::\n\n## Phase 1 — Audit (call-level)\n\n### Setup (imports + toggles)\n\n```{python}\n#| echo: false\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport hashlib\nfrom collections import OrderedDict\n\n# -----------------------\n# GitHub safety toggles\n# -----------------------\nSAVE_FILES = False  # keep OFF by default\n\nDATA_URL = (\n    \"https://learn.walsoftcomputers.com/machine_learning/walsoft_phonecall/\"\n    \"original_dataset_with_instructions/walsoft_semi_categorized_phone_dataset.csv\"\n)\n\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 140)\n\n# -----------------------\n# Hard constraints (exact)\n# -----------------------\nTIME_BANDS = {\n    \"business_hours\": (8, 17),   # 08:00–17:59\n    \"evening\":        (18, 20),  # 18:00–20:59\n    \"late_night\":     (21, 23),  # 21:00–23:59\n    \"early_morning\":  (0, 7),    # 00:00–07:59\n}\n\nOPEN_DOW = set([0, 1, 2, 3, 4, 5])  # Mon..Sat\nCLOSED_DOW = set([6])              # Sun\n```\n\n### Load the dataset\n\n\n#### Load data\n\n```{python}\n# Load dataset (no assumptions, no output)\ndf = pd.read_csv(DATA_URL)\n```\n\n#### Dataset overview\n\n```{python}\n# Dataset overview (executive-level structural metrics)\n\ndataset_overview = pd.DataFrame({\n    \"Metric\": [\n        \"Rows\",\n        \"Columns\",\n        \"Total cells\",\n        \"Memory usage (MB)\"\n    ],\n    \"Value\": [\n        int(df.shape[0]),\n        int(df.shape[1]),\n        int(df.shape[0] * df.shape[1]),\n        f\"{df.memory_usage(deep=True).sum() / 1e6:.2f}\"\n    ]\n})\n\ndataset_overview\n```\n\n#### Column inventory (schema inspection)\n\n```{python}\n# Column inventory (schema audit table)\n\ncolumn_inventory = (\n    pd.DataFrame({\n        \"Column name\": df.columns,\n        \"Data type\": df.dtypes.astype(str),\n        \"Non-null count\": df.notna().sum().values,\n        \"Non-null rate\": (df.notna().mean()).round(3).values\n    })\n    .sort_values(\"Column name\")\n    .reset_index(drop=True)\n)\n\ncolumn_inventory\n```\n\n#### Sample records (visual sanity check)\n\n```{python}\n# Sample records (human sanity check only)\ndf.head(3)\n```\n\n\n\n---\n\n### Identify key columns (robust, from the data itself)\n\n```{python}\n# Identify key columns (schema audit, no assumptions)\n\n\n# ------------------------------------------------------------------\n# Normalise column names once (single source of truth)\n# ------------------------------------------------------------------\ncol_map = {c: c.lower().strip() for c in df.columns}\n\n# ------------------------------------------------------------------\n# Detection rules (explicit, auditable, extensible)\n# ------------------------------------------------------------------\nDETECTION_RULES = OrderedDict({\n    \"Date\": {\n        \"description\": \"Calendar date component\",\n        \"match_any\": [\"date\"]\n    },\n    \"Time\": {\n        \"description\": \"Clock time component\",\n        \"match_exact_or_contains\": [\"time\"]\n    },\n    \"Duration\": {\n        \"description\": \"Call duration (numeric, any unit)\",\n        \"match_any\": [\"duration\", \"seconds\", \"secs\", \"sec\", \"minutes\", \"mins\"]\n    },\n    \"Identifier\": {\n        \"description\": \"Contact or phone identifier\",\n        \"match_any\": [\"dialled\", \"dialed\", \"phone\", \"number\", \"contact\"]\n    },\n    \"Category\": {\n        \"description\": \"Human or system call classification\",\n        \"match_any\": [\"category\"]\n    }\n})\n\n# ------------------------------------------------------------------\n# Apply detection rules\n# ------------------------------------------------------------------\nrecords = []\n\nfor field, rule in DETECTION_RULES.items():\n    detected = []\n\n    for col, lc in col_map.items():\n        if \"match_exact_or_contains\" in rule:\n            if any(lc == k or k in lc for k in rule[\"match_exact_or_contains\"]):\n                detected.append(col)\n        elif \"match_any\" in rule:\n            if any(k in lc for k in rule[\"match_any\"]):\n                detected.append(col)\n\n    records.append({\n        \"Field type\": field,\n        \"Purpose\": rule[\"description\"],\n        \"Detected columns\": \", \".join(detected) if detected else \"—\",\n        \"Count\": len(detected),\n        \"Status\": \"OK\" if detected else \"Missing\"\n    })\n\n# ------------------------------------------------------------------\n# Canonical schema audit table (Quarto renders automatically)\n# ------------------------------------------------------------------\nschema_audit = pd.DataFrame(records)\n\nschema_audit\n```\n\n### Build canonical timestamp and numeric duration (audit-quality checks)\n\n\n#### Validate required schema\n\n```{python}\n# Validate required columns exist (hard audit gate)\n\nrequired_cols = [\n    \"date_stamp\",\n    \"time\",\n    \"duration_in_seconds\",\n    \"dialled_phone_number\"\n]\n\nmissing_required = [c for c in required_cols if c not in df.columns]\n\nschema_validation = pd.DataFrame({\n    \"Required column\": required_cols,\n    \"Present in dataset\": [c in df.columns for c in required_cols]\n})\n\nschema_validation\n```\n\n```{python}\n# Stop execution if schema is invalid\nassert len(missing_required) == 0, f\"Missing required columns: {missing_required}\"\n```\n\n---\n\n#### Build canonical fields\n\n```{python}\n# Canonical timestamp\ndf[\"call_ts\"] = pd.to_datetime(\n    df[\"date_stamp\"].astype(str).str.strip() + \" \" +\n    df[\"time\"].astype(str).str.strip(),\n    errors=\"coerce\"\n)\n\n# Canonical duration (seconds)\ndf[\"dur_sec\"] = pd.to_numeric(\n    df[\"duration_in_seconds\"],\n    errors=\"coerce\"\n)\n```\n\n---\n\n#### Data integrity audit\n\n```{python}\nbad_ts = int(df[\"call_ts\"].isna().sum())\nbad_dur = int(df[\"dur_sec\"].isna().sum())\nnegative_dur = int((df[\"dur_sec\"] < 0).sum())\n\ntimestamp_min = df[\"call_ts\"].min()\ntimestamp_max = df[\"call_ts\"].max()\n\nintegrity_audit = pd.DataFrame({\n    \"Check\": [\n        \"Timestamp parse failures\",\n        \"Duration numeric failures\",\n        \"Negative durations\",\n        \"Timestamp range (min)\",\n        \"Timestamp range (max)\",\n        \"Timestamp success rate\",\n        \"Duration success rate\"\n    ],\n    \"Value\": [\n        bad_ts,\n        bad_dur,\n        negative_dur,\n        str(timestamp_min),\n        str(timestamp_max),\n        f\"{df['call_ts'].notna().mean():.3f}\",\n        f\"{df['dur_sec'].notna().mean():.3f}\"\n    ]\n})\n\nintegrity_audit\n```\n\n---\n\n#### Hard integrity gates\n\n```{python}\nassert bad_ts == 0, \"Timestamp parsing failed for some rows.\"\nassert bad_dur == 0, \"Duration numeric conversion failed for some rows.\"\nassert negative_dur == 0, \"Negative durations detected.\"\n```\n\n\n\n### Hash a stable `contact_id` (do not export raw identifiers)\n\n```{python}\ndef stable_contact_id(x: int | str) -> str:\n    s = str(x).encode(\"utf-8\")\n    return hashlib.sha256(s).hexdigest()[:12]\n\ndf[\"contact_id\"] = df[\"dialled_phone_number\"].map(stable_contact_id)\n\nprint(\"Unique contacts (hashed):\", int(df[\"contact_id\"].nunique()))\nassert df[\"contact_id\"].isna().sum() == 0\n```\n\n### Derive time-band + open/closed-day flags (hard constraints)\n\n```{python}\ndf[\"hour\"] = df[\"call_ts\"].dt.hour\ndf[\"dow\"] = df[\"call_ts\"].dt.dayofweek  # Mon=0..Sun=6\n\ndef assign_time_band(hour: int) -> str:\n    if TIME_BANDS[\"early_morning\"][0] <= hour <= TIME_BANDS[\"early_morning\"][1]:\n        return \"early_morning\"\n    if TIME_BANDS[\"business_hours\"][0] <= hour <= TIME_BANDS[\"business_hours\"][1]:\n        return \"business_hours\"\n    if TIME_BANDS[\"evening\"][0] <= hour <= TIME_BANDS[\"evening\"][1]:\n        return \"evening\"\n    if TIME_BANDS[\"late_night\"][0] <= hour <= TIME_BANDS[\"late_night\"][1]:\n        return \"late_night\"\n    raise ValueError(f\"Hour out of range: {hour}\")\n\ndf[\"time_band\"] = df[\"hour\"].map(assign_time_band)\n\ndf[\"is_open_day\"] = df[\"dow\"].isin(OPEN_DOW).astype(int)\ndf[\"is_closed_day\"] = df[\"dow\"].isin(CLOSED_DOW).astype(int)\n\n# sanity checks: mutually exclusive and exhaustive\nassert set(df[\"time_band\"].unique()) == {\"early_morning\", \"business_hours\", \"evening\", \"late_night\"}\nassert int((df[\"is_open_day\"] + df[\"is_closed_day\"]).min()) == 1\nassert int((df[\"is_open_day\"] + df[\"is_closed_day\"]).max()) == 1\n\ndf[[\"call_ts\",\"hour\",\"dow\",\"time_band\",\"is_open_day\",\"is_closed_day\"]].head(5)\n```\n\n### Executive summary table (stakeholder-ready)\n\n```{python}\ncat_col = \"category\" if \"category\" in df.columns else None\n\nsummary = pd.DataFrame({\n    \"Metric\": [\n        \"Rows (calls)\",\n        \"Columns\",\n        \"Unique contacts (hashed)\",\n        \"Timestamp parsable rate\",\n        \"Timestamp range (min)\",\n        \"Timestamp range (max)\",\n        \"Duration numeric rate\",\n        \"Duplicate rows\",\n        \"Category column present?\",\n        \"Category non-null rate (if present)\",\n        \"Data readiness score (ts & dur)\"\n    ],\n    \"Value\": [\n        int(df.shape[0]),\n        int(df.shape[1]),\n        int(df[\"contact_id\"].nunique()),\n        f\"{df['call_ts'].notna().mean():.3f}\",\n        str(df[\"call_ts\"].min()),\n        str(df[\"call_ts\"].max()),\n        f\"{df['dur_sec'].notna().mean():.3f}\",\n        int(df.duplicated().sum()),\n        bool(cat_col is not None),\n        f\"{df[cat_col].notna().mean():.3f}\" if cat_col else \"N/A\",\n        f\"{(df['call_ts'].notna() & df['dur_sec'].notna()).mean():.3f}\",\n    ]\n})\nsummary\n```\n\n### Visual 1 — Monthly call volume (coverage)\n\n```{python}\n#| echo: false\nmonthly = (\n    df.assign(month_id=lambda x: x[\"call_ts\"].dt.to_period(\"M\").astype(str))\n      .groupby(\"month_id\")\n      .size()\n      .reset_index(name=\"n_calls\")\n)\n\nplt.figure()\nplt.plot(monthly[\"month_id\"], monthly[\"n_calls\"])\nplt.xticks(rotation=90)\nplt.title(\"Monthly call volume (data coverage)\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Number of calls\")\nplt.tight_layout()\nplt.show()\n\nmonthly.tail(12)\n```\n\n### Visual 2 — Duration distribution (outlier awareness)\n\n```{python}\n#| echo: false\nplt.figure()\nplt.hist(df[\"dur_sec\"], bins=60)\nplt.title(\"Call duration distribution (seconds)\")\nplt.xlabel(\"Duration (seconds)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\nplt.figure()\nplt.boxplot(df[\"dur_sec\"], vert=True)\nplt.title(\"Call duration boxplot (seconds)\")\nplt.ylabel(\"Duration (seconds)\")\nplt.tight_layout()\nplt.show()\n\ndf[\"dur_sec\"].quantile([0, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1.0]).to_frame(\"duration_seconds\")\n```\n\n### Visual 3 — Time-band mix + open/closed-day mix (hard-constraint diagnostics)\n\n```{python}\n#| echo: false\nband_counts = df[\"time_band\"].value_counts().sort_index()\nday_counts = df[\"is_closed_day\"].map({0: \"Open day (Mon–Sat)\", 1: \"Closed day (Sun)\"}).value_counts()\n\nplt.figure()\nplt.bar(band_counts.index.astype(str), band_counts.values)\nplt.title(\"Calls by time band (hard constraints)\")\nplt.xlabel(\"Time band\")\nplt.ylabel(\"Number of calls\")\nplt.tight_layout()\nplt.show()\n\nplt.figure()\nplt.bar(day_counts.index.astype(str), day_counts.values)\nplt.title(\"Calls on open vs closed days (hard constraint)\")\nplt.xlabel(\"Day type\")\nplt.ylabel(\"Number of calls\")\nplt.tight_layout()\nplt.show()\n\nband_counts.to_frame(\"n_calls\"), day_counts.to_frame(\"n_calls\")\n```\n\n---\n\n\n::: callout-note\n## Phase 1 — Audit summary (stakeholder-ready)\n\n**Dataset health (ready for segmentation):**\n\n- **24,952 call records** across **2,091 unique contacts** (hashed).\n- **Time coverage:** 2022-01-01 → 2024-10-04 (timestamps parse perfectly; durations fully numeric).\n- **No duplicates** detected; **category is fully populated**.\n- **Data readiness score = 1.000** (timestamp + duration fully usable).\n\n**Behavioral signals visible already:**\n\n- Calls concentrate in **business hours** (20,624 / 24,952 ≈ 82.7%).\n- Smaller but meaningful activity in **evening** (2,877 ≈ 11.5%) and **early morning** (1,071 ≈ 4.3%); **late night** is rare (380 ≈ 1.5%).\n- Most calls happen on **open days (Mon–Sat)** (23,300 ≈ 93.4%); **Sunday** exists but is limited (1,652 ≈ 6.6%).\n\n**Duration profile (important for feature design):**\n\n- Typical call is short: median **34s**, 75th percentile **84s**.\n- Heavy tail: 99th percentile **~1,274s**, max **7,200s** (2 hours) → we will use **robust statistics** (median, IQR, log transforms, and long-call share) instead of relying only on means.\n\n**Coverage caution (modeling hygiene):**\n\n- Monthly volume is broadly stable until mid-2024, then shows sharp drops in some late months.\n- We will treat this as **coverage/behavior regime change** and rely on **contact-level aggregates + recency** (not month-level trends) for clustering.\n\n### Transition to Phase 2\nNext, we convert the call-level table into a **contact-level feature table** (1 row per contact).  \nThis is the modeling dataset for clustering.\n:::\n\n---\n\n## Phase 2 — Contact-level features\n\nThis phase transforms the **call-level dataset** into a **contact-level feature table**, producing **one row per contact**. These features capture activity intensity, temporal patterns, and category distributions, forming the basis for clustering and behavioral segmentation.\n\n---\n\n### Guardrails & Setup\n\nWe first ensure all required columns from Phase 1 exist. This prevents accidental execution on incomplete data.\n\n```{python}\n# Guardrails: ensure critical columns exist\nassert \"contact_id\" in df.columns, \"contact_id missing (run Phase 1 first).\"\nassert \"call_ts\" in df.columns, \"call_ts missing (run Phase 1 first).\"\nassert \"dur_sec\" in df.columns, \"dur_sec missing (run Phase 1 first).\"\nassert \"time_band\" in df.columns, \"time_band missing (run Phase 1 first).\"\nassert \"is_open_day\" in df.columns and \"is_closed_day\" in df.columns, \"open/closed flags missing.\"\n```\n\nHelper for safe division (avoids division by zero):\n\n```{python}\ndef safe_div(a, b):\n    return np.where(b == 0, 0.0, a / b)\n```\n\nReference timestamp for recency calculations:\n\n```{python}\nASOF_TS = df[\"call_ts\"].max()\n```\n\n---\n\n### Base Aggregations\n\nCompute **core contact-level metrics**: total calls, active days, duration statistics, and recency/tenure measures.\n\n```{python}\ng = df.groupby(\"contact_id\", as_index=False)\n\nbase = g.agg(\n    n_calls=(\"call_ts\", \"size\"),\n    n_days_active=(\"call_ts\", lambda x: x.dt.date.nunique()),\n    first_call_ts=(\"call_ts\", \"min\"),\n    last_call_ts=(\"call_ts\", \"max\"),\n    total_dur_sec=(\"dur_sec\", \"sum\"),\n    mean_dur_sec=(\"dur_sec\", \"mean\"),\n    median_dur_sec=(\"dur_sec\", \"median\"),\n    p90_dur_sec=(\"dur_sec\", lambda x: x.quantile(0.90)),\n)\n\n# Recency and tenure (days)\nbase[\"recency_days\"] = (ASOF_TS - base[\"last_call_ts\"]).dt.total_seconds() / (24 * 3600)\nbase[\"tenure_days\"] = (base[\"last_call_ts\"] - base[\"first_call_ts\"]).dt.total_seconds() / (24 * 3600)\n\n# Call rates\nbase[\"calls_per_active_day\"] = safe_div(base[\"n_calls\"], base[\"n_days_active\"])\nbase[\"calls_per_tenure_day\"] = safe_div(base[\"n_calls\"], (base[\"tenure_days\"] + 1.0))\n\n# Log transforms to stabilize heavy-tailed distributions\nbase[\"log_n_calls\"] = np.log1p(base[\"n_calls\"])\nbase[\"log_total_dur\"] = np.log1p(base[\"total_dur_sec\"])\nbase[\"log_median_dur\"] = np.log1p(base[\"median_dur_sec\"])\n```\n\n---\n\n### Long-call Share\n\nDefine **long calls** as those exceeding the 95th percentile. Compute each contact's share of long calls to capture **tail behavior**.\n\n```{python}\nLONG_CALL_THRESHOLD = float(df[\"dur_sec\"].quantile(0.95))\ndf[\"_is_long_call\"] = (df[\"dur_sec\"] >= LONG_CALL_THRESHOLD).astype(int)\n\nlong_share = (\n    df.groupby(\"contact_id\", as_index=False)\n      .agg(long_call_share=(\"_is_long_call\", \"mean\"))\n)\n```\n\n---\n\n### Time-Band Mix\n\nContacts may have different temporal activity patterns. We compute **proportions of calls per time band** (business hours, evening, late night, early morning).\n\n```{python}\nband_mix = (\n    pd.crosstab(df[\"contact_id\"], df[\"time_band\"], normalize=\"index\")\n      .reset_index()\n      .rename(columns={\n          \"business_hours\": \"share_business_hours\",\n          \"evening\": \"share_evening\",\n          \"late_night\": \"share_late_night\",\n          \"early_morning\": \"share_early_morning\",\n      })\n)\n\n# Ensure all expected columns exist\nfor col in [\"share_business_hours\",\"share_evening\",\"share_late_night\",\"share_early_morning\"]:\n    if col not in band_mix.columns:\n        band_mix[col] = 0.0\n```\n\n---\n\n### Open vs Closed Day Mix\n\nCapture **day-of-week activity patterns**. Open vs. closed days are mutually exclusive and sum to 1 per contact.\n\n```{python}\nday_mix = (\n    df.groupby(\"contact_id\", as_index=False)\n      .agg(\n          share_open_day=(\"is_open_day\", \"mean\"),\n          share_closed_day=(\"is_closed_day\", \"mean\")\n      )\n)\n\n# Sanity check\nday_mix[\"_sum\"] = day_mix[\"share_open_day\"] + day_mix[\"share_closed_day\"]\nassert np.allclose(day_mix[\"_sum\"], 1.0, atol=1e-9), \"Open+Closed shares not summing to 1.\"\nday_mix = day_mix.drop(columns=[\"_sum\"])\n```\n\n---\n\n### Category Mix (Optional)\n\nFor interpretation, compute the **top-K category proportions per contact**, preserving sparsity and numeric representation.\n\n```{python}\nTOPK = 8\ntop_categories = df[\"category\"].value_counts().head(TOPK).index.tolist()\n\ncat_tab = (\n    pd.crosstab(df[\"contact_id\"], df[\"category\"])\n      .reindex(columns=top_categories, fill_value=0)\n)\n\n# Convert to proportions\ncat_mix = (cat_tab.div(cat_tab.sum(axis=1).replace(0, 1), axis=0)\n                 .reset_index()\n                 .rename(columns={c: f\"share_cat_{c}\" for c in top_categories}))\n```\n\n---\n\n### Merge All Features\n\nCombine **base, long-call, time-band, day-mix, and category features** into a single contact-level table.\n\n```{python}\ncontact_features = (\n    base.merge(long_share, on=\"contact_id\", how=\"left\")\n        .merge(band_mix, on=\"contact_id\", how=\"left\")\n        .merge(day_mix, on=\"contact_id\", how=\"left\")\n        .merge(cat_mix, on=\"contact_id\", how=\"left\")\n)\n\n# Fill any leftover NaNs\nnum_cols = contact_features.select_dtypes(include=[np.number]).columns\ncontact_features[num_cols] = contact_features[num_cols].fillna(0.0)\n\n# Preview\nprint(\"contact_features.shape =\", contact_features.shape)\nprint(\"Long-call threshold (95th pct, seconds) =\", LONG_CALL_THRESHOLD)\n\ncontact_features.head(5).T\n```\n\n---\n\n### Quick Feature Audit\n\nSanity check **feature completeness and distribution**.\n\n```{python}\nfeature_audit = pd.DataFrame({\n    \"n_contacts\": [int(contact_features.shape[0])],\n    \"n_features_total\": [int(contact_features.shape[1])],\n    \"any_missing_numeric\": [bool(contact_features.select_dtypes(include=[np.number]).isna().any().any())],\n    \"min_calls\": [int(contact_features[\"n_calls\"].min())],\n    \"median_calls\": [float(contact_features[\"n_calls\"].median())],\n    \"max_calls\": [int(contact_features[\"n_calls\"].max())],\n    \"median_recency_days\": [float(contact_features[\"recency_days\"].median())],\n})\nfeature_audit\n```\n\n---\n\n### Top Activity Contacts\n\nInspect **most active contacts** without exposing raw identifiers. This helps validate engineered features and distribution of behavioral metrics.\n\n```{python}\ntop_activity = contact_features.sort_values(\"n_calls\", ascending=False).head(10)[\n    [\"contact_id\",\"n_calls\",\"n_days_active\",\"total_dur_sec\",\"median_dur_sec\",\"recency_days\",\n     \"share_business_hours\",\"share_evening\",\"share_early_morning\",\"share_late_night\",\n     \"share_open_day\",\"share_closed_day\",\"long_call_share\"]\n]\ntop_activity.T\n```\n\n---\n\n::: callout-note\n## Phase 2 — Stakeholder-ready summary (what we learned)\n\nWe successfully converted **24,952 calls** into a **contact-level modeling table** with **2,091 contacts** and **28 features**.\n\n### What the Phase 2 feature table represents (business meaning)\nEach row is a **contact** (hashed ID), and the features capture:\n\n- **Volume / intensity:** `n_calls`, `n_days_active`, `calls_per_active_day`, `calls_per_tenure_day`\n- **Relationship depth / effort proxy:** `total_dur_sec`, `median_dur_sec`, `p90_dur_sec`, plus log versions\n- **Recency / dormancy risk:** `recency_days` (how long since last call)\n- **Time preference / accessibility:** shares by time band (business/evening/early/late)\n- **Open vs closed day behavior:** `share_open_day`, `share_closed_day`\n- **Tail behavior:** `long_call_share` where “long” is **≥ 396s** (95th percentile)\n- **Category mix (interpretation aid):** proportions of top-8 categories per contact\n\n### Sanity checks passed (safe to proceed)\n- No missing numeric values in engineered features.\n- Open + closed day shares sum to 1 per contact (validated).\n- Time-band shares exist for every contact (columns ensured even if 0).\n\n### What the results already hint at (without clustering yet)\n- **Highly skewed activity:** median contact has **2 calls**, but the top contact has **2,413** calls → we must **scale/transform** before K-means.\n- **Recency is large for many contacts:** median `recency_days ≈ 445` → many contacts are dormant; segmentation must separate “active” vs “inactive”.\n- Top activity contacts show diverse patterns:\n  - some are high-volume but mostly business hours\n  - some show meaningful evening/early activity\n  - some have high `long_call_share` (deep conversations or issue resolution)\n\n### Modeling implication\n\nAt this point we have a **contact-level feature table** with strong business meaning (volume, duration, recency, timing).  \nBefore clustering, we must build a **clean clustering matrix** $X$ so distance-based algorithms behave correctly.\n\nWe will build $X$ such that it:\n\n1) **Excludes non-model columns**  \n   - No timestamps (`first_call_ts`, `last_call_ts`) and no raw identifiers.  \n   - Category mix is kept for interpretation, but excluded from default training to avoid “baking in labels”.\n\n2) **Controls heavy tails and near-constant features**  \n   - Phone-call behavior is highly skewed (a few contacts dominate calls/duration).  \n   - We apply robust transforms already engineered (log features) and use a *robust preprocessing pipeline*.  \n   - We also drop **near-constant (IQR≈0) features** to avoid unstable scaling and distance domination.\n\n3) **Scales fairly for distance-based clustering**  \n   - We use **median imputation** (robust, future-proof if new features introduce missingness).  \n   - We use **RobustScaler (median/IQR)** instead of StandardScaler to reduce outlier influence.\n\n---\n\n### Transition\n\nPhase 3 converts `contact_features` into **clustering-ready inputs** and produces:\n\n- a **feature-block dictionary** for later sensitivity tests (volume vs duration vs timing vs recency),\n- a cleaned, scaled matrix **`X`** and aligned **`feature_names`** for reproducibility and interpretation.\n:::\n\n---\n\n\n## Phase 3 — Preprocess (bullet-proof, distance-based clustering ready)\n\nThis phase prepares the **contact-level feature table** for distance-based clustering by producing a stable numeric matrix **`X`** (rows = contacts, columns = standardized features).\n\nWe do four things:\n\n1) **Define feature blocks** (auditable and reusable for sensitivity tests later)  \n2) **Build the default model matrix** (exclude timestamps and category mix by default)  \n3) **Stabilize heavy tails** (clip extreme values in raw space)  \n4) **Impute + robust-scale** (median imputation + RobustScaler) with diagnostics\n\n::: callout-tip\n### Why RobustScaler (not StandardScaler)\nContact behaviour is heavy-tailed (a few contacts dominate activity).  \nDistance-based clustering is scale-sensitive. **RobustScaler uses median and IQR**, reducing outlier influence and producing more stable distances.\n:::\n\n---\n\n### Freeze an interpretation copy (read-only view)\n\n```{python}\ncontact_features_view = contact_features.copy()\n```\n\n---\n\n### Define feature blocks (for interpretation and sensitivity analysis)\n\n```{python}\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n```\n\n```{python}\n# -----------------------------\n# Feature blocks (business meaning)\n# -----------------------------\nvolume_features = [\n    \"n_calls\", \"n_days_active\", \"calls_per_active_day\", \"calls_per_tenure_day\", \"log_n_calls\"\n]\n\nduration_features = [\n    \"total_dur_sec\", \"mean_dur_sec\", \"median_dur_sec\", \"p90_dur_sec\",\n    \"log_total_dur\", \"log_median_dur\", \"long_call_share\"\n]\n\nrecency_features = [\n    \"recency_days\", \"tenure_days\"\n]\n\ntiming_features = [\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\n# Category mix is useful for interpretation, but excluded from default model training\ncategory_features = [c for c in contact_features.columns if c.startswith(\"share_cat_\")]\n\nFEATURE_BLOCKS = {\n    \"volume\": volume_features,\n    \"duration\": duration_features,\n    \"recency\": recency_features,\n    \"timing\": timing_features,\n    \"category\": category_features\n}\n\n# Validate engineered feature presence (hard gate)\nmissing = {k: [f for f in v if f not in contact_features.columns] for k, v in FEATURE_BLOCKS.items()}\nmissing = {k: v for k, v in missing.items() if len(v) > 0}\nassert len(missing) == 0, f\"Missing engineered features: {missing}\"\n\nFEATURE_BLOCKS\n```\n\n---\n\n### Select default model features (no timestamps, no category mix by default)\n\nWe train clustering on behaviour (volume, duration, recency, timing).\nCategory mix stays available for interpretation and later sensitivity checks.\n\n```{python}\nMODEL_FEATURES = (\n    FEATURE_BLOCKS[\"volume\"]\n    + FEATURE_BLOCKS[\"duration\"]\n    + FEATURE_BLOCKS[\"recency\"]\n    + FEATURE_BLOCKS[\"timing\"]\n)\n\nX_raw = contact_features[MODEL_FEATURES].copy()\nprint(\"X_raw.shape =\", X_raw.shape)\n```\n\n---\n\n### Diagnostics before preprocessing (missingness + inf safety)\n\n```{python}\n# Missingness inspection (future-proof if new features introduce NaNs)\nmiss = X_raw.isna().sum().sort_values(ascending=False)\nmiss_table = miss[miss > 0].to_frame(\"n_missing\")\nmiss_table\n```\n\n```{python}\n# Inf / -Inf inspection (must not exist)\nhas_inf = np.isinf(X_raw.to_numpy(dtype=float)).any()\nprint(\"Any inf in X_raw:\", bool(has_inf))\nassert not has_inf, \"Infinite values detected in X_raw (must fix before scaling).\"\n```\n\n---\n\n### Stability strategy (recommended): drop only truly constant columns + clip extremes\n\n**Important correction:**\nDropping “low IQR” features is too aggressive in contact data (many contacts have 1–2 calls, which makes shares look constant).\nInstead we:\n\n* drop only **truly constant** columns (no information),\n* clip each feature to **p01–p99** in raw units (winsorization),\n* then apply RobustScaler.\n\n```{python}\n# A) Drop truly constant columns only (no information)\nnunique = X_raw.nunique(dropna=False)\nconstant_cols = nunique[nunique <= 1].index.tolist()\nprint(\"Truly-constant cols (drop):\", constant_cols)\n\nX_raw2 = X_raw.drop(columns=constant_cols) if constant_cols else X_raw.copy()\n\n# B) Clip extremes in RAW space to stabilize heavy tails\nCLIP_LO = 0.01\nCLIP_HI = 0.99\n\nclip_info = []\nX_clip = X_raw2.copy()\n\nfor col in X_clip.columns:\n    lo = float(X_clip[col].quantile(CLIP_LO))\n    hi = float(X_clip[col].quantile(CLIP_HI))\n    X_clip[col] = X_clip[col].clip(lower=lo, upper=hi)\n    clip_info.append({\"feature\": col, \"p01\": lo, \"p99\": hi})\n\nclip_table = pd.DataFrame(clip_info).sort_values(\"feature\").reset_index(drop=True)\n\nX_clip.shape, clip_table.head(10)\n```\n\n---\n\n### Median imputation + RobustScaler\n\n```{python}\n# Median imputation is robust (and future-proof for features that may introduce NaNs later)\nimputer = SimpleImputer(strategy=\"median\")\n\n# RobustScaler uses median/IQR to reduce outlier influence\nscaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n\nX_imputed = imputer.fit_transform(X_clip)\nX = scaler.fit_transform(X_imputed)\n\nprint(\"X.shape =\", X.shape)\nprint(\"Any NaN in X:\", bool(np.isnan(X).any()))\nassert not np.isnan(X).any(), \"NaNs remain after preprocessing (must fix).\"\n\nprint(\"max_abs_after_scaling =\", float(np.max(np.abs(X))))\n```\n\n---\n\n### Diagnostics: identify any dominating feature in scaled space\n\nIf one feature still dominates distances, we detect it explicitly.\n\n```{python}\nfeature_names = list(X_clip.columns)\nmax_abs_by_feature = np.max(np.abs(X), axis=0)\n\ndominance = (\n    pd.DataFrame({\n        \"feature\": feature_names,\n        \"max_abs_scaled\": max_abs_by_feature,\n        \"raw_p01\": clip_table.set_index(\"feature\").loc[feature_names, \"p01\"].values,\n        \"raw_p99\": clip_table.set_index(\"feature\").loc[feature_names, \"p99\"].values,\n        \"raw_iqr\": (X_raw2.quantile(0.75) - X_raw2.quantile(0.25)).reindex(feature_names).values,\n        \"n_unique\": X_raw2.nunique().reindex(feature_names).values\n    })\n    .sort_values(\"max_abs_scaled\", ascending=False)\n    .reset_index(drop=True)\n)\n\ndominance.head(12)\n```\n\n---\n\n### Preprocess audit (compact, decision-useful)\n\n```{python}\npreprocess_audit = pd.DataFrame({\n    \"n_contacts\": [int(X.shape[0])],\n    \"n_model_features\": [int(X.shape[1])],\n    \"n_constant_dropped\": [int(len(constant_cols))],\n    \"constant_dropped\": [\", \".join(constant_cols) if constant_cols else \"None\"],\n    \"clip_low_quantile\": [CLIP_LO],\n    \"clip_high_quantile\": [CLIP_HI],\n    \"n_features_with_missing_before\": [int((X_raw.isna().sum() > 0).sum())],\n    \"total_missing_cells_before\": [int(X_raw.isna().sum().sum())],\n    \"max_abs_after_scaling\": [float(np.max(np.abs(X)))],\n})\n\npreprocess_audit\n```\n\n---\n\n### Store preprocessing artifacts (in-memory, safe by default)\n\n```{python}\nprep = {\n    \"MODEL_FEATURES\": MODEL_FEATURES,\n    \"FEATURE_BLOCKS\": FEATURE_BLOCKS,\n    \"feature_names\": feature_names,\n    \"imputer\": imputer,\n    \"scaler\": scaler,\n    \"X_raw\": X_raw2,      # before clipping (audit)\n    \"X_clip\": X_clip,     # clipped raw matrix (audit)\n    \"X\": X,               # final clustering matrix\n    \"clip_table\": clip_table,\n    \"dominance\": dominance,\n    \"preprocess_audit\": preprocess_audit\n}\n\n# Raw-unit summary (post-clip = what the scaler actually sees)\nraw_summary = X_clip.describe(percentiles=[0.5, 0.9, 0.95, 0.99]).T\nraw_summary = raw_summary[[\"count\", \"mean\", \"std\", \"min\", \"50%\", \"90%\", \"95%\", \"99%\", \"max\"]]\nraw_summary.head(12)\n```\n\n\n\n---\n\n\n### A Final numeric stability guardrail\n\nEven after robust scaling, distance-based clustering can still be dominated by a small number of extreme points.  \nTo make the pipeline **bullet-proof**, we apply a final cap in standardized space so no single feature can overwhelm Euclidean distances.\n\n```{python}\n# Final stability cap in scaled space (portfolio-grade guardrail)\nSCALED_CAP = 10.0\n\nX_uncapped = prep[\"X\"]\nX_capped = np.clip(X_uncapped, -SCALED_CAP, SCALED_CAP)\n\ncap_audit = pd.DataFrame({\n    \"scaled_cap\": [SCALED_CAP],\n    \"max_abs_before\": [float(np.max(np.abs(X_uncapped)))],\n    \"max_abs_after\": [float(np.max(np.abs(X_capped)))],\n})\n\n# Update the matrix used downstream (keep uncapped for audit)\nprep[\"X_uncapped\"] = X_uncapped\nprep[\"X\"] = X_capped\ncap_audit\n```\n\n---\n\n\n::: callout-note\n## Phase 3 — Sign-off (ready for K-selection)\n\nWe produced a stable clustering matrix `prep[\"X\"]` for **2,091 contacts × 20 features**.\n\n**Robustness guarantees:**\n- No missing values or infinities\n- Heavy tails controlled by **p01–p99 clipping**\n- Distance stability guaranteed by **post-scaling cap** (max absolute value ≤ 10)\n\n**Next:** Phase 4 chooses the number of clusters $K$ using a multi-metric selection protocol (inertia, silhouette, Davies–Bouldin) on `prep[\"X\"]`.\n:::\n\n\n\n## Phase 4 — Choose $K$ (model selection for clustering)\n\nChoosing $K$ is a **model selection** problem: different values of $K$ define different segmentations.\nWe evaluate candidate $K$ values using three complementary criteria:\n\n- **Inertia (SSE)**: decreases with $K$; we look for an “elbow” (diminishing returns).\n- **Silhouette**: higher is better; measures separation vs cohesion.\n- **Davies–Bouldin**: lower is better; penalizes overlapping clusters.\n\n::: callout-important\nWe compute metrics on the **preprocessed matrix** `prep[\"X\"]` (robust-scaled and capped).  \nThis ensures the $K$ decision reflects behavioural structure rather than raw-unit dominance.\n:::\n\n---\n\n### Compute $K$-search metrics\n\n```{python}\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\n\nX = prep[\"X\"]  # capped, clustering-ready matrix\n\n# Candidate K range (employer-ready default)\nK_MIN, K_MAX = 2, 12\n\nrows = []\nfor k in range(K_MIN, K_MAX + 1):\n    km = KMeans(\n        n_clusters=k,\n        n_init=50,         # more restarts = more reliable\n        random_state=42\n    )\n    labels = km.fit_predict(X)\n\n    inertia = float(km.inertia_)\n    sil = float(silhouette_score(X, labels))\n    db = float(davies_bouldin_score(X, labels))\n\n    rows.append({\n        \"k\": k,\n        \"inertia\": inertia,\n        \"silhouette\": sil,\n        \"davies_bouldin\": db\n    })\n\nk_metrics = pd.DataFrame(rows)\n\n# Add a simple \"elbow help\": marginal gain in inertia\nk_metrics[\"inertia_drop\"] = k_metrics[\"inertia\"].shift(1) - k_metrics[\"inertia\"]\nk_metrics[\"inertia_drop_pct\"] = (k_metrics[\"inertia_drop\"] / k_metrics[\"inertia\"].shift(1)) * 100\n\nk_metrics\n```\n\n---\n\n### Visualize metrics (elbow + quality trade-offs)\n\n```{python}\n#| echo: false\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.plot(k_metrics[\"k\"], k_metrics[\"inertia\"], marker=\"o\")\nplt.title(\"Elbow plot: inertia vs K\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Inertia (SSE)\")\nplt.tight_layout()\nplt.show()\n\nplt.figure()\nplt.plot(k_metrics[\"k\"], k_metrics[\"silhouette\"], marker=\"o\")\nplt.title(\"Silhouette vs K (higher is better)\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Silhouette\")\nplt.tight_layout()\nplt.show()\n\nplt.figure()\nplt.plot(k_metrics[\"k\"], k_metrics[\"davies_bouldin\"], marker=\"o\")\nplt.title(\"Davies–Bouldin vs K (lower is better)\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Davies–Bouldin\")\nplt.tight_layout()\nplt.show()\n```\n\n---\n\n### Candidate shortlist (data-driven)\n\nWe shortlist $K$ values that are plausible trade-offs:\n\n* among the **top silhouette** scores,\n* among the **lowest Davies–Bouldin** scores,\n* and not far beyond the elbow (where inertia gains flatten).\n\n\n```{python}\n# ----------------------------------------\n# Candidate shortlist — modern, auditable\n# ----------------------------------------\n\n# Identify top-K by each metric\ntop_sil = k_metrics.nlargest(5, \"silhouette\").copy()\ntop_sil[\"reason\"] = \"top_silhouette\"\n\nlow_db = k_metrics.nsmallest(5, \"davies_bouldin\").copy()\nlow_db[\"reason\"] = \"low_davies_bouldin\"\n\n# Combine and mark duplicates\nshortlist = pd.concat([top_sil, low_db]).reset_index(drop=True)\n\n# If a K appears in both, update reason\nshortlist = (\n    shortlist.groupby(\"k\", as_index=False)\n    .agg({\n        \"silhouette\": \"first\",\n        \"davies_bouldin\": \"first\",\n        \"inertia\": \"first\",\n        \"reason\": lambda x: \" & \".join(sorted(set(x)))\n    })\n    .sort_values(\"k\")\n    .reset_index(drop=True)\n)\n\n# Round metrics for readability\nshortlist[[\"silhouette\", \"davies_bouldin\", \"inertia\"]] = shortlist[\n    [\"silhouette\", \"davies_bouldin\", \"inertia\"]\n].round(4)\n\nshortlist\n\n```\n\n\n---\n\n\n\n### What the metrics say (from first principles)\n\n#### Inertia (elbow)\nHuge drops from **K=2 → 3** (~27%) and **3 → 4** (~30%).  \nAfter **K=4**, the drop collapses to ~12% and then <~11% thereafter.  \nThat’s a classic **elbow at K≈4**.\n\n#### Silhouette (separation vs cohesion)\n- **K=2:** 0.651 (very strong)  \n- **K=3:** 0.646 (very strong)  \n- **K=4:** 0.589 (still strong)  \n\nThen it falls sharply (e.g., **K=5** = 0.483 and continues down).  \nStructure is strongest for small K, especially **2–4**.\n\n#### Davies–Bouldin (overlap; lower is better)\n- Best is **K=2** (0.747), then **K=4** (0.772), then **K=3** (0.829).  \nAfter that, the metric worsens notably.\n\n---\n\n### Decision\nChoose **K=4** (recommended).  \n\n- **K=2:** too coarse for actionability (“low vs high activity”)  \n- **K=3:** better but still merges distinct behaviours (“steady frequent” vs “bursty intense”)  \n- **K=4:** lands at the elbow, with strong silhouette & Davies–Bouldin — best balance of interpretability, actionability, and metric support.\n\n::: callout-important\n## Decision: choose $K=4$\n\nWe select **$K=4$** as the operational segmentation because it balances:\n\n- **Structure quality:** strong silhouette (0.589) and low Davies–Bouldin (0.772)  \n- **Parsimony:** clear elbow around **K=4** (large inertia drops up to 4, then diminishing returns)  \n- **Actionability:** more useful than K=2 while avoiding over-fragmentation at higher K\n\n**Next:** fit K-means with **K=4** and interpret clusters using feature-block summaries (volume, duration, recency, timing).\n:::\n\n---\n\n### Optional: why we may explore $K=5$ (exploratory only)\n\n::: callout-warning\n$K=5$ is **not** recommended as the default operational segmentation in this project.\n\nCompared to $K=4$, separation quality declines noticeably:\n\n- Silhouette: **0.589 → 0.483** (weaker cohesion/separation)\n- Davies–Bouldin: **0.772 → 0.927** (more overlap between clusters)\n\nWe may still explore $K=5$ **only as an exploratory lens** when stakeholders want finer distinctions inside high-activity groups (e.g., separating “steady frequent” from “burst-intense”). Any $K=5$ results should be presented as *additional insight* without changing the primary framework.\n:::\n\n> **Stakeholder takeaway:** K=4 remains the primary segmentation. K=5 can be used **selectively** to provide additional insights where fine distinctions are important, without altering the main operational framework.\n\n---\n\n## Phase 5 — Fit and interpret clusters (K-means, $K=4$)\n\nWe fit K-means on the **clustering matrix** `prep[\"X\"]` (robust-scaled and capped for numeric stability).  \nFor interpretation, we summarise clusters back in **business units** using `contact_features_view` (raw-unit features) and translate clusters into **stakeholder-ready segments with actions**.\n\n::: callout-important\n### Training vs interpretation (do not mix these)\n- **Training:** uses `prep[\"X\"]` (scaled + capped) so Euclidean distances are fair and stable.\n- **Interpretation:** uses raw-unit features (calls, seconds, days, shares) so segments can be explained and acted on.\n- **Privacy:** we keep `contact_id` hashed and do not merge any name/number fields into report outputs.\n:::\n\n---\n\n### Fit K-means ($K=4$) and store labels\n\n```{python}\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\n\nX = prep[\"X\"]   # final clustering matrix (scaled + capped)\nK_FINAL = 4\n\nkmeans = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42)\nlabels = kmeans.fit_predict(X)\n\n# One label per contact (hashed ID only)\ncontact_clusters = pd.DataFrame({\n    \"contact_id\": contact_features_view[\"contact_id\"].values,\n    \"cluster\": labels\n})\n\ncluster_sizes = (\n    contact_clusters[\"cluster\"]\n    .value_counts()\n    .sort_index()\n    .to_frame(\"n_contacts\")\n)\n\ncluster_sizes\n```\n\n---\n\n### Join clusters back to contact-level table (raw units)\n\n```{python}\ncontact_labeled = contact_features_view.merge(contact_clusters, on=\"contact_id\", how=\"left\")\n\n# Hard gate: every contact must get a cluster label\nassert contact_labeled[\"cluster\"].isna().sum() == 0\n\ncontact_labeled[[\"contact_id\", \"cluster\"]].head()\n```\n\n---\n\n### Cluster “cards” (robust medians + size)\n\nThese cards are stakeholder-ready: **one row per cluster**, using **medians** (robust to heavy tails).\n\n```{python}\ncard_features = [\n    # volume / intensity\n    \"n_calls\", \"n_days_active\", \"calls_per_active_day\",\n\n    # duration / relationship depth proxy\n    \"total_dur_sec\", \"median_dur_sec\", \"long_call_share\",\n\n    # recency / tenure\n    \"recency_days\", \"tenure_days\",\n\n    # timing mix\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\nmissing_cards = [c for c in card_features if c not in contact_labeled.columns]\nassert len(missing_cards) == 0, f\"Missing card features: {missing_cards}\"\n\ncards = (\n    contact_labeled\n    .groupby(\"cluster\")[card_features]\n    .median()\n    .merge(cluster_sizes, left_index=True, right_index=True)\n    .reset_index()\n)\n\ncards = cards[[\"cluster\", \"n_contacts\"] + card_features]\ncards.T\n```\n\n---\n\n### Within-cluster spread check (to avoid misleading medians)\n\nWe report 25/50/75% for a few “anchor” variables.\n\n```{python}\nspread_features = [\"n_calls\", \"total_dur_sec\", \"recency_days\", \"median_dur_sec\"]\n\nspread = (\n    contact_labeled\n    .groupby(\"cluster\")[spread_features]\n    .quantile([0.25, 0.50, 0.75])\n    .unstack(level=1)\n)\n\nspread.columns = [f\"{feat}_q{int(q*100)}\" for feat, q in spread.columns]\nspread = spread.reset_index()\n\nspread.T\n```\n\n---\n\n::: callout-tip\n\n### Why timing-share medians often look like 0 or 1\n\nMany contacts have only 1–2 calls. With tiny denominators, share features become extreme.\nSo for timing behaviour we also report **means** and **% of contacts with any activity** in each time band.\n:::\n\n---\n\n### Timing patterns (mean shares + % with any non-business-hours / Sunday activity)\n\n```{python}\nshare_cols = [\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\ntiming_mean = (\n    contact_labeled\n    .groupby(\"cluster\")[share_cols]\n    .mean()\n    .add_suffix(\"_mean\")\n    .reset_index()\n)\n\ntiming_any = (\n    contact_labeled\n    .assign(\n        any_evening=lambda x: (x[\"share_evening\"] > 0).astype(int),\n        any_early_morning=lambda x: (x[\"share_early_morning\"] > 0).astype(int),\n        any_late_night=lambda x: (x[\"share_late_night\"] > 0).astype(int),\n        any_sunday=lambda x: (x[\"share_closed_day\"] > 0).astype(int),\n    )\n    .groupby(\"cluster\")[[\"any_evening\", \"any_early_morning\", \"any_late_night\", \"any_sunday\"]]\n    .mean()\n    .mul(100)\n    .add_suffix(\"_pct_contacts\")\n    .reset_index()\n)\n\ntiming_summary = (\n    timing_mean\n    .merge(timing_any, on=\"cluster\", how=\"left\")\n    .merge(cluster_sizes.reset_index(), on=\"cluster\", how=\"left\")\n    .sort_values(\"cluster\")\n    .reset_index(drop=True)\n)\n\ntiming_summary.T\n```\n\n---\n\n### Segment names + action policy (stakeholder-ready layer)\n\nCluster IDs are arbitrary. We map clusters to segment labels using **behavioural signatures** from `cards`.\n\n```{python}\nsig = cards[[\n    \"cluster\", \"n_contacts\",\n    \"n_calls\", \"total_dur_sec\", \"median_dur_sec\", \"long_call_share\",\n    \"recency_days\", \"tenure_days\"\n]].copy()\n\n# Identify clusters by signatures (robust against label permutation)\none_off_cluster = int(sig.sort_values([\"n_calls\", \"total_dur_sec\"], ascending=[True, True]).iloc[0][\"cluster\"])\ncore_active_cluster = int(sig.sort_values([\"n_calls\", \"n_contacts\"], ascending=[False, False]).iloc[0][\"cluster\"])\nrare_deep_cluster = int(sig.sort_values([\"median_dur_sec\", \"long_call_share\"], ascending=[False, False]).iloc[0][\"cluster\"])\n\nremaining = sorted(set(sig[\"cluster\"]) - {one_off_cluster, core_active_cluster, rare_deep_cluster})\nassert len(remaining) == 1, \"Expected exactly one remaining cluster for 'warm occasional'.\"\nwarm_cluster = int(remaining[0])\n\nsegment_map = {\n    one_off_cluster: {\n        \"label\": \"One-off / low-engagement contacts\",\n        \"primary_action\": \"Deprioritize by default; automate nurture; reactivate only if new activity appears\"\n    },\n    core_active_cluster: {\n        \"label\": \"Core active relationships\",\n        \"primary_action\": \"Retention + priority servicing; assign owner; proactive follow-ups\"\n    },\n    warm_cluster: {\n        \"label\": \"Warm occasional contacts\",\n        \"primary_action\": \"Nurture cadence (monthly/quarterly); structured check-ins to increase engagement\"\n    },\n    rare_deep_cluster: {\n        \"label\": \"Rare deep conversations\",\n        \"primary_action\": \"High-touch when active; preserve context; personalized re-engagement when dormant\"\n    }\n}\n\ncluster_labels = (\n    pd.DataFrame([\n        {\"cluster\": k, \"segment_label\": v[\"label\"], \"primary_action\": v[\"primary_action\"]}\n        for k, v in segment_map.items()\n    ])\n    .merge(cluster_sizes.reset_index(), on=\"cluster\", how=\"left\")\n    .sort_values(\"cluster\")\n    .reset_index(drop=True)\n)\n\ncluster_labels\n```\n\n---\n\n### Cluster narrative (auto-generated from the cards)\n\nShort and operational: “what it looks like” + “what to do”.\n\n```{python}\ndef fmt(x, nd=1):\n    if pd.isna(x):\n        return \"NA\"\n    return f\"{float(x):.{nd}f}\"\n\ncards_idx = cards.set_index(\"cluster\")\nlabels_idx = cluster_labels.set_index(\"cluster\")\n\nnarr_rows = []\nfor c in sorted(cards_idx.index):\n    row = cards_idx.loc[c]\n    seg = labels_idx.loc[c, \"segment_label\"]\n    act = labels_idx.loc[c, \"primary_action\"]\n\n    narrative = (\n        f\"{seg}: median calls={fmt(row['n_calls'],1)}, \"\n        f\"median active days={fmt(row['n_days_active'],1)}, \"\n        f\"median total duration (sec)={fmt(row['total_dur_sec'],0)}, \"\n        f\"median call duration (sec)={fmt(row['median_dur_sec'],0)}, \"\n        f\"median recency (days)={fmt(row['recency_days'],0)}.\"\n    )\n\n    narr_rows.append({\n        \"cluster\": int(c),\n        \"segment_label\": seg,\n        \"n_contacts\": int(row[\"n_contacts\"]),\n        \"cluster_story\": narrative,\n        \"default_action\": act\n    })\n\ncluster_story = pd.DataFrame(narr_rows).sort_values(\"cluster\").reset_index(drop=True)\ncluster_story\n```\n\n---\n\n### “What decisions can the business make with these segments?”\n\nPolicy rules we can implement in a CRM without exposing identifiers.\n\n```{python}\nrules = pd.DataFrame([\n    {\n        \"segment_label\": \"Core active relationships\",\n        \"decision_use\": \"Prioritization and retention\",\n        \"operational_rule\": \"Assign owner and SLA; proactive check-ins\",\n        \"typical_pattern\": \"High calls, many active days, low recency\"\n    },\n    {\n        \"segment_label\": \"Warm occasional contacts\",\n        \"decision_use\": \"Nurture and growth\",\n        \"operational_rule\": \"Monthly/quarterly outreach; reminders; targeted offers\",\n        \"typical_pattern\": \"Moderate repeat calls, moderate tenure, mid recency\"\n    },\n    {\n        \"segment_label\": \"Rare deep conversations\",\n        \"decision_use\": \"High-touch exceptions and win-back\",\n        \"operational_rule\": \"Personalized follow-up; preserve context; targeted reactivation if dormant\",\n        \"typical_pattern\": \"Few calls but long conversations (high median duration / long-call share)\"\n    },\n    {\n        \"segment_label\": \"One-off / low-engagement contacts\",\n        \"decision_use\": \"Noise filtering and automation\",\n        \"operational_rule\": \"Exclude from priority lists; automate nurture only\",\n        \"typical_pattern\": \"One call, one day, small total duration, often old\"\n    },\n])\n\nrules\n```\n\n---\n\n### Store artifacts for later phases (robustness, sensitivity, PCA)\n\n```{python}\nprep[\"K_FINAL\"] = K_FINAL\nprep[\"kmeans\"] = kmeans\nprep[\"labels\"] = labels\n\nprep[\"contact_clusters\"] = contact_clusters\nprep[\"cluster_sizes\"] = cluster_sizes\n\nprep[\"cards\"] = cards\nprep[\"spread\"] = spread\nprep[\"timing_summary\"] = timing_summary\nprep[\"cluster_labels\"] = cluster_labels\nprep[\"cluster_story\"] = cluster_story\nprep[\"rules\"] = rules\n\nlist(prep.keys())[:15]\n```\n\n\n\n---\n\n\n## Phase 6 — Robustness and sensitivity (do the clusters “hold up”?)\n\nA good clustering result is not just “a nice plot.” It should be **stable** when we rerun the algorithm and **reasonably consistent** when we change *feature blocks*.\n\nIn this phase we test two kinds of robustness:\n\n1. **Seed stability:** If we change the random seed (different K-means initializations), do we get essentially the same segmentation?\n2. **Feature-block sensitivity:** If we drop one block (volume, duration, recency, timing), do the segments remain broadly similar?\n\nWe quantify stability using **Adjusted Rand Index (ARI)**:\n\n\n- $ARI = 1$ means two clusterings are identical up to label permutation.\n- $ARI \\approx 0$ means agreement is no better than random.\n- Negative $ARI$ can happen (worse than random agreement), usually a warning sign.\n\n::: callout-important\n### Training matrix\nAll robustness tests use `prep[\"X\"]` (robust-scaled + capped) so distances are fair and numerically stable.\n\n### Privacy\nWe do not merge names/phone numbers into any robustness outputs. We only evaluate labels as arrays.\n:::\n\n---\n\n### Seed stability test (ARI across many random seeds)\n\nWe run K-means many times with different `random_state`, always using $K = 4$, and compare each run to a baseline labeling.\n\n```{python}\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# -------------------------\n# Setup\n# -------------------------\nX = prep[\"X\"]\nK_FINAL = int(prep.get(\"K_FINAL\", 4))\n\n# Baseline labels (use Phase 5 if available)\nif \"labels\" in prep and prep.get(\"labels\", None) is not None:\n    base_labels = np.asarray(prep[\"labels\"])\nelse:\n    km0 = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42)\n    base_labels = km0.fit_predict(X)\n\n# -------------------------\n# Run many seeds\n# -------------------------\nSEEDS = list(range(0, 50))  # employer-ready default\nrows = []\n\nfor s in SEEDS:\n    km = KMeans(n_clusters=K_FINAL, n_init=50, random_state=s)\n    lab = km.fit_predict(X)\n    ari = float(adjusted_rand_score(base_labels, lab))\n    rows.append({\"seed\": s, \"ari_vs_baseline\": ari, \"inertia\": float(km.inertia_)})\n\nseed_stability = (\n    pd.DataFrame(rows)\n    .sort_values(\"ari_vs_baseline\", ascending=True)\n    .reset_index(drop=True)\n)\n\nseed_audit = pd.DataFrame({\n    \"K_FINAL\": [K_FINAL],\n    \"n_seeds_tested\": [len(SEEDS)],\n    \"ari_min\": [float(seed_stability[\"ari_vs_baseline\"].min())],\n    \"ari_p05\": [float(seed_stability[\"ari_vs_baseline\"].quantile(0.05))],\n    \"ari_median\": [float(seed_stability[\"ari_vs_baseline\"].median())],\n    \"ari_p95\": [float(seed_stability[\"ari_vs_baseline\"].quantile(0.95))],\n    \"ari_max\": [float(seed_stability[\"ari_vs_baseline\"].max())],\n})\n\nseed_audit, seed_stability.head(10)\n```\n\n::: callout-tip\n\n### How to interpret seed ARI\n\nA practical rule of thumb:\n\n* **Very stable:** most ARI values $\\ge 0.90$\n* **Reasonably stable:** most ARI values $\\ge 0.75$\n* **Unstable:** many ARI values $< 0.60$\n\nIf stability is weak, we usually revisit preprocessing, $K$, or consider a different clustering method.\n:::\n\n---\n\n### “Worst-case” rerun inspection (label-aligned)\n\nEven when ARI is $1.0$, K-means can output the same partition with different numeric labels (label permutation).\nSo we align labels before comparing cluster sizes.\n\n```{python}\n# Identify worst seed run (lowest ARI vs baseline)\nworst = seed_stability.iloc[0]\nworst_seed = int(worst[\"seed\"])\n\nkm_worst = KMeans(n_clusters=K_FINAL, n_init=50, random_state=worst_seed)\nlabels_worst = km_worst.fit_predict(X)\n\nari_worst = float(adjusted_rand_score(base_labels, labels_worst))\n\n# Contingency table: baseline cluster IDs (rows) vs worst run IDs (cols)\nct = pd.crosstab(\n    pd.Series(base_labels, name=\"baseline\"),\n    pd.Series(labels_worst, name=\"worst\")\n)\n\n# Map each worst-cluster to the baseline cluster it overlaps with most\nmapping = ct.idxmax(axis=0).to_dict()\n\nlabels_worst_aligned = np.vectorize(mapping.get)(labels_worst)\n\n# Compare cluster sizes after alignment\nbase_sizes = pd.Series(base_labels).value_counts().sort_index()\nworst_sizes = pd.Series(labels_worst_aligned).value_counts().sort_index()\n\nsize_compare = pd.DataFrame({\n    \"cluster_id\": sorted(set(base_sizes.index) | set(worst_sizes.index)),\n}).set_index(\"cluster_id\")\n\nsize_compare[\"baseline_n\"] = base_sizes.reindex(size_compare.index).fillna(0).astype(int)\nsize_compare[\"worst_n_aligned\"] = worst_sizes.reindex(size_compare.index).fillna(0).astype(int)\nsize_compare[\"baseline_pct\"] = (size_compare[\"baseline_n\"] / size_compare[\"baseline_n\"].sum() * 100).round(2)\nsize_compare[\"worst_pct_aligned\"] = (size_compare[\"worst_n_aligned\"] / size_compare[\"worst_n_aligned\"].sum() * 100).round(2)\n\nworst_seed, float(worst[\"ari_vs_baseline\"]), ari_worst, mapping, size_compare.reset_index()\n```\n\n---\n\n### Feature-block sensitivity (drop-one-block tests)\n\nNow we test whether the segmentation depends too heavily on a single block.\nWe refit K-means on reduced matrices and compare to baseline using ARI.\n\nWe do “drop one block at a time” on:\n\n* volume\n* duration\n* recency\n* timing\n\n::: callout-note\n\n### Why ARI is valid here\n\nCluster labels are arbitrary (cluster 0 in one run is not “the same” as cluster 0 in another).\nARI is invariant to label permutation, so it compares structure, not label IDs.\n:::\n\n```{python}\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n\n# Rebuild X for each variant using the SAME preprocessing recipe as Phase 3:\n# - drop constant cols\n# - clip p01–p99 in raw space\n# - median impute + RobustScaler\n# - cap in scaled space\n\nSCALED_CAP = float(prep.get(\"scaled_cap\", 10.0))  # safe default\n\nFEATURE_BLOCKS = prep[\"FEATURE_BLOCKS\"]\nMODEL_FEATURES = prep[\"MODEL_FEATURES\"]\ndf_view = contact_features_view  # raw-unit feature table\n\ndef build_X_from_features(df, features, clip_lo=0.01, clip_hi=0.99, scaled_cap=10.0):\n    X_raw = df[features].copy()\n\n    # Drop truly constant columns\n    nunique = X_raw.nunique(dropna=False)\n    constant_cols = nunique[nunique <= 1].index.tolist()\n    X_raw2 = X_raw.drop(columns=constant_cols) if constant_cols else X_raw.copy()\n\n    # Clip extremes in raw space\n    X_clip = X_raw2.copy()\n    for col in X_clip.columns:\n        lo = float(X_clip[col].quantile(clip_lo))\n        hi = float(X_clip[col].quantile(clip_hi))\n        X_clip[col] = X_clip[col].clip(lower=lo, upper=hi)\n\n    # Impute + robust scale\n    imputer = SimpleImputer(strategy=\"median\")\n    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n    X_imp = imputer.fit_transform(X_clip)\n    X_scaled = scaler.fit_transform(X_imp)\n\n    # Final cap in scaled space\n    X_final = np.clip(X_scaled, -scaled_cap, scaled_cap)\n\n    return X_final, list(X_clip.columns)\n\n# Baseline rebuild for consistent comparisons\nX_base, base_feature_names = build_X_from_features(\n    df_view, MODEL_FEATURES, clip_lo=0.01, clip_hi=0.99, scaled_cap=SCALED_CAP\n)\nbase_labels_for_sens = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42).fit_predict(X_base)\n\n# Define variants: drop one block at a time (excluding \"category\")\nvariants = {}\nfor block in [\"volume\", \"duration\", \"recency\", \"timing\"]:\n    keep = []\n    for b, feats in FEATURE_BLOCKS.items():\n        if b == \"category\":\n            continue\n        if b != block:\n            keep += feats\n    variants[f\"drop_{block}\"] = keep\n\nrows = []\nfor name, feats in variants.items():\n    X_var, feat_names_var = build_X_from_features(\n        df_view, feats, clip_lo=0.01, clip_hi=0.99, scaled_cap=SCALED_CAP\n    )\n    lab = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42).fit_predict(X_var)\n    ari = float(adjusted_rand_score(base_labels_for_sens, lab))\n    rows.append({\n        \"variant\": name,\n        \"n_features\": int(X_var.shape[1]),\n        \"ari_vs_baseline\": ari\n    })\n\nblock_sensitivity = (\n    pd.DataFrame(rows)\n    .sort_values(\"ari_vs_baseline\", ascending=False)\n    .reset_index(drop=True)\n)\n\nblock_sensitivity\n```\n\n---\n\n### Sensitivity summary (decision-useful)\n\n```{python}\nsens_audit = pd.DataFrame({\n    \"K_FINAL\": [K_FINAL],\n    \"seed_stability_ari_median\": [float(seed_audit[\"ari_median\"].iloc[0])],\n    \"seed_stability_ari_p05\": [float(seed_audit[\"ari_p05\"].iloc[0])],\n    \"seed_stability_ari_min\": [float(seed_audit[\"ari_min\"].iloc[0])],\n    \"block_sensitivity_ari_min\": [float(block_sensitivity[\"ari_vs_baseline\"].min())],\n    \"block_sensitivity_ari_median\": [float(block_sensitivity[\"ari_vs_baseline\"].median())],\n})\n\nsens_audit.T\n```\n\n::: callout-important\n\n### What we conclude here (based on your results)\n\n* **Seed stability:** ARI is $1.0$ for all tested seeds $\\Rightarrow$ K-means solution is *fully stable*.\n* **Block sensitivity:** dropping **duration** or **recency** reduces agreement (ARI $\\approx 0.70$–$0.74$), which implies these blocks carry meaningful segmentation signal.\n\nThis is robust enough for stakeholder-facing use, with a clear explanation of what drives the segments.\n:::\n\n---\n\n### Store robustness artifacts (for later reporting)\n\n```{python}\nprep[\"seed_stability\"] = seed_stability\nprep[\"seed_audit\"] = seed_audit\n\nprep[\"labels_worst_seed\"] = labels_worst\nprep[\"labels_worst_seed_aligned\"] = labels_worst_aligned\nprep[\"size_compare_worst_seed\"] = size_compare.reset_index()\nprep[\"worst_seed_label_mapping\"] = mapping\n\nprep[\"block_sensitivity\"] = block_sensitivity\nprep[\"sens_audit\"] = sens_audit\n\nlist(prep.keys())[:25]\n```\n\n---\n\n::: callout-important\n## Phase 6 summary (robustness verdict)\n\nBased on your results:\n\n- **Seed stability (random restarts):** $ARI = 1.00$ for all tested seeds $\\Rightarrow$ the $K=4$ K-means solution is **fully stable** on `prep[\"X\"]`.\n- **Feature-block sensitivity (drop-one-block):**\n  - Drop timing: $ARI = 1.00$ (timing is not driving the segmentation)\n  - Drop volume: $ARI \\approx 0.94$ (volume contributes, but is not the sole driver)\n  - Drop recency: $ARI \\approx 0.74$ (recency contains meaningful signal)\n  - Drop duration: $ARI \\approx 0.70$ (duration contains meaningful signal)\n\n**Decision-useful takeaway:** The segmentation is stable and not an artifact of random initialization.  \nThe clusters are primarily supported by **duration** and **recency** behaviour (with volume contributing), so these are the main behavioural levers to emphasize in stakeholder explanations.\n\n:::\n\n\n---\n\n\n\n## Phase 7 — PCA visualization (for interpretation only)\n\nPCA is **not** used to train clustering.  \nWe use PCA only to create a 2D “map” of contacts for storytelling, sanity-checking, and explaining separation.\n\n::: callout-warning\n### PCA is not the clustering model\n- **Clustering was trained on:** `prep[\"X\"]` (robust-scaled + capped)\n- **PCA is used for:** visualization only (2D projection)\nIf PCA looks messy, it does **not** automatically mean clustering is wrong. PCA compresses information into 2 dimensions.\n:::\n\n::: callout-important\n### Privacy\nWe plot only:\n\n- PCA coordinates\n- cluster labels\nWe do **not** plot names or phone numbers, and we keep `contact_id` hashed.\n:::\n\n---\n\n### Fit PCA ($2$ components) on the clustering matrix\n\n```{python}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nX = prep[\"X\"]  # scaled + capped matrix used for clustering\nlabels = np.asarray(prep[\"labels\"])\nK_FINAL = int(prep.get(\"K_FINAL\", 4))\n\npca = PCA(n_components=2, random_state=42)\nZ = pca.fit_transform(X)\n\npca_audit = pd.DataFrame({\n    \"n_components\": [2],\n    \"explained_var_ratio_pc1\": [float(pca.explained_variance_ratio_[0])],\n    \"explained_var_ratio_pc2\": [float(pca.explained_variance_ratio_[1])],\n    \"explained_var_ratio_total_2pc\": [float(pca.explained_variance_ratio_[:2].sum())],\n})\n\npca_audit\n```\n\n---\n\n### Build a plotting table (hashed id + PCA coordinates + cluster)\n\n```{python}\npca_view = pd.DataFrame({\n    \"contact_id\": contact_features_view[\"contact_id\"].values,  # hashed only\n    \"pc1\": Z[:, 0],\n    \"pc2\": Z[:, 1],\n    \"cluster\": labels\n})\n\npca_view.head()\n```\n\n---\n\n### PCA scatter plot (clusters on the 2D map)\n\n```{python}\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot each cluster separately for a clear legend\nfor c in sorted(pca_view[\"cluster\"].unique()):\n    sub = pca_view[pca_view[\"cluster\"] == c]\n    ax.scatter(sub[\"pc1\"], sub[\"pc2\"], s=12, alpha=0.7, label=f\"Cluster {c}\")\n\nax.set_title(f\"PCA map of contacts (K-means clusters, $K={K_FINAL}$)\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.legend(title=\"Cluster\", fontsize=9)\nplt.show()\n```\n\n---\n\n### Optional: cluster centroids projected into PCA space\n\nThis helps stakeholders see “where the centers sit” on the map.\n\n```{python}\n# K-means centroids exist in scaled feature space; PCA was fit on the same space.\ncentroids = prep[\"kmeans\"].cluster_centers_\ncentroids_2d = pca.transform(centroids)\n\ncent = pd.DataFrame({\n    \"cluster\": list(range(K_FINAL)),\n    \"pc1_centroid\": centroids_2d[:, 0],\n    \"pc2_centroid\": centroids_2d[:, 1],\n})\n\ncent\n```\n\n```{python}\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor c in sorted(pca_view[\"cluster\"].unique()):\n    sub = pca_view[pca_view[\"cluster\"] == c]\n    ax.scatter(sub[\"pc1\"], sub[\"pc2\"], s=10, alpha=0.45, label=f\"Cluster {c}\")\n\n# Centroids\nax.scatter(cent[\"pc1_centroid\"], cent[\"pc2_centroid\"], s=120, marker=\"X\", label=\"Centroids\")\n\nax.set_title(f\"PCA map with centroids (K={K_FINAL})\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.legend(fontsize=9)\nplt.show()\n```\n\n---\n\n### Interpretation guidance (what we conclude from PCA)\n\n::: callout-note\nPCA is a **compression** from many dimensions down to 2.\n\nUse this plot to check:\n\n* whether clusters are *roughly separated* (good sign),\n* whether some clusters overlap heavily (could be fine; segmentation can still be valid),\n* whether a small cluster looks like extreme outliers (could indicate a “special handling” segment).\n\nDo **not** use PCA to choose $K$ or to claim “the true number of clusters.”\n:::\n\n---\n\n### Store PCA artifacts (for the final report)\n\n```{python}\nprep[\"pca_model\"] = pca\nprep[\"pca_audit\"] = pca_audit\nprep[\"pca_view\"] = pca_view\nprep[\"pca_centroids_2d\"] = cent\n\nlist(prep.keys())[:30]\n```\n\n---\n\n\n## What PCA tells us (interpretation only)\n\nPCA is a 2D projection of the same scaled training matrix used for clustering, `prep[\"X\"]`.\nIt does **not** change the model; it only helps us *see* separation.\n\n### Variance captured\n\nFrom `pca_audit`, the first two components explain:\n\n- $PC1 \\approx 0.629$\n- $PC2 \\approx 0.216$\n- Total (2D) $\\approx 0.844$\n\nSo about **84.4%** of the variance in the scaled feature space is visible in the 2D map.\nThat is high enough that the plot is a meaningful sanity-check.\n\n### Visual separation (sanity-check)\n\nThe PCA map shows clear structure:\n\n- One cluster sits far to the **right** on $PC1$ (strong separation in the main direction of variance).\n- One cluster is concentrated far to the **left** (tight wedge near low $PC1$ values).\n- Two clusters occupy the middle/bottom region with partial overlap, suggesting they differ in *multiple* dimensions (not fully separable in 2D, which is normal).\n\nThis supports the earlier interpretation: the segmentation is not an artifact of one random run (Phase 6 already proved stability), and the clusters correspond to genuinely different behavioral regimes in feature space.\n\n### Centroids on the PCA map\n\nProjected centroids help explain “where the centers sit”:\n\n- The centroid that is far right on $PC1$ corresponds to a high-intensity / high-activity regime.\n- The centroid that is far left corresponds to low-engagement / near-baseline contacts.\n- The centroid that is high on $PC2$ indicates a behavior pattern that is not just “more or less activity,” but a different mix (often duration/recency-related structure).\n\n**Important:** overlap in PCA does not invalidate clusters. K-means was fit in the full feature space; PCA compresses information into 2 dimensions.\n\n\n---\n\n## Phase 8 — Final deliverables (stakeholder-ready outputs)\n\nIn this phase we package the work into **decision-ready artifacts** that can be used in a CRM or reporting workflow **without exposing identities**.\n\n::: callout-important\n### Privacy and operational use\n- We **do not** join names or phone numbers into any outputs.\n- All contact identifiers remain **hashed**.\n- Default is **no file export**. You can enable export if you want portfolio artifacts.\n:::\n\n---\n\n### Assemble the “final segment table” (labels + key medians)\n\n```{python}\nimport pandas as pd\nimport numpy as np\n\n# Guardrails\nrequired = [\"cards\", \"cluster_labels\", \"rules\", \"sens_audit\", \"pca_audit\"]\nmissing = [k for k in required if k not in prep]\nassert len(missing) == 0, f\"Missing Phase artifacts in prep: {missing}\"\n\ncards = prep[\"cards\"].copy()\ncluster_labels = prep[\"cluster_labels\"].copy()\nrules = prep[\"rules\"].copy()\n\n# Join: cluster -> segment label + action\nfinal_segments = (\n    cards.merge(cluster_labels[[\"cluster\", \"segment_label\", \"primary_action\"]], on=\"cluster\", how=\"left\")\n    .sort_values(\"n_contacts\", ascending=False)\n    .reset_index(drop=True)\n)\n\n# Reorder columns for a stakeholder view\nfront = [\"cluster\", \"segment_label\", \"n_contacts\", \"primary_action\"]\nrest = [c for c in final_segments.columns if c not in front]\nfinal_segments = final_segments[front + rest]\n\nfinal_segments\n```\n\n---\n\n### Decision rules (CRM / outreach policy)\n\n```{python}\nrules\n```\n\n---\n\n### “Executive summary” (one screen)\n\n```{python}\nsens_audit = prep[\"sens_audit\"].copy()\npca_audit = prep[\"pca_audit\"].copy()\n\n# Key stats\nn_contacts_total = int(prep[\"contact_clusters\"].shape[0])\nk_final = int(prep.get(\"K_FINAL\", 4))\n\nseed_median = float(sens_audit[\"seed_stability_ari_median\"].iloc[0])\nseed_min = float(sens_audit[\"seed_stability_ari_min\"].iloc[0])\nblock_min = float(sens_audit[\"block_sensitivity_ari_min\"].iloc[0])\n\npca_total2 = float(pca_audit[\"explained_var_ratio_total_2pc\"].iloc[0])\n\nexec_summary = pd.DataFrame([{\n    \"n_contacts_clustered\": n_contacts_total,\n    \"K_FINAL\": k_final,\n    \"seed_stability_ARI_median\": seed_median,\n    \"seed_stability_ARI_min\": seed_min,\n    \"block_sensitivity_ARI_min\": block_min,\n    \"pca_variance_explained_2PC\": pca_total2,\n}])\n\nexec_summary\n```\n\n---\n\n### Findings (plain-English, decision-first)\n\n```{python}\n# Convert the segment table into a short narrative summary\nseg_counts = final_segments[[\"segment_label\", \"n_contacts\"]].copy()\nseg_counts[\"share_pct\"] = (seg_counts[\"n_contacts\"] / seg_counts[\"n_contacts\"].sum() * 100).round(2)\n\nseg_counts\n```\n\n```{python}\n# Build a compact “what it is + what to do” list (no IDs)\nlabels_idx = cluster_labels.set_index(\"segment_label\")\n\nrows = []\nfor _, r in seg_counts.iterrows():\n    seg = r[\"segment_label\"]\n    n = int(r[\"n_contacts\"])\n    pct = float(r[\"share_pct\"])\n    action = labels_idx.loc[seg, \"primary_action\"] if seg in labels_idx.index else \"TBD\"\n    rows.append({\"segment_label\": seg, \"n_contacts\": n, \"share_pct\": pct, \"default_action\": action})\n\nfindings_table = pd.DataFrame(rows).sort_values(\"n_contacts\", ascending=False).reset_index(drop=True)\nfindings_table\n```\n\n::: callout-note\n\n### What we learned from your results\n\n* **The solution is fully stable to random initialization** (seed ARI $\\approx 1$ across tested seeds), so you can rerun K-means and get the same segmentation (up to label permutation).\n* **Recency and duration matter**: dropping either block reduces agreement (lower ARI), so those blocks carry meaningful segmentation signal.\n* **PCA is supportive, not decisive**: the first two PCs explain a large share of variance, and the 2D map shows visible separation patterns that align with the segmentation.\n:::\n\n---\n\n### Limitations and “how not to misuse this”\n\n::: callout-warning\n\n### Limitations\n\n* This is **behavior-based segmentation**, not a causal model. It supports prioritization and outreach strategy, not “ground truth identities.”\n* K-means assumes roughly spherical clusters in the feature space. If your business needs non-spherical shapes, consider alternative methods later (e.g., GMM, HDBSCAN).\n* Very sparse contacts (1–2 calls) can make share features extreme; we handled this by using medians + mean/%-any checks for timing.\n:::\n\n---\n\n### Optional: export portfolio artifacts (OFF by default)\n\n```{python}\nSAVE_FILES = False  # keep False unless you explicitly want exports\n\nif SAVE_FILES:\n    # These contain NO names/phone numbers. contact_id stays hashed.\n    final_segments.to_csv(\"final_segments_cluster_cards.csv\", index=False)\n    rules.to_csv(\"segment_policy_rules.csv\", index=False)\n    findings_table.to_csv(\"segment_findings_summary.csv\", index=False)\n    exec_summary.to_csv(\"executive_summary_metrics.csv\", index=False)\n\n    print(\"Saved CSV files (hashed IDs only, no raw identifiers).\")\nelse:\n    print(\"SAVE_FILES=False (no files written).\")\n```\n\n---\n\n\n## Phase 10 — Final wrap-up (competition framing + decision narrative)\n\n### Competition prompt (what we were given)\n\n**We were given** a historical phone-call behaviour dataset where each record represents a call event.  \nThe dataset includes **timestamps, call duration, and categorised call context**, but **no business labels** such as “good customer” or “bad customer”.\n\n**Task:** Without supervision (no labels), we had to build a **contact-level segmentation** that a business could actually use for prioritisation, outreach cadence, and relationship management.\n\n**Hard constraints:**\n\n- **Privacy-first:** do not use names or phone numbers; keep identifiers hashed.\n- **Operational:** output must translate into **clear segment actions**, not just clusters.\n- **Professional robustness:** results must be stable, not a “nice plot”.\n\n---\n\n### What we had to produce (deliverable definition)\n\nWe had to deliver:\n\n1. A **clean contact-level feature table** (one row per contact).\n2. A **distance-ready clustering matrix** (robust preprocessing).\n3. A final segmentation using **K-means with $K=4$**.\n4. **Stakeholder-ready interpretation** (cluster “cards” + segment labels + default actions).\n5. **Robustness evidence** (seed stability and feature sensitivity).\n6. **Interpretation-only visual sanity-check** (PCA map).\n\n---\n\n### What we built (end-to-end pipeline)\n\n1. **Audit** the raw call-event data (types, missingness, constraints).\n2. **Aggregate to contacts** (the clustering unit), producing features that capture:\n   - volume/intensity (calls, active days),\n   - relationship depth proxy (duration metrics, long-call share),\n   - recency/tenure (dormancy vs continuity),\n   - timing mix (business hours vs off-hours).\n3. **Preprocess for K-means**:\n   - clip extreme values in raw space,\n   - median imputation,\n   - robust scaling,\n   - cap scaled values for numerical stability.\n4. **Fit K-means** with $K=4$ and attach labels to contacts (hashed IDs only).\n5. **Interpret** clusters using raw units and translate into segments + actions.\n6. **Validate robustness** using ARI across seeds and drop-one-block sensitivity.\n7. **Use PCA only for visualization** (interpretation, not training).\n\n---\n\n### The segments (what we found + what to do)\n\nFrom the cluster cards and behaviour-based mapping, we obtained four operational segments:\n\n- **One-off / low-engagement contacts** (largest share)  \n  **Action:** deprioritize by default; automate nurture; reactivate only if new activity appears.\n\n- **Warm occasional contacts**  \n  **Action:** nurture cadence (monthly/quarterly); structured check-ins to increase engagement.\n\n- **Core active relationships**  \n  **Action:** retention and priority servicing; assign owner; proactive follow-ups.\n\n- **Rare deep conversations**  \n  **Action:** high-touch when active; preserve context; personalized re-engagement when dormant.\n\n---\n\n### Proof the solution “holds up” (robustness)\n\n**Seed stability:** ARI was $1.00$ across all tested seeds.  \nSo the segmentation is fully stable to random initialization (up to label permutation).\n\n**Feature sensitivity:** dropping duration or recency reduces agreement (ARI drops to about $0.70$–$0.74$).  \nInterpretation: **duration and recency carry meaningful segmentation signal**; timing contributes less to the final partition.\n\n---\n\n### What PCA contributed (interpretation only)\n\nPCA is a 2D projection of `prep[\"X\"]` for storytelling and sanity-checking.\n\n- The first two components explain about $84.45\\%$ of variance.\n- The map shows visible structure consistent with the segmentation.\n- PCA overlap does not invalidate clusters because K-means was trained in the full feature space.\n\n---\n\n### Decision use: what a business can do with these segments\n\n- **Routing:** prioritize “Core active” + “Rare deep” to high-touch handling.\n- **Cadence:** automate outreach for “Warm occasional”; suppress manual effort for “One-off”.\n- **Reactivation:** trigger win-back campaigns for dormant “Rare deep” and “Warm occasional”.\n- **Service levels:** define SLA tiers using segment label and recency.\n\n---\n\n### Limitations (how not to misuse this)\n\n- This is behaviour segmentation, not identity inference and not causal.\n- K-means prefers spherical separation in feature space; alternative methods can be tested later.\n- Very sparse contacts can make timing shares extreme; we mitigated via robust summaries and mean/%-any checks.\n\n::: callout-note\n**Final deliverables:**\n\n- A stable $K=4$ segmentation (hashed IDs only).\n- Stakeholder-ready cluster cards + action policy rules.\n- Robustness evidence (seed ARI + block sensitivity).\n- PCA map used only for interpretation and storytelling.\n:::\n\n\n---\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"output-file":"walsoft_contact_segmentation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","title":"Walsoft Decision-driven contact segmentation"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}