{
  "hash": "ed2ac2c3d8444c6af9023ce670c8ed14",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Walsoft Decision-driven contact segmentation\"\nformat: html\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n## Business objective\n\nSegment contacts into **actionable behavioral groups** to support **outreach planning, CRM prioritization, and relationship management**.\n\n::: callout-important\n### Hard constraints enforced in this project\n\n**Time bands (exact):**\n\n- `business_hours`: 08:00–17:59  \n- `evening`: 18:00–20:59  \n- `late_night`: 21:00–23:59  \n- `early_morning`: 00:00–07:59  \n\n**Working days:**\n\n- Open days: Monday–Saturday  \n- Closed day: Sunday only\n:::\n\n::: callout-tip\n### Workflow (phases)\nPhase 1 Audit → Phase 2 Contact-level features → Phase 3 Preprocess → Phase 4 Choose K → Phase 5 Fit/Interpret → then stability/ARI → sensitivity (feature blocks) → PCA viz → final recommendations.\n:::\n\n## Phase 1 — Audit (call-level)\n\n### Setup (imports + toggles)\n\n\n\n### Load the dataset\n\n\n#### Load data\n\n::: {#9954538a .cell execution_count=2}\n``` {.python .cell-code}\n# Load dataset (no assumptions, no output)\ndf = pd.read_csv(DATA_URL)\n```\n:::\n\n\n#### Dataset overview\n\n::: {#92780e10 .cell execution_count=3}\n``` {.python .cell-code}\n# Dataset overview (executive-level structural metrics)\n\ndataset_overview = pd.DataFrame({\n    \"Metric\": [\n        \"Rows\",\n        \"Columns\",\n        \"Total cells\",\n        \"Memory usage (MB)\"\n    ],\n    \"Value\": [\n        int(df.shape[0]),\n        int(df.shape[1]),\n        int(df.shape[0] * df.shape[1]),\n        f\"{df.memory_usage(deep=True).sum() / 1e6:.2f}\"\n    ]\n})\n\ndataset_overview\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Rows</td>\n      <td>24952</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Columns</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Total cells</td>\n      <td>224568</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Memory usage (MB)</td>\n      <td>9.10</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n#### Column inventory (schema inspection)\n\n::: {#78d03a53 .cell execution_count=4}\n``` {.python .cell-code}\n# Column inventory (schema audit table)\n\ncolumn_inventory = (\n    pd.DataFrame({\n        \"Column name\": df.columns,\n        \"Data type\": df.dtypes.astype(str),\n        \"Non-null count\": df.notna().sum().values,\n        \"Non-null rate\": (df.notna().mean()).round(3).values\n    })\n    .sort_values(\"Column name\")\n    .reset_index(drop=True)\n)\n\ncolumn_inventory\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Column name</th>\n      <th>Data type</th>\n      <th>Non-null count</th>\n      <th>Non-null rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>category</td>\n      <td>str</td>\n      <td>24952</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>date_stamp</td>\n      <td>str</td>\n      <td>24952</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>day-of_week</td>\n      <td>str</td>\n      <td>24952</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dialled_phone_number</td>\n      <td>int64</td>\n      <td>24952</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>duration_in_seconds</td>\n      <td>int64</td>\n      <td>24952</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>month</td>\n      <td>str</td>\n      <td>24952</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>name</td>\n      <td>str</td>\n      <td>24952</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>time</td>\n      <td>str</td>\n      <td>24952</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>year</td>\n      <td>int64</td>\n      <td>24952</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n#### Sample records (visual sanity check)\n\n::: {#0a4a5c3e .cell execution_count=5}\n``` {.python .cell-code}\n# Sample records (human sanity check only)\ndf.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_stamp</th>\n      <th>time</th>\n      <th>day-of_week</th>\n      <th>month</th>\n      <th>year</th>\n      <th>dialled_phone_number</th>\n      <th>name</th>\n      <th>duration_in_seconds</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1/1/2022</td>\n      <td>16:03:01</td>\n      <td>Saturday</td>\n      <td>January</td>\n      <td>2022</td>\n      <td>648578192</td>\n      <td>Abel</td>\n      <td>179</td>\n      <td>Unknown</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1/1/2022</td>\n      <td>16:06:17</td>\n      <td>Saturday</td>\n      <td>January</td>\n      <td>2022</td>\n      <td>814500001</td>\n      <td>Husband CEL01</td>\n      <td>66</td>\n      <td>Family</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1/1/2022</td>\n      <td>19:08:44</td>\n      <td>Saturday</td>\n      <td>January</td>\n      <td>2022</td>\n      <td>814500001</td>\n      <td>Husband CEL01</td>\n      <td>38</td>\n      <td>Family</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Identify key columns (robust, from the data itself)\n\n::: {#91fca6f0 .cell execution_count=6}\n``` {.python .cell-code}\n# Identify key columns (schema audit, no assumptions)\n\n\n# ------------------------------------------------------------------\n# Normalise column names once (single source of truth)\n# ------------------------------------------------------------------\ncol_map = {c: c.lower().strip() for c in df.columns}\n\n# ------------------------------------------------------------------\n# Detection rules (explicit, auditable, extensible)\n# ------------------------------------------------------------------\nDETECTION_RULES = OrderedDict({\n    \"Date\": {\n        \"description\": \"Calendar date component\",\n        \"match_any\": [\"date\"]\n    },\n    \"Time\": {\n        \"description\": \"Clock time component\",\n        \"match_exact_or_contains\": [\"time\"]\n    },\n    \"Duration\": {\n        \"description\": \"Call duration (numeric, any unit)\",\n        \"match_any\": [\"duration\", \"seconds\", \"secs\", \"sec\", \"minutes\", \"mins\"]\n    },\n    \"Identifier\": {\n        \"description\": \"Contact or phone identifier\",\n        \"match_any\": [\"dialled\", \"dialed\", \"phone\", \"number\", \"contact\"]\n    },\n    \"Category\": {\n        \"description\": \"Human or system call classification\",\n        \"match_any\": [\"category\"]\n    }\n})\n\n# ------------------------------------------------------------------\n# Apply detection rules\n# ------------------------------------------------------------------\nrecords = []\n\nfor field, rule in DETECTION_RULES.items():\n    detected = []\n\n    for col, lc in col_map.items():\n        if \"match_exact_or_contains\" in rule:\n            if any(lc == k or k in lc for k in rule[\"match_exact_or_contains\"]):\n                detected.append(col)\n        elif \"match_any\" in rule:\n            if any(k in lc for k in rule[\"match_any\"]):\n                detected.append(col)\n\n    records.append({\n        \"Field type\": field,\n        \"Purpose\": rule[\"description\"],\n        \"Detected columns\": \", \".join(detected) if detected else \"—\",\n        \"Count\": len(detected),\n        \"Status\": \"OK\" if detected else \"Missing\"\n    })\n\n# ------------------------------------------------------------------\n# Canonical schema audit table (Quarto renders automatically)\n# ------------------------------------------------------------------\nschema_audit = pd.DataFrame(records)\n\nschema_audit\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Field type</th>\n      <th>Purpose</th>\n      <th>Detected columns</th>\n      <th>Count</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Date</td>\n      <td>Calendar date component</td>\n      <td>date_stamp</td>\n      <td>1</td>\n      <td>OK</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Time</td>\n      <td>Clock time component</td>\n      <td>time</td>\n      <td>1</td>\n      <td>OK</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Duration</td>\n      <td>Call duration (numeric, any unit)</td>\n      <td>duration_in_seconds</td>\n      <td>1</td>\n      <td>OK</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Identifier</td>\n      <td>Contact or phone identifier</td>\n      <td>dialled_phone_number</td>\n      <td>1</td>\n      <td>OK</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Category</td>\n      <td>Human or system call classification</td>\n      <td>category</td>\n      <td>1</td>\n      <td>OK</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Build canonical timestamp and numeric duration (audit-quality checks)\n\n\n#### Validate required schema\n\n::: {#73d1659e .cell execution_count=7}\n``` {.python .cell-code}\n# Validate required columns exist (hard audit gate)\n\nrequired_cols = [\n    \"date_stamp\",\n    \"time\",\n    \"duration_in_seconds\",\n    \"dialled_phone_number\"\n]\n\nmissing_required = [c for c in required_cols if c not in df.columns]\n\nschema_validation = pd.DataFrame({\n    \"Required column\": required_cols,\n    \"Present in dataset\": [c in df.columns for c in required_cols]\n})\n\nschema_validation\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Required column</th>\n      <th>Present in dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>date_stamp</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>time</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>duration_in_seconds</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dialled_phone_number</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#3d753fdb .cell execution_count=8}\n``` {.python .cell-code}\n# Stop execution if schema is invalid\nassert len(missing_required) == 0, f\"Missing required columns: {missing_required}\"\n```\n:::\n\n\n---\n\n#### Build canonical fields\n\n::: {#c975a5e5 .cell execution_count=9}\n``` {.python .cell-code}\n# Canonical timestamp\ndf[\"call_ts\"] = pd.to_datetime(\n    df[\"date_stamp\"].astype(str).str.strip() + \" \" +\n    df[\"time\"].astype(str).str.strip(),\n    errors=\"coerce\"\n)\n\n# Canonical duration (seconds)\ndf[\"dur_sec\"] = pd.to_numeric(\n    df[\"duration_in_seconds\"],\n    errors=\"coerce\"\n)\n```\n:::\n\n\n---\n\n#### Data integrity audit\n\n::: {#0ce41d6a .cell execution_count=10}\n``` {.python .cell-code}\nbad_ts = int(df[\"call_ts\"].isna().sum())\nbad_dur = int(df[\"dur_sec\"].isna().sum())\nnegative_dur = int((df[\"dur_sec\"] < 0).sum())\n\ntimestamp_min = df[\"call_ts\"].min()\ntimestamp_max = df[\"call_ts\"].max()\n\nintegrity_audit = pd.DataFrame({\n    \"Check\": [\n        \"Timestamp parse failures\",\n        \"Duration numeric failures\",\n        \"Negative durations\",\n        \"Timestamp range (min)\",\n        \"Timestamp range (max)\",\n        \"Timestamp success rate\",\n        \"Duration success rate\"\n    ],\n    \"Value\": [\n        bad_ts,\n        bad_dur,\n        negative_dur,\n        str(timestamp_min),\n        str(timestamp_max),\n        f\"{df['call_ts'].notna().mean():.3f}\",\n        f\"{df['dur_sec'].notna().mean():.3f}\"\n    ]\n})\n\nintegrity_audit\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Check</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Timestamp parse failures</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Duration numeric failures</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Negative durations</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Timestamp range (min)</td>\n      <td>2022-01-01 16:03:01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Timestamp range (max)</td>\n      <td>2024-10-04 20:15:16</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Timestamp success rate</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Duration success rate</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n#### Hard integrity gates\n\n::: {#63c7e9a2 .cell execution_count=11}\n``` {.python .cell-code}\nassert bad_ts == 0, \"Timestamp parsing failed for some rows.\"\nassert bad_dur == 0, \"Duration numeric conversion failed for some rows.\"\nassert negative_dur == 0, \"Negative durations detected.\"\n```\n:::\n\n\n### Hash a stable `contact_id` (do not export raw identifiers)\n\n::: {#23d2087a .cell execution_count=12}\n``` {.python .cell-code}\ndef stable_contact_id(x: int | str) -> str:\n    s = str(x).encode(\"utf-8\")\n    return hashlib.sha256(s).hexdigest()[:12]\n\ndf[\"contact_id\"] = df[\"dialled_phone_number\"].map(stable_contact_id)\n\nprint(\"Unique contacts (hashed):\", int(df[\"contact_id\"].nunique()))\nassert df[\"contact_id\"].isna().sum() == 0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnique contacts (hashed): 2091\n```\n:::\n:::\n\n\n### Derive time-band + open/closed-day flags (hard constraints)\n\n::: {#f268c635 .cell execution_count=13}\n``` {.python .cell-code}\ndf[\"hour\"] = df[\"call_ts\"].dt.hour\ndf[\"dow\"] = df[\"call_ts\"].dt.dayofweek  # Mon=0..Sun=6\n\ndef assign_time_band(hour: int) -> str:\n    if TIME_BANDS[\"early_morning\"][0] <= hour <= TIME_BANDS[\"early_morning\"][1]:\n        return \"early_morning\"\n    if TIME_BANDS[\"business_hours\"][0] <= hour <= TIME_BANDS[\"business_hours\"][1]:\n        return \"business_hours\"\n    if TIME_BANDS[\"evening\"][0] <= hour <= TIME_BANDS[\"evening\"][1]:\n        return \"evening\"\n    if TIME_BANDS[\"late_night\"][0] <= hour <= TIME_BANDS[\"late_night\"][1]:\n        return \"late_night\"\n    raise ValueError(f\"Hour out of range: {hour}\")\n\ndf[\"time_band\"] = df[\"hour\"].map(assign_time_band)\n\ndf[\"is_open_day\"] = df[\"dow\"].isin(OPEN_DOW).astype(int)\ndf[\"is_closed_day\"] = df[\"dow\"].isin(CLOSED_DOW).astype(int)\n\n# sanity checks: mutually exclusive and exhaustive\nassert set(df[\"time_band\"].unique()) == {\"early_morning\", \"business_hours\", \"evening\", \"late_night\"}\nassert int((df[\"is_open_day\"] + df[\"is_closed_day\"]).min()) == 1\nassert int((df[\"is_open_day\"] + df[\"is_closed_day\"]).max()) == 1\n\ndf[[\"call_ts\",\"hour\",\"dow\",\"time_band\",\"is_open_day\",\"is_closed_day\"]].head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>call_ts</th>\n      <th>hour</th>\n      <th>dow</th>\n      <th>time_band</th>\n      <th>is_open_day</th>\n      <th>is_closed_day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-01-01 16:03:01</td>\n      <td>16</td>\n      <td>5</td>\n      <td>business_hours</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-01-01 16:06:17</td>\n      <td>16</td>\n      <td>5</td>\n      <td>business_hours</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-01-01 19:08:44</td>\n      <td>19</td>\n      <td>5</td>\n      <td>evening</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-01-01 20:03:11</td>\n      <td>20</td>\n      <td>5</td>\n      <td>evening</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-01-02 14:22:44</td>\n      <td>14</td>\n      <td>6</td>\n      <td>business_hours</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Executive summary table (stakeholder-ready)\n\n::: {#434c5638 .cell execution_count=14}\n``` {.python .cell-code}\ncat_col = \"category\" if \"category\" in df.columns else None\n\nsummary = pd.DataFrame({\n    \"Metric\": [\n        \"Rows (calls)\",\n        \"Columns\",\n        \"Unique contacts (hashed)\",\n        \"Timestamp parsable rate\",\n        \"Timestamp range (min)\",\n        \"Timestamp range (max)\",\n        \"Duration numeric rate\",\n        \"Duplicate rows\",\n        \"Category column present?\",\n        \"Category non-null rate (if present)\",\n        \"Data readiness score (ts & dur)\"\n    ],\n    \"Value\": [\n        int(df.shape[0]),\n        int(df.shape[1]),\n        int(df[\"contact_id\"].nunique()),\n        f\"{df['call_ts'].notna().mean():.3f}\",\n        str(df[\"call_ts\"].min()),\n        str(df[\"call_ts\"].max()),\n        f\"{df['dur_sec'].notna().mean():.3f}\",\n        int(df.duplicated().sum()),\n        bool(cat_col is not None),\n        f\"{df[cat_col].notna().mean():.3f}\" if cat_col else \"N/A\",\n        f\"{(df['call_ts'].notna() & df['dur_sec'].notna()).mean():.3f}\",\n    ]\n})\nsummary\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Metric</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Rows (calls)</td>\n      <td>24952</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Columns</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Unique contacts (hashed)</td>\n      <td>2091</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Timestamp parsable rate</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Timestamp range (min)</td>\n      <td>2022-01-01 16:03:01</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Timestamp range (max)</td>\n      <td>2024-10-04 20:15:16</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Duration numeric rate</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Duplicate rows</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Category column present?</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Category non-null rate (if present)</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Data readiness score (ts &amp; dur)</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Visual 1 — Monthly call volume (coverage)\n\n::: {#615f1860 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-16-output-1.png){width=662 height=470}\n:::\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>month_id</th>\n      <th>n_calls</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22</th>\n      <td>2023-11</td>\n      <td>627</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2023-12</td>\n      <td>691</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2024-01</td>\n      <td>944</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2024-02</td>\n      <td>608</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2024-03</td>\n      <td>783</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>2024-04</td>\n      <td>516</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>2024-05</td>\n      <td>165</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>2024-06</td>\n      <td>1034</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>2024-07</td>\n      <td>1049</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>2024-08</td>\n      <td>193</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>2024-09</td>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>2024-10</td>\n      <td>128</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Visual 2 — Duration distribution (outlier awareness)\n\n::: {#a7aeba07 .cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-17-output-1.png){width=662 height=470}\n:::\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-17-output-2.png){width=662 height=470}\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>duration_seconds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.00</th>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>0.25</th>\n      <td>9.00</td>\n    </tr>\n    <tr>\n      <th>0.50</th>\n      <td>34.00</td>\n    </tr>\n    <tr>\n      <th>0.75</th>\n      <td>84.00</td>\n    </tr>\n    <tr>\n      <th>0.90</th>\n      <td>211.00</td>\n    </tr>\n    <tr>\n      <th>0.95</th>\n      <td>396.00</td>\n    </tr>\n    <tr>\n      <th>0.99</th>\n      <td>1273.98</td>\n    </tr>\n    <tr>\n      <th>1.00</th>\n      <td>7200.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Visual 3 — Time-band mix + open/closed-day mix (hard-constraint diagnostics)\n\n::: {#bd38ae00 .cell execution_count=17}\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-18-output-1.png){width=662 height=470}\n:::\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-18-output-2.png){width=662 height=470}\n:::\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n(                n_calls\n time_band              \n business_hours    20624\n early_morning      1071\n evening            2877\n late_night          380,\n                     n_calls\n is_closed_day              \n Open day (Mon–Sat)    23300\n Closed day (Sun)       1652)\n```\n:::\n:::\n\n\n---\n\n\n::: callout-note\n## Phase 1 — Audit summary (stakeholder-ready)\n\n**Dataset health (ready for segmentation):**\n\n- **24,952 call records** across **2,091 unique contacts** (hashed).\n- **Time coverage:** 2022-01-01 → 2024-10-04 (timestamps parse perfectly; durations fully numeric).\n- **No duplicates** detected; **category is fully populated**.\n- **Data readiness score = 1.000** (timestamp + duration fully usable).\n\n**Behavioral signals visible already:**\n\n- Calls concentrate in **business hours** (20,624 / 24,952 ≈ 82.7%).\n- Smaller but meaningful activity in **evening** (2,877 ≈ 11.5%) and **early morning** (1,071 ≈ 4.3%); **late night** is rare (380 ≈ 1.5%).\n- Most calls happen on **open days (Mon–Sat)** (23,300 ≈ 93.4%); **Sunday** exists but is limited (1,652 ≈ 6.6%).\n\n**Duration profile (important for feature design):**\n\n- Typical call is short: median **34s**, 75th percentile **84s**.\n- Heavy tail: 99th percentile **~1,274s**, max **7,200s** (2 hours) → we will use **robust statistics** (median, IQR, log transforms, and long-call share) instead of relying only on means.\n\n**Coverage caution (modeling hygiene):**\n\n- Monthly volume is broadly stable until mid-2024, then shows sharp drops in some late months.\n- We will treat this as **coverage/behavior regime change** and rely on **contact-level aggregates + recency** (not month-level trends) for clustering.\n\n### Transition to Phase 2\nNext, we convert the call-level table into a **contact-level feature table** (1 row per contact).  \nThis is the modeling dataset for clustering.\n:::\n\n---\n\n## Phase 2 — Contact-level features\n\nThis phase transforms the **call-level dataset** into a **contact-level feature table**, producing **one row per contact**. These features capture activity intensity, temporal patterns, and category distributions, forming the basis for clustering and behavioral segmentation.\n\n---\n\n### Guardrails & Setup\n\nWe first ensure all required columns from Phase 1 exist. This prevents accidental execution on incomplete data.\n\n::: {#49e6d8d3 .cell execution_count=18}\n``` {.python .cell-code}\n# Guardrails: ensure critical columns exist\nassert \"contact_id\" in df.columns, \"contact_id missing (run Phase 1 first).\"\nassert \"call_ts\" in df.columns, \"call_ts missing (run Phase 1 first).\"\nassert \"dur_sec\" in df.columns, \"dur_sec missing (run Phase 1 first).\"\nassert \"time_band\" in df.columns, \"time_band missing (run Phase 1 first).\"\nassert \"is_open_day\" in df.columns and \"is_closed_day\" in df.columns, \"open/closed flags missing.\"\n```\n:::\n\n\nHelper for safe division (avoids division by zero):\n\n::: {#0137a5a0 .cell execution_count=19}\n``` {.python .cell-code}\ndef safe_div(a, b):\n    return np.where(b == 0, 0.0, a / b)\n```\n:::\n\n\nReference timestamp for recency calculations:\n\n::: {#d1eef31e .cell execution_count=20}\n``` {.python .cell-code}\nASOF_TS = df[\"call_ts\"].max()\n```\n:::\n\n\n---\n\n### Base Aggregations\n\nCompute **core contact-level metrics**: total calls, active days, duration statistics, and recency/tenure measures.\n\n::: {#5e536af6 .cell execution_count=21}\n``` {.python .cell-code}\ng = df.groupby(\"contact_id\", as_index=False)\n\nbase = g.agg(\n    n_calls=(\"call_ts\", \"size\"),\n    n_days_active=(\"call_ts\", lambda x: x.dt.date.nunique()),\n    first_call_ts=(\"call_ts\", \"min\"),\n    last_call_ts=(\"call_ts\", \"max\"),\n    total_dur_sec=(\"dur_sec\", \"sum\"),\n    mean_dur_sec=(\"dur_sec\", \"mean\"),\n    median_dur_sec=(\"dur_sec\", \"median\"),\n    p90_dur_sec=(\"dur_sec\", lambda x: x.quantile(0.90)),\n)\n\n# Recency and tenure (days)\nbase[\"recency_days\"] = (ASOF_TS - base[\"last_call_ts\"]).dt.total_seconds() / (24 * 3600)\nbase[\"tenure_days\"] = (base[\"last_call_ts\"] - base[\"first_call_ts\"]).dt.total_seconds() / (24 * 3600)\n\n# Call rates\nbase[\"calls_per_active_day\"] = safe_div(base[\"n_calls\"], base[\"n_days_active\"])\nbase[\"calls_per_tenure_day\"] = safe_div(base[\"n_calls\"], (base[\"tenure_days\"] + 1.0))\n\n# Log transforms to stabilize heavy-tailed distributions\nbase[\"log_n_calls\"] = np.log1p(base[\"n_calls\"])\nbase[\"log_total_dur\"] = np.log1p(base[\"total_dur_sec\"])\nbase[\"log_median_dur\"] = np.log1p(base[\"median_dur_sec\"])\n```\n:::\n\n\n---\n\n### Long-call Share\n\nDefine **long calls** as those exceeding the 95th percentile. Compute each contact's share of long calls to capture **tail behavior**.\n\n::: {#acbecbfe .cell execution_count=22}\n``` {.python .cell-code}\nLONG_CALL_THRESHOLD = float(df[\"dur_sec\"].quantile(0.95))\ndf[\"_is_long_call\"] = (df[\"dur_sec\"] >= LONG_CALL_THRESHOLD).astype(int)\n\nlong_share = (\n    df.groupby(\"contact_id\", as_index=False)\n      .agg(long_call_share=(\"_is_long_call\", \"mean\"))\n)\n```\n:::\n\n\n---\n\n### Time-Band Mix\n\nContacts may have different temporal activity patterns. We compute **proportions of calls per time band** (business hours, evening, late night, early morning).\n\n::: {#c66fdc0a .cell execution_count=23}\n``` {.python .cell-code}\nband_mix = (\n    pd.crosstab(df[\"contact_id\"], df[\"time_band\"], normalize=\"index\")\n      .reset_index()\n      .rename(columns={\n          \"business_hours\": \"share_business_hours\",\n          \"evening\": \"share_evening\",\n          \"late_night\": \"share_late_night\",\n          \"early_morning\": \"share_early_morning\",\n      })\n)\n\n# Ensure all expected columns exist\nfor col in [\"share_business_hours\",\"share_evening\",\"share_late_night\",\"share_early_morning\"]:\n    if col not in band_mix.columns:\n        band_mix[col] = 0.0\n```\n:::\n\n\n---\n\n### Open vs Closed Day Mix\n\nCapture **day-of-week activity patterns**. Open vs. closed days are mutually exclusive and sum to 1 per contact.\n\n::: {#b7098197 .cell execution_count=24}\n``` {.python .cell-code}\nday_mix = (\n    df.groupby(\"contact_id\", as_index=False)\n      .agg(\n          share_open_day=(\"is_open_day\", \"mean\"),\n          share_closed_day=(\"is_closed_day\", \"mean\")\n      )\n)\n\n# Sanity check\nday_mix[\"_sum\"] = day_mix[\"share_open_day\"] + day_mix[\"share_closed_day\"]\nassert np.allclose(day_mix[\"_sum\"], 1.0, atol=1e-9), \"Open+Closed shares not summing to 1.\"\nday_mix = day_mix.drop(columns=[\"_sum\"])\n```\n:::\n\n\n---\n\n### Category Mix (Optional)\n\nFor interpretation, compute the **top-K category proportions per contact**, preserving sparsity and numeric representation.\n\n::: {#8a5e8dac .cell execution_count=25}\n``` {.python .cell-code}\nTOPK = 8\ntop_categories = df[\"category\"].value_counts().head(TOPK).index.tolist()\n\ncat_tab = (\n    pd.crosstab(df[\"contact_id\"], df[\"category\"])\n      .reindex(columns=top_categories, fill_value=0)\n)\n\n# Convert to proportions\ncat_mix = (cat_tab.div(cat_tab.sum(axis=1).replace(0, 1), axis=0)\n                 .reset_index()\n                 .rename(columns={c: f\"share_cat_{c}\" for c in top_categories}))\n```\n:::\n\n\n---\n\n### Merge All Features\n\nCombine **base, long-call, time-band, day-mix, and category features** into a single contact-level table.\n\n::: {#79c9b82e .cell execution_count=26}\n``` {.python .cell-code}\ncontact_features = (\n    base.merge(long_share, on=\"contact_id\", how=\"left\")\n        .merge(band_mix, on=\"contact_id\", how=\"left\")\n        .merge(day_mix, on=\"contact_id\", how=\"left\")\n        .merge(cat_mix, on=\"contact_id\", how=\"left\")\n)\n\n# Fill any leftover NaNs\nnum_cols = contact_features.select_dtypes(include=[np.number]).columns\ncontact_features[num_cols] = contact_features[num_cols].fillna(0.0)\n\n# Preview\nprint(\"contact_features.shape =\", contact_features.shape)\nprint(\"Long-call threshold (95th pct, seconds) =\", LONG_CALL_THRESHOLD)\n\ncontact_features.head(5).T\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncontact_features.shape = (2091, 28)\nLong-call threshold (95th pct, seconds) = 396.0\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=26}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>contact_id</th>\n      <td>00047e187745</td>\n      <td>000d71073f3e</td>\n      <td>001c36642ecb</td>\n      <td>00ad9a0b1291</td>\n      <td>00ed0d18f9e3</td>\n    </tr>\n    <tr>\n      <th>n_calls</th>\n      <td>1</td>\n      <td>905</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>n_days_active</th>\n      <td>1</td>\n      <td>384</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>first_call_ts</th>\n      <td>2024-03-22 17:01:05</td>\n      <td>2022-01-03 08:26:21</td>\n      <td>2022-07-24 20:33:20</td>\n      <td>2023-06-01 15:28:13</td>\n      <td>2023-12-08 19:38:04</td>\n    </tr>\n    <tr>\n      <th>last_call_ts</th>\n      <td>2024-03-22 17:01:05</td>\n      <td>2024-09-28 15:01:13</td>\n      <td>2024-10-01 14:11:06</td>\n      <td>2023-06-01 15:28:13</td>\n      <td>2023-12-09 07:36:06</td>\n    </tr>\n    <tr>\n      <th>total_dur_sec</th>\n      <td>43</td>\n      <td>44383</td>\n      <td>564</td>\n      <td>239</td>\n      <td>232</td>\n    </tr>\n    <tr>\n      <th>mean_dur_sec</th>\n      <td>43.0</td>\n      <td>49.041989</td>\n      <td>94.0</td>\n      <td>239.0</td>\n      <td>116.0</td>\n    </tr>\n    <tr>\n      <th>median_dur_sec</th>\n      <td>43.0</td>\n      <td>22.0</td>\n      <td>59.0</td>\n      <td>239.0</td>\n      <td>116.0</td>\n    </tr>\n    <tr>\n      <th>p90_dur_sec</th>\n      <td>43.0</td>\n      <td>99.0</td>\n      <td>201.0</td>\n      <td>239.0</td>\n      <td>139.2</td>\n    </tr>\n    <tr>\n      <th>recency_days</th>\n      <td>196.13485</td>\n      <td>6.21809</td>\n      <td>3.252894</td>\n      <td>491.19934</td>\n      <td>300.527199</td>\n    </tr>\n    <tr>\n      <th>tenure_days</th>\n      <td>0.0</td>\n      <td>999.274213</td>\n      <td>799.73456</td>\n      <td>0.0</td>\n      <td>0.498634</td>\n    </tr>\n    <tr>\n      <th>calls_per_active_day</th>\n      <td>1.0</td>\n      <td>2.356771</td>\n      <td>1.5</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>calls_per_tenure_day</th>\n      <td>1.0</td>\n      <td>0.904752</td>\n      <td>0.007493</td>\n      <td>1.0</td>\n      <td>1.334548</td>\n    </tr>\n    <tr>\n      <th>log_n_calls</th>\n      <td>0.693147</td>\n      <td>6.809039</td>\n      <td>1.94591</td>\n      <td>0.693147</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>log_total_dur</th>\n      <td>3.78419</td>\n      <td>10.700634</td>\n      <td>6.336826</td>\n      <td>5.480639</td>\n      <td>5.451038</td>\n    </tr>\n    <tr>\n      <th>log_median_dur</th>\n      <td>3.78419</td>\n      <td>3.135494</td>\n      <td>4.094345</td>\n      <td>5.480639</td>\n      <td>4.762174</td>\n    </tr>\n    <tr>\n      <th>long_call_share</th>\n      <td>0.0</td>\n      <td>0.009945</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>share_business_hours</th>\n      <td>1.0</td>\n      <td>0.923757</td>\n      <td>0.833333</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>share_early_morning</th>\n      <td>0.0</td>\n      <td>0.028729</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>share_evening</th>\n      <td>0.0</td>\n      <td>0.047514</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>share_late_night</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>share_open_day</th>\n      <td>1.0</td>\n      <td>0.967956</td>\n      <td>0.666667</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>share_closed_day</th>\n      <td>0.0</td>\n      <td>0.032044</td>\n      <td>0.333333</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>share_cat_Unknown</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>share_cat_Family</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>share_cat_Supplier</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>share_cat_Important Contacts</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>share_cat_Service Provider</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Quick Feature Audit\n\nSanity check **feature completeness and distribution**.\n\n::: {#2f789136 .cell execution_count=27}\n``` {.python .cell-code}\nfeature_audit = pd.DataFrame({\n    \"n_contacts\": [int(contact_features.shape[0])],\n    \"n_features_total\": [int(contact_features.shape[1])],\n    \"any_missing_numeric\": [bool(contact_features.select_dtypes(include=[np.number]).isna().any().any())],\n    \"min_calls\": [int(contact_features[\"n_calls\"].min())],\n    \"median_calls\": [float(contact_features[\"n_calls\"].median())],\n    \"max_calls\": [int(contact_features[\"n_calls\"].max())],\n    \"median_recency_days\": [float(contact_features[\"recency_days\"].median())],\n})\nfeature_audit\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_contacts</th>\n      <th>n_features_total</th>\n      <th>any_missing_numeric</th>\n      <th>min_calls</th>\n      <th>median_calls</th>\n      <th>max_calls</th>\n      <th>median_recency_days</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2091</td>\n      <td>28</td>\n      <td>False</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>2413</td>\n      <td>445.260324</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Top Activity Contacts\n\nInspect **most active contacts** without exposing raw identifiers. This helps validate engineered features and distribution of behavioral metrics.\n\n::: {#c718ddf5 .cell execution_count=28}\n``` {.python .cell-code}\ntop_activity = contact_features.sort_values(\"n_calls\", ascending=False).head(10)[\n    [\"contact_id\",\"n_calls\",\"n_days_active\",\"total_dur_sec\",\"median_dur_sec\",\"recency_days\",\n     \"share_business_hours\",\"share_evening\",\"share_early_morning\",\"share_late_night\",\n     \"share_open_day\",\"share_closed_day\",\"long_call_share\"]\n]\ntop_activity.T\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>900</th>\n      <th>1026</th>\n      <th>572</th>\n      <th>1103</th>\n      <th>1959</th>\n      <th>1</th>\n      <th>791</th>\n      <th>1885</th>\n      <th>675</th>\n      <th>1147</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>contact_id</th>\n      <td>70ac660de5a1</td>\n      <td>8080cfddc014</td>\n      <td>497ce042ecb8</td>\n      <td>88d17d0474e5</td>\n      <td>f2f949ce03e7</td>\n      <td>000d71073f3e</td>\n      <td>632b8c0f82f4</td>\n      <td>eb0aeed50ff6</td>\n      <td>550412a925c4</td>\n      <td>8d6e4e8282d5</td>\n    </tr>\n    <tr>\n      <th>n_calls</th>\n      <td>2413</td>\n      <td>1473</td>\n      <td>1272</td>\n      <td>1077</td>\n      <td>917</td>\n      <td>905</td>\n      <td>756</td>\n      <td>477</td>\n      <td>421</td>\n      <td>369</td>\n    </tr>\n    <tr>\n      <th>n_days_active</th>\n      <td>606</td>\n      <td>592</td>\n      <td>433</td>\n      <td>447</td>\n      <td>398</td>\n      <td>384</td>\n      <td>307</td>\n      <td>97</td>\n      <td>180</td>\n      <td>186</td>\n    </tr>\n    <tr>\n      <th>total_dur_sec</th>\n      <td>120519</td>\n      <td>192181</td>\n      <td>74136</td>\n      <td>222060</td>\n      <td>186933</td>\n      <td>44383</td>\n      <td>77638</td>\n      <td>25168</td>\n      <td>23209</td>\n      <td>102636</td>\n    </tr>\n    <tr>\n      <th>median_dur_sec</th>\n      <td>28.0</td>\n      <td>48.0</td>\n      <td>12.0</td>\n      <td>76.0</td>\n      <td>32.0</td>\n      <td>22.0</td>\n      <td>32.0</td>\n      <td>36.0</td>\n      <td>40.0</td>\n      <td>75.0</td>\n    </tr>\n    <tr>\n      <th>recency_days</th>\n      <td>59.330243</td>\n      <td>4.232801</td>\n      <td>0.029317</td>\n      <td>0.124444</td>\n      <td>158.600069</td>\n      <td>6.21809</td>\n      <td>1.081655</td>\n      <td>59.142512</td>\n      <td>2.317778</td>\n      <td>63.253345</td>\n    </tr>\n    <tr>\n      <th>share_business_hours</th>\n      <td>0.822213</td>\n      <td>0.680244</td>\n      <td>0.672956</td>\n      <td>0.733519</td>\n      <td>0.72301</td>\n      <td>0.923757</td>\n      <td>0.727513</td>\n      <td>0.953878</td>\n      <td>0.973872</td>\n      <td>0.731707</td>\n    </tr>\n    <tr>\n      <th>share_evening</th>\n      <td>0.116453</td>\n      <td>0.170401</td>\n      <td>0.279088</td>\n      <td>0.163417</td>\n      <td>0.202835</td>\n      <td>0.047514</td>\n      <td>0.187831</td>\n      <td>0.033543</td>\n      <td>0.021378</td>\n      <td>0.186992</td>\n    </tr>\n    <tr>\n      <th>share_early_morning</th>\n      <td>0.033154</td>\n      <td>0.127631</td>\n      <td>0.038522</td>\n      <td>0.077994</td>\n      <td>0.061069</td>\n      <td>0.028729</td>\n      <td>0.064815</td>\n      <td>0.0</td>\n      <td>0.002375</td>\n      <td>0.04878</td>\n    </tr>\n    <tr>\n      <th>share_late_night</th>\n      <td>0.028181</td>\n      <td>0.021724</td>\n      <td>0.009434</td>\n      <td>0.02507</td>\n      <td>0.013086</td>\n      <td>0.0</td>\n      <td>0.019841</td>\n      <td>0.012579</td>\n      <td>0.002375</td>\n      <td>0.03252</td>\n    </tr>\n    <tr>\n      <th>share_open_day</th>\n      <td>0.930792</td>\n      <td>0.901561</td>\n      <td>0.956761</td>\n      <td>0.889508</td>\n      <td>0.876772</td>\n      <td>0.967956</td>\n      <td>0.915344</td>\n      <td>0.960168</td>\n      <td>0.980998</td>\n      <td>0.864499</td>\n    </tr>\n    <tr>\n      <th>share_closed_day</th>\n      <td>0.069208</td>\n      <td>0.098439</td>\n      <td>0.043239</td>\n      <td>0.110492</td>\n      <td>0.123228</td>\n      <td>0.032044</td>\n      <td>0.084656</td>\n      <td>0.039832</td>\n      <td>0.019002</td>\n      <td>0.135501</td>\n    </tr>\n    <tr>\n      <th>long_call_share</th>\n      <td>0.008288</td>\n      <td>0.076035</td>\n      <td>0.017296</td>\n      <td>0.138347</td>\n      <td>0.139586</td>\n      <td>0.009945</td>\n      <td>0.047619</td>\n      <td>0.002096</td>\n      <td>0.004751</td>\n      <td>0.195122</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n::: callout-note\n## Phase 2 — Stakeholder-ready summary (what we learned)\n\nWe successfully converted **24,952 calls** into a **contact-level modeling table** with **2,091 contacts** and **28 features**.\n\n### What the Phase 2 feature table represents (business meaning)\nEach row is a **contact** (hashed ID), and the features capture:\n\n- **Volume / intensity:** `n_calls`, `n_days_active`, `calls_per_active_day`, `calls_per_tenure_day`\n- **Relationship depth / effort proxy:** `total_dur_sec`, `median_dur_sec`, `p90_dur_sec`, plus log versions\n- **Recency / dormancy risk:** `recency_days` (how long since last call)\n- **Time preference / accessibility:** shares by time band (business/evening/early/late)\n- **Open vs closed day behavior:** `share_open_day`, `share_closed_day`\n- **Tail behavior:** `long_call_share` where “long” is **≥ 396s** (95th percentile)\n- **Category mix (interpretation aid):** proportions of top-8 categories per contact\n\n### Sanity checks passed (safe to proceed)\n- No missing numeric values in engineered features.\n- Open + closed day shares sum to 1 per contact (validated).\n- Time-band shares exist for every contact (columns ensured even if 0).\n\n### What the results already hint at (without clustering yet)\n- **Highly skewed activity:** median contact has **2 calls**, but the top contact has **2,413** calls → we must **scale/transform** before K-means.\n- **Recency is large for many contacts:** median `recency_days ≈ 445` → many contacts are dormant; segmentation must separate “active” vs “inactive”.\n- Top activity contacts show diverse patterns:\n  - some are high-volume but mostly business hours\n  - some show meaningful evening/early activity\n  - some have high `long_call_share` (deep conversations or issue resolution)\n\n### Modeling implication\n\nAt this point we have a **contact-level feature table** with strong business meaning (volume, duration, recency, timing).  \nBefore clustering, we must build a **clean clustering matrix** $X$ so distance-based algorithms behave correctly.\n\nWe will build $X$ such that it:\n\n1) **Excludes non-model columns**  \n   - No timestamps (`first_call_ts`, `last_call_ts`) and no raw identifiers.  \n   - Category mix is kept for interpretation, but excluded from default training to avoid “baking in labels”.\n\n2) **Controls heavy tails and near-constant features**  \n   - Phone-call behavior is highly skewed (a few contacts dominate calls/duration).  \n   - We apply robust transforms already engineered (log features) and use a *robust preprocessing pipeline*.  \n   - We also drop **near-constant (IQR≈0) features** to avoid unstable scaling and distance domination.\n\n3) **Scales fairly for distance-based clustering**  \n   - We use **median imputation** (robust, future-proof if new features introduce missingness).  \n   - We use **RobustScaler (median/IQR)** instead of StandardScaler to reduce outlier influence.\n\n---\n\n### Transition\n\nPhase 3 converts `contact_features` into **clustering-ready inputs** and produces:\n\n- a **feature-block dictionary** for later sensitivity tests (volume vs duration vs timing vs recency),\n- a cleaned, scaled matrix **`X`** and aligned **`feature_names`** for reproducibility and interpretation.\n:::\n\n---\n\n\n## Phase 3 — Preprocess (bullet-proof, distance-based clustering ready)\n\nThis phase prepares the **contact-level feature table** for distance-based clustering by producing a stable numeric matrix **`X`** (rows = contacts, columns = standardized features).\n\nWe do four things:\n\n1) **Define feature blocks** (auditable and reusable for sensitivity tests later)  \n2) **Build the default model matrix** (exclude timestamps and category mix by default)  \n3) **Stabilize heavy tails** (clip extreme values in raw space)  \n4) **Impute + robust-scale** (median imputation + RobustScaler) with diagnostics\n\n::: callout-tip\n### Why RobustScaler (not StandardScaler)\nContact behaviour is heavy-tailed (a few contacts dominate activity).  \nDistance-based clustering is scale-sensitive. **RobustScaler uses median and IQR**, reducing outlier influence and producing more stable distances.\n:::\n\n---\n\n### Freeze an interpretation copy (read-only view)\n\n::: {#0ba7bb09 .cell execution_count=29}\n``` {.python .cell-code}\ncontact_features_view = contact_features.copy()\n```\n:::\n\n\n---\n\n### Define feature blocks (for interpretation and sensitivity analysis)\n\n::: {#acb423ea .cell execution_count=30}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n```\n:::\n\n\n::: {#ba9e6cdf .cell execution_count=31}\n``` {.python .cell-code}\n# -----------------------------\n# Feature blocks (business meaning)\n# -----------------------------\nvolume_features = [\n    \"n_calls\", \"n_days_active\", \"calls_per_active_day\", \"calls_per_tenure_day\", \"log_n_calls\"\n]\n\nduration_features = [\n    \"total_dur_sec\", \"mean_dur_sec\", \"median_dur_sec\", \"p90_dur_sec\",\n    \"log_total_dur\", \"log_median_dur\", \"long_call_share\"\n]\n\nrecency_features = [\n    \"recency_days\", \"tenure_days\"\n]\n\ntiming_features = [\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\n# Category mix is useful for interpretation, but excluded from default model training\ncategory_features = [c for c in contact_features.columns if c.startswith(\"share_cat_\")]\n\nFEATURE_BLOCKS = {\n    \"volume\": volume_features,\n    \"duration\": duration_features,\n    \"recency\": recency_features,\n    \"timing\": timing_features,\n    \"category\": category_features\n}\n\n# Validate engineered feature presence (hard gate)\nmissing = {k: [f for f in v if f not in contact_features.columns] for k, v in FEATURE_BLOCKS.items()}\nmissing = {k: v for k, v in missing.items() if len(v) > 0}\nassert len(missing) == 0, f\"Missing engineered features: {missing}\"\n\nFEATURE_BLOCKS\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n{'volume': ['n_calls',\n  'n_days_active',\n  'calls_per_active_day',\n  'calls_per_tenure_day',\n  'log_n_calls'],\n 'duration': ['total_dur_sec',\n  'mean_dur_sec',\n  'median_dur_sec',\n  'p90_dur_sec',\n  'log_total_dur',\n  'log_median_dur',\n  'long_call_share'],\n 'recency': ['recency_days', 'tenure_days'],\n 'timing': ['share_business_hours',\n  'share_evening',\n  'share_early_morning',\n  'share_late_night',\n  'share_open_day',\n  'share_closed_day'],\n 'category': ['share_cat_Unknown',\n  'share_cat_Family',\n  'share_cat_Supplier',\n  'share_cat_Important Contacts',\n  'share_cat_Service Provider']}\n```\n:::\n:::\n\n\n---\n\n### Select default model features (no timestamps, no category mix by default)\n\nWe train clustering on behaviour (volume, duration, recency, timing).\nCategory mix stays available for interpretation and later sensitivity checks.\n\n::: {#22c837b6 .cell execution_count=32}\n``` {.python .cell-code}\nMODEL_FEATURES = (\n    FEATURE_BLOCKS[\"volume\"]\n    + FEATURE_BLOCKS[\"duration\"]\n    + FEATURE_BLOCKS[\"recency\"]\n    + FEATURE_BLOCKS[\"timing\"]\n)\n\nX_raw = contact_features[MODEL_FEATURES].copy()\nprint(\"X_raw.shape =\", X_raw.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX_raw.shape = (2091, 20)\n```\n:::\n:::\n\n\n---\n\n### Diagnostics before preprocessing (missingness + inf safety)\n\n::: {#617d8076 .cell execution_count=33}\n``` {.python .cell-code}\n# Missingness inspection (future-proof if new features introduce NaNs)\nmiss = X_raw.isna().sum().sort_values(ascending=False)\nmiss_table = miss[miss > 0].to_frame(\"n_missing\")\nmiss_table\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_missing</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#4cb3f4d0 .cell execution_count=34}\n``` {.python .cell-code}\n# Inf / -Inf inspection (must not exist)\nhas_inf = np.isinf(X_raw.to_numpy(dtype=float)).any()\nprint(\"Any inf in X_raw:\", bool(has_inf))\nassert not has_inf, \"Infinite values detected in X_raw (must fix before scaling).\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAny inf in X_raw: False\n```\n:::\n:::\n\n\n---\n\n### Stability strategy (recommended): drop only truly constant columns + clip extremes\n\n**Important correction:**\nDropping “low IQR” features is too aggressive in contact data (many contacts have 1–2 calls, which makes shares look constant).\nInstead we:\n\n* drop only **truly constant** columns (no information),\n* clip each feature to **p01–p99** in raw units (winsorization),\n* then apply RobustScaler.\n\n::: {#c8d02342 .cell execution_count=35}\n``` {.python .cell-code}\n# A) Drop truly constant columns only (no information)\nnunique = X_raw.nunique(dropna=False)\nconstant_cols = nunique[nunique <= 1].index.tolist()\nprint(\"Truly-constant cols (drop):\", constant_cols)\n\nX_raw2 = X_raw.drop(columns=constant_cols) if constant_cols else X_raw.copy()\n\n# B) Clip extremes in RAW space to stabilize heavy tails\nCLIP_LO = 0.01\nCLIP_HI = 0.99\n\nclip_info = []\nX_clip = X_raw2.copy()\n\nfor col in X_clip.columns:\n    lo = float(X_clip[col].quantile(CLIP_LO))\n    hi = float(X_clip[col].quantile(CLIP_HI))\n    X_clip[col] = X_clip[col].clip(lower=lo, upper=hi)\n    clip_info.append({\"feature\": col, \"p01\": lo, \"p99\": hi})\n\nclip_table = pd.DataFrame(clip_info).sort_values(\"feature\").reset_index(drop=True)\n\nX_clip.shape, clip_table.head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTruly-constant cols (drop): []\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\n((2091, 20),\n                 feature       p01         p99\n 0  calls_per_active_day  1.000000    5.000000\n 1  calls_per_tenure_day  0.005871    4.028542\n 2        log_median_dur  1.098612    6.634896\n 3           log_n_calls  0.693147    5.153539\n 4         log_total_dur  1.098612    9.842541\n 5       long_call_share  0.000000    1.000000\n 6          mean_dur_sec  2.000000  771.900000\n 7        median_dur_sec  2.000000  760.200000\n 8               n_calls  1.000000  172.300000\n 9         n_days_active  1.000000   96.100000)\n```\n:::\n:::\n\n\n---\n\n### Median imputation + RobustScaler\n\n::: {#b63c6f8e .cell execution_count=36}\n``` {.python .cell-code}\n# Median imputation is robust (and future-proof for features that may introduce NaNs later)\nimputer = SimpleImputer(strategy=\"median\")\n\n# RobustScaler uses median/IQR to reduce outlier influence\nscaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n\nX_imputed = imputer.fit_transform(X_clip)\nX = scaler.fit_transform(X_imputed)\n\nprint(\"X.shape =\", X.shape)\nprint(\"Any NaN in X:\", bool(np.isnan(X).any()))\nassert not np.isnan(X).any(), \"NaNs remain after preprocessing (must fix).\"\n\nprint(\"max_abs_after_scaling =\", float(np.max(np.abs(X))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX.shape = (2091, 20)\nAny NaN in X: False\nmax_abs_after_scaling = 95.09999999999991\n```\n:::\n:::\n\n\n---\n\n### Diagnostics: identify any dominating feature in scaled space\n\nIf one feature still dominates distances, we detect it explicitly.\n\n::: {#3026b6b7 .cell execution_count=37}\n``` {.python .cell-code}\nfeature_names = list(X_clip.columns)\nmax_abs_by_feature = np.max(np.abs(X), axis=0)\n\ndominance = (\n    pd.DataFrame({\n        \"feature\": feature_names,\n        \"max_abs_scaled\": max_abs_by_feature,\n        \"raw_p01\": clip_table.set_index(\"feature\").loc[feature_names, \"p01\"].values,\n        \"raw_p99\": clip_table.set_index(\"feature\").loc[feature_names, \"p99\"].values,\n        \"raw_iqr\": (X_raw2.quantile(0.75) - X_raw2.quantile(0.25)).reindex(feature_names).values,\n        \"n_unique\": X_raw2.nunique().reindex(feature_names).values\n    })\n    .sort_values(\"max_abs_scaled\", ascending=False)\n    .reset_index(drop=True)\n)\n\ndominance.head(12)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>max_abs_scaled</th>\n      <th>raw_p01</th>\n      <th>raw_p99</th>\n      <th>raw_iqr</th>\n      <th>n_unique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n_days_active</td>\n      <td>95.100000</td>\n      <td>1.000000</td>\n      <td>96.100000</td>\n      <td>1.000000</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>total_dur_sec</td>\n      <td>78.405858</td>\n      <td>2.000000</td>\n      <td>18830.000000</td>\n      <td>239.000000</td>\n      <td>706</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n_calls</td>\n      <td>56.766667</td>\n      <td>1.000000</td>\n      <td>172.300000</td>\n      <td>3.000000</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tenure_days</td>\n      <td>25.858679</td>\n      <td>0.000000</td>\n      <td>941.093146</td>\n      <td>36.393634</td>\n      <td>1108</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>p90_dur_sec</td>\n      <td>13.405819</td>\n      <td>2.000000</td>\n      <td>1375.100000</td>\n      <td>97.950000</td>\n      <td>1070</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>median_dur_sec</td>\n      <td>11.060536</td>\n      <td>2.000000</td>\n      <td>760.200000</td>\n      <td>65.250000</td>\n      <td>424</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>mean_dur_sec</td>\n      <td>10.365775</td>\n      <td>2.000000</td>\n      <td>771.900000</td>\n      <td>70.125000</td>\n      <td>920</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>log_n_calls</td>\n      <td>4.425372</td>\n      <td>0.693147</td>\n      <td>5.153539</td>\n      <td>0.916291</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>calls_per_active_day</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>170</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>calls_per_tenure_day</td>\n      <td>3.649338</td>\n      <td>0.005871</td>\n      <td>4.028542</td>\n      <td>0.829888</td>\n      <td>1116</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>log_total_dur</td>\n      <td>2.554233</td>\n      <td>1.098612</td>\n      <td>9.842541</td>\n      <td>2.083111</td>\n      <td>706</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>log_median_dur</td>\n      <td>1.706673</td>\n      <td>1.098612</td>\n      <td>6.634896</td>\n      <td>1.733545</td>\n      <td>424</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Preprocess audit (compact, decision-useful)\n\n::: {#108515cf .cell execution_count=38}\n``` {.python .cell-code}\npreprocess_audit = pd.DataFrame({\n    \"n_contacts\": [int(X.shape[0])],\n    \"n_model_features\": [int(X.shape[1])],\n    \"n_constant_dropped\": [int(len(constant_cols))],\n    \"constant_dropped\": [\", \".join(constant_cols) if constant_cols else \"None\"],\n    \"clip_low_quantile\": [CLIP_LO],\n    \"clip_high_quantile\": [CLIP_HI],\n    \"n_features_with_missing_before\": [int((X_raw.isna().sum() > 0).sum())],\n    \"total_missing_cells_before\": [int(X_raw.isna().sum().sum())],\n    \"max_abs_after_scaling\": [float(np.max(np.abs(X)))],\n})\n\npreprocess_audit\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_contacts</th>\n      <th>n_model_features</th>\n      <th>n_constant_dropped</th>\n      <th>constant_dropped</th>\n      <th>clip_low_quantile</th>\n      <th>clip_high_quantile</th>\n      <th>n_features_with_missing_before</th>\n      <th>total_missing_cells_before</th>\n      <th>max_abs_after_scaling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2091</td>\n      <td>20</td>\n      <td>0</td>\n      <td>None</td>\n      <td>0.01</td>\n      <td>0.99</td>\n      <td>0</td>\n      <td>0</td>\n      <td>95.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Store preprocessing artifacts (in-memory, safe by default)\n\n::: {#0f736885 .cell execution_count=39}\n``` {.python .cell-code}\nprep = {\n    \"MODEL_FEATURES\": MODEL_FEATURES,\n    \"FEATURE_BLOCKS\": FEATURE_BLOCKS,\n    \"feature_names\": feature_names,\n    \"imputer\": imputer,\n    \"scaler\": scaler,\n    \"X_raw\": X_raw2,      # before clipping (audit)\n    \"X_clip\": X_clip,     # clipped raw matrix (audit)\n    \"X\": X,               # final clustering matrix\n    \"clip_table\": clip_table,\n    \"dominance\": dominance,\n    \"preprocess_audit\": preprocess_audit\n}\n\n# Raw-unit summary (post-clip = what the scaler actually sees)\nraw_summary = X_clip.describe(percentiles=[0.5, 0.9, 0.95, 0.99]).T\nraw_summary = raw_summary[[\"count\", \"mean\", \"std\", \"min\", \"50%\", \"90%\", \"95%\", \"99%\", \"max\"]]\nraw_summary.head(12)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>50%</th>\n      <th>90%</th>\n      <th>95%</th>\n      <th>99%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n_calls</th>\n      <td>2091.0</td>\n      <td>7.560641</td>\n      <td>22.535264</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>12.000000</td>\n      <td>28.000000</td>\n      <td>169.330000</td>\n      <td>172.300000</td>\n    </tr>\n    <tr>\n      <th>n_days_active</th>\n      <td>2091.0</td>\n      <td>4.503156</td>\n      <td>12.474802</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>7.000000</td>\n      <td>17.000000</td>\n      <td>96.010000</td>\n      <td>96.100000</td>\n    </tr>\n    <tr>\n      <th>calls_per_active_day</th>\n      <td>2091.0</td>\n      <td>1.490754</td>\n      <td>0.784284</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.333333</td>\n      <td>3.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>calls_per_tenure_day</th>\n      <td>2091.0</td>\n      <td>0.987043</td>\n      <td>0.853444</td>\n      <td>0.005871</td>\n      <td>1.000000</td>\n      <td>1.998381</td>\n      <td>2.894101</td>\n      <td>4.005407</td>\n      <td>4.028542</td>\n    </tr>\n    <tr>\n      <th>log_n_calls</th>\n      <td>2091.0</td>\n      <td>1.337442</td>\n      <td>0.932739</td>\n      <td>0.693147</td>\n      <td>1.098612</td>\n      <td>2.564949</td>\n      <td>3.367296</td>\n      <td>5.137573</td>\n      <td>5.153539</td>\n    </tr>\n    <tr>\n      <th>total_dur_sec</th>\n      <td>2091.0</td>\n      <td>711.445720</td>\n      <td>2477.954937</td>\n      <td>2.000000</td>\n      <td>91.000000</td>\n      <td>1107.000000</td>\n      <td>3019.500000</td>\n      <td>18608.600000</td>\n      <td>18830.000000</td>\n    </tr>\n    <tr>\n      <th>mean_dur_sec</th>\n      <td>2091.0</td>\n      <td>84.096441</td>\n      <td>126.033557</td>\n      <td>2.000000</td>\n      <td>45.000000</td>\n      <td>184.000000</td>\n      <td>310.800000</td>\n      <td>767.490000</td>\n      <td>771.900000</td>\n    </tr>\n    <tr>\n      <th>median_dur_sec</th>\n      <td>2091.0</td>\n      <td>72.592396</td>\n      <td>115.864389</td>\n      <td>2.000000</td>\n      <td>38.500000</td>\n      <td>153.500000</td>\n      <td>276.500000</td>\n      <td>760.020000</td>\n      <td>760.200000</td>\n    </tr>\n    <tr>\n      <th>p90_dur_sec</th>\n      <td>2091.0</td>\n      <td>126.747585</td>\n      <td>215.263200</td>\n      <td>2.000000</td>\n      <td>62.000000</td>\n      <td>268.600000</td>\n      <td>494.400000</td>\n      <td>1375.010000</td>\n      <td>1375.100000</td>\n    </tr>\n    <tr>\n      <th>log_total_dur</th>\n      <td>2091.0</td>\n      <td>4.617053</td>\n      <td>1.825819</td>\n      <td>1.098612</td>\n      <td>4.521789</td>\n      <td>7.010312</td>\n      <td>8.013169</td>\n      <td>9.831353</td>\n      <td>9.842541</td>\n    </tr>\n    <tr>\n      <th>log_median_dur</th>\n      <td>2091.0</td>\n      <td>3.546764</td>\n      <td>1.254063</td>\n      <td>1.098612</td>\n      <td>3.676301</td>\n      <td>5.040194</td>\n      <td>5.625780</td>\n      <td>6.634660</td>\n      <td>6.634896</td>\n    </tr>\n    <tr>\n      <th>long_call_share</th>\n      <td>2091.0</td>\n      <td>0.038448</td>\n      <td>0.156275</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.008288</td>\n      <td>0.285714</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n\n### A Final numeric stability guardrail\n\nEven after robust scaling, distance-based clustering can still be dominated by a small number of extreme points.  \nTo make the pipeline **bullet-proof**, we apply a final cap in standardized space so no single feature can overwhelm Euclidean distances.\n\n::: {#04e3a87b .cell execution_count=40}\n``` {.python .cell-code}\n# Final stability cap in scaled space (portfolio-grade guardrail)\nSCALED_CAP = 10.0\n\nX_uncapped = prep[\"X\"]\nX_capped = np.clip(X_uncapped, -SCALED_CAP, SCALED_CAP)\n\ncap_audit = pd.DataFrame({\n    \"scaled_cap\": [SCALED_CAP],\n    \"max_abs_before\": [float(np.max(np.abs(X_uncapped)))],\n    \"max_abs_after\": [float(np.max(np.abs(X_capped)))],\n})\n\n# Update the matrix used downstream (keep uncapped for audit)\nprep[\"X_uncapped\"] = X_uncapped\nprep[\"X\"] = X_capped\ncap_audit\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scaled_cap</th>\n      <th>max_abs_before</th>\n      <th>max_abs_after</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10.0</td>\n      <td>95.1</td>\n      <td>10.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n\n::: callout-note\n## Phase 3 — Sign-off (ready for K-selection)\n\nWe produced a stable clustering matrix `prep[\"X\"]` for **2,091 contacts × 20 features**.\n\n**Robustness guarantees:**\n- No missing values or infinities\n- Heavy tails controlled by **p01–p99 clipping**\n- Distance stability guaranteed by **post-scaling cap** (max absolute value ≤ 10)\n\n**Next:** Phase 4 chooses the number of clusters $K$ using a multi-metric selection protocol (inertia, silhouette, Davies–Bouldin) on `prep[\"X\"]`.\n:::\n\n\n\n## Phase 4 — Choose $K$ (model selection for clustering)\n\nChoosing $K$ is a **model selection** problem: different values of $K$ define different segmentations.\nWe evaluate candidate $K$ values using three complementary criteria:\n\n- **Inertia (SSE)**: decreases with $K$; we look for an “elbow” (diminishing returns).\n- **Silhouette**: higher is better; measures separation vs cohesion.\n- **Davies–Bouldin**: lower is better; penalizes overlapping clusters.\n\n::: callout-important\nWe compute metrics on the **preprocessed matrix** `prep[\"X\"]` (robust-scaled and capped).  \nThis ensures the $K$ decision reflects behavioural structure rather than raw-unit dominance.\n:::\n\n---\n\n### Compute $K$-search metrics\n\n::: {#aaff3e9b .cell execution_count=41}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\n\nX = prep[\"X\"]  # capped, clustering-ready matrix\n\n# Candidate K range (employer-ready default)\nK_MIN, K_MAX = 2, 12\n\nrows = []\nfor k in range(K_MIN, K_MAX + 1):\n    km = KMeans(\n        n_clusters=k,\n        n_init=50,         # more restarts = more reliable\n        random_state=42\n    )\n    labels = km.fit_predict(X)\n\n    inertia = float(km.inertia_)\n    sil = float(silhouette_score(X, labels))\n    db = float(davies_bouldin_score(X, labels))\n\n    rows.append({\n        \"k\": k,\n        \"inertia\": inertia,\n        \"silhouette\": sil,\n        \"davies_bouldin\": db\n    })\n\nk_metrics = pd.DataFrame(rows)\n\n# Add a simple \"elbow help\": marginal gain in inertia\nk_metrics[\"inertia_drop\"] = k_metrics[\"inertia\"].shift(1) - k_metrics[\"inertia\"]\nk_metrics[\"inertia_drop_pct\"] = (k_metrics[\"inertia_drop\"] / k_metrics[\"inertia\"].shift(1)) * 100\n\nk_metrics\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>k</th>\n      <th>inertia</th>\n      <th>silhouette</th>\n      <th>davies_bouldin</th>\n      <th>inertia_drop</th>\n      <th>inertia_drop_pct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>50974.583857</td>\n      <td>0.651267</td>\n      <td>0.746627</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>37194.004774</td>\n      <td>0.646105</td>\n      <td>0.828924</td>\n      <td>13780.579083</td>\n      <td>27.034216</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>26086.981574</td>\n      <td>0.589088</td>\n      <td>0.771859</td>\n      <td>11107.023200</td>\n      <td>29.862402</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>22856.984343</td>\n      <td>0.482657</td>\n      <td>0.926567</td>\n      <td>3229.997231</td>\n      <td>12.381644</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>20323.525176</td>\n      <td>0.457261</td>\n      <td>0.999610</td>\n      <td>2533.459166</td>\n      <td>11.083961</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7</td>\n      <td>18523.891058</td>\n      <td>0.430309</td>\n      <td>1.050928</td>\n      <td>1799.634118</td>\n      <td>8.854931</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8</td>\n      <td>16933.390393</td>\n      <td>0.396590</td>\n      <td>1.021011</td>\n      <td>1590.500665</td>\n      <td>8.586213</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9</td>\n      <td>15403.105156</td>\n      <td>0.375016</td>\n      <td>1.019315</td>\n      <td>1530.285237</td>\n      <td>9.037087</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10</td>\n      <td>14055.290862</td>\n      <td>0.320327</td>\n      <td>0.997488</td>\n      <td>1347.814294</td>\n      <td>8.750277</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>11</td>\n      <td>13118.789180</td>\n      <td>0.319340</td>\n      <td>1.022192</td>\n      <td>936.501682</td>\n      <td>6.662983</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>12</td>\n      <td>12285.369653</td>\n      <td>0.321039</td>\n      <td>1.040977</td>\n      <td>833.419528</td>\n      <td>6.352869</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Visualize metrics (elbow + quality trade-offs)\n\n::: {#67616547 .cell execution_count=42}\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-43-output-1.png){width=662 height=470}\n:::\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-43-output-2.png){width=662 height=470}\n:::\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-43-output-3.png){width=662 height=470}\n:::\n:::\n\n\n---\n\n### Candidate shortlist (data-driven)\n\nWe shortlist $K$ values that are plausible trade-offs:\n\n* among the **top silhouette** scores,\n* among the **lowest Davies–Bouldin** scores,\n* and not far beyond the elbow (where inertia gains flatten).\n\n::: {#1e14cd6f .cell execution_count=43}\n``` {.python .cell-code}\n# ----------------------------------------\n# Candidate shortlist — modern, auditable\n# ----------------------------------------\n\n# Identify top-K by each metric\ntop_sil = k_metrics.nlargest(5, \"silhouette\").copy()\ntop_sil[\"reason\"] = \"top_silhouette\"\n\nlow_db = k_metrics.nsmallest(5, \"davies_bouldin\").copy()\nlow_db[\"reason\"] = \"low_davies_bouldin\"\n\n# Combine and mark duplicates\nshortlist = pd.concat([top_sil, low_db]).reset_index(drop=True)\n\n# If a K appears in both, update reason\nshortlist = (\n    shortlist.groupby(\"k\", as_index=False)\n    .agg({\n        \"silhouette\": \"first\",\n        \"davies_bouldin\": \"first\",\n        \"inertia\": \"first\",\n        \"reason\": lambda x: \" & \".join(sorted(set(x)))\n    })\n    .sort_values(\"k\")\n    .reset_index(drop=True)\n)\n\n# Round metrics for readability\nshortlist[[\"silhouette\", \"davies_bouldin\", \"inertia\"]] = shortlist[\n    [\"silhouette\", \"davies_bouldin\", \"inertia\"]\n].round(4)\n\nshortlist\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>k</th>\n      <th>silhouette</th>\n      <th>davies_bouldin</th>\n      <th>inertia</th>\n      <th>reason</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>0.6513</td>\n      <td>0.7466</td>\n      <td>50974.5839</td>\n      <td>low_davies_bouldin &amp; top_silhouette</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0.6461</td>\n      <td>0.8289</td>\n      <td>37194.0048</td>\n      <td>low_davies_bouldin &amp; top_silhouette</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>0.5891</td>\n      <td>0.7719</td>\n      <td>26086.9816</td>\n      <td>low_davies_bouldin &amp; top_silhouette</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>0.4827</td>\n      <td>0.9266</td>\n      <td>22856.9843</td>\n      <td>low_davies_bouldin &amp; top_silhouette</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>0.4573</td>\n      <td>0.9996</td>\n      <td>20323.5252</td>\n      <td>top_silhouette</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10</td>\n      <td>0.3203</td>\n      <td>0.9975</td>\n      <td>14055.2909</td>\n      <td>low_davies_bouldin</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n\n\n### What the metrics say (from first principles)\n\n#### Inertia (elbow)\nHuge drops from **K=2 → 3** (~27%) and **3 → 4** (~30%).  \nAfter **K=4**, the drop collapses to ~12% and then <~11% thereafter.  \nThat’s a classic **elbow at K≈4**.\n\n#### Silhouette (separation vs cohesion)\n- **K=2:** 0.651 (very strong)  \n- **K=3:** 0.646 (very strong)  \n- **K=4:** 0.589 (still strong)  \n\nThen it falls sharply (e.g., **K=5** = 0.483 and continues down).  \nStructure is strongest for small K, especially **2–4**.\n\n#### Davies–Bouldin (overlap; lower is better)\n- Best is **K=2** (0.747), then **K=4** (0.772), then **K=3** (0.829).  \nAfter that, the metric worsens notably.\n\n---\n\n### Decision\nChoose **K=4** (recommended).  \n\n- **K=2:** too coarse for actionability (“low vs high activity”)  \n- **K=3:** better but still merges distinct behaviours (“steady frequent” vs “bursty intense”)  \n- **K=4:** lands at the elbow, with strong silhouette & Davies–Bouldin — best balance of interpretability, actionability, and metric support.\n\n::: callout-important\n## Decision: choose $K=4$\n\nWe select **$K=4$** as the operational segmentation because it balances:\n\n- **Structure quality:** strong silhouette (0.589) and low Davies–Bouldin (0.772)  \n- **Parsimony:** clear elbow around **K=4** (large inertia drops up to 4, then diminishing returns)  \n- **Actionability:** more useful than K=2 while avoiding over-fragmentation at higher K\n\n**Next:** fit K-means with **K=4** and interpret clusters using feature-block summaries (volume, duration, recency, timing).\n:::\n\n---\n\n### Optional: why we may explore $K=5$ (exploratory only)\n\n::: callout-warning\n$K=5$ is **not** recommended as the default operational segmentation in this project.\n\nCompared to $K=4$, separation quality declines noticeably:\n\n- Silhouette: **0.589 → 0.483** (weaker cohesion/separation)\n- Davies–Bouldin: **0.772 → 0.927** (more overlap between clusters)\n\nWe may still explore $K=5$ **only as an exploratory lens** when stakeholders want finer distinctions inside high-activity groups (e.g., separating “steady frequent” from “burst-intense”). Any $K=5$ results should be presented as *additional insight* without changing the primary framework.\n:::\n\n> **Stakeholder takeaway:** K=4 remains the primary segmentation. K=5 can be used **selectively** to provide additional insights where fine distinctions are important, without altering the main operational framework.\n\n---\n\n## Phase 5 — Fit and interpret clusters (K-means, $K=4$)\n\nWe fit K-means on the **clustering matrix** `prep[\"X\"]` (robust-scaled and capped for numeric stability).  \nFor interpretation, we summarise clusters back in **business units** using `contact_features_view` (raw-unit features) and translate clusters into **stakeholder-ready segments with actions**.\n\n::: callout-important\n### Training vs interpretation (do not mix these)\n- **Training:** uses `prep[\"X\"]` (scaled + capped) so Euclidean distances are fair and stable.\n- **Interpretation:** uses raw-unit features (calls, seconds, days, shares) so segments can be explained and acted on.\n- **Privacy:** we keep `contact_id` hashed and do not merge any name/number fields into report outputs.\n:::\n\n---\n\n### Fit K-means ($K=4$) and store labels\n\n::: {#d8ed4b8c .cell execution_count=44}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\n\nX = prep[\"X\"]   # final clustering matrix (scaled + capped)\nK_FINAL = 4\n\nkmeans = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42)\nlabels = kmeans.fit_predict(X)\n\n# One label per contact (hashed ID only)\ncontact_clusters = pd.DataFrame({\n    \"contact_id\": contact_features_view[\"contact_id\"].values,\n    \"cluster\": labels\n})\n\ncluster_sizes = (\n    contact_clusters[\"cluster\"]\n    .value_counts()\n    .sort_index()\n    .to_frame(\"n_contacts\")\n)\n\ncluster_sizes\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_contacts</th>\n    </tr>\n    <tr>\n      <th>cluster</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>91</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>176</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1599</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>225</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Join clusters back to contact-level table (raw units)\n\n::: {#d0ad2656 .cell execution_count=45}\n``` {.python .cell-code}\ncontact_labeled = contact_features_view.merge(contact_clusters, on=\"contact_id\", how=\"left\")\n\n# Hard gate: every contact must get a cluster label\nassert contact_labeled[\"cluster\"].isna().sum() == 0\n\ncontact_labeled[[\"contact_id\", \"cluster\"]].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contact_id</th>\n      <th>cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00047e187745</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000d71073f3e</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001c36642ecb</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00ad9a0b1291</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00ed0d18f9e3</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Cluster “cards” (robust medians + size)\n\nThese cards are stakeholder-ready: **one row per cluster**, using **medians** (robust to heavy tails).\n\n::: {#74f6cc6b .cell execution_count=46}\n``` {.python .cell-code}\ncard_features = [\n    # volume / intensity\n    \"n_calls\", \"n_days_active\", \"calls_per_active_day\",\n\n    # duration / relationship depth proxy\n    \"total_dur_sec\", \"median_dur_sec\", \"long_call_share\",\n\n    # recency / tenure\n    \"recency_days\", \"tenure_days\",\n\n    # timing mix\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\nmissing_cards = [c for c in card_features if c not in contact_labeled.columns]\nassert len(missing_cards) == 0, f\"Missing card features: {missing_cards}\"\n\ncards = (\n    contact_labeled\n    .groupby(\"cluster\")[card_features]\n    .median()\n    .merge(cluster_sizes, left_index=True, right_index=True)\n    .reset_index()\n)\n\ncards = cards[[\"cluster\", \"n_contacts\"] + card_features]\ncards.T\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>cluster</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>n_contacts</th>\n      <td>91.000000</td>\n      <td>176.000000</td>\n      <td>1599.000000</td>\n      <td>225.000000</td>\n    </tr>\n    <tr>\n      <th>n_calls</th>\n      <td>2.000000</td>\n      <td>35.500000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>n_days_active</th>\n      <td>2.000000</td>\n      <td>21.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>calls_per_active_day</th>\n      <td>1.200000</td>\n      <td>1.707143</td>\n      <td>1.000000</td>\n      <td>1.333333</td>\n    </tr>\n    <tr>\n      <th>total_dur_sec</th>\n      <td>1553.000000</td>\n      <td>2776.000000</td>\n      <td>61.000000</td>\n      <td>270.000000</td>\n    </tr>\n    <tr>\n      <th>median_dur_sec</th>\n      <td>494.000000</td>\n      <td>35.500000</td>\n      <td>37.000000</td>\n      <td>34.000000</td>\n    </tr>\n    <tr>\n      <th>long_call_share</th>\n      <td>0.600000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>recency_days</th>\n      <td>621.220833</td>\n      <td>84.802164</td>\n      <td>501.331516</td>\n      <td>259.384988</td>\n    </tr>\n    <tr>\n      <th>tenure_days</th>\n      <td>0.813924</td>\n      <td>676.594664</td>\n      <td>0.000000</td>\n      <td>312.908426</td>\n    </tr>\n    <tr>\n      <th>share_business_hours</th>\n      <td>1.000000</td>\n      <td>0.911879</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>share_evening</th>\n      <td>0.000000</td>\n      <td>0.047619</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>share_early_morning</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>share_late_night</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>share_open_day</th>\n      <td>1.000000</td>\n      <td>0.983416</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>share_closed_day</th>\n      <td>0.000000</td>\n      <td>0.016584</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Within-cluster spread check (to avoid misleading medians)\n\nWe report 25/50/75% for a few “anchor” variables.\n\n::: {#e03dd6e6 .cell execution_count=47}\n``` {.python .cell-code}\nspread_features = [\"n_calls\", \"total_dur_sec\", \"recency_days\", \"median_dur_sec\"]\n\nspread = (\n    contact_labeled\n    .groupby(\"cluster\")[spread_features]\n    .quantile([0.25, 0.50, 0.75])\n    .unstack(level=1)\n)\n\nspread.columns = [f\"{feat}_q{int(q*100)}\" for feat, q in spread.columns]\nspread = spread.reset_index()\n\nspread.T\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>cluster</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>n_calls_q25</th>\n      <td>1.000000</td>\n      <td>23.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>n_calls_q50</th>\n      <td>2.000000</td>\n      <td>35.500000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>n_calls_q75</th>\n      <td>4.000000</td>\n      <td>85.250000</td>\n      <td>2.000000</td>\n      <td>9.000000</td>\n    </tr>\n    <tr>\n      <th>total_dur_sec_q25</th>\n      <td>819.500000</td>\n      <td>1156.750000</td>\n      <td>22.000000</td>\n      <td>109.000000</td>\n    </tr>\n    <tr>\n      <th>total_dur_sec_q50</th>\n      <td>1553.000000</td>\n      <td>2776.000000</td>\n      <td>61.000000</td>\n      <td>270.000000</td>\n    </tr>\n    <tr>\n      <th>total_dur_sec_q75</th>\n      <td>2971.500000</td>\n      <td>8409.500000</td>\n      <td>134.000000</td>\n      <td>507.000000</td>\n    </tr>\n    <tr>\n      <th>recency_days_q25</th>\n      <td>273.385498</td>\n      <td>59.149308</td>\n      <td>259.746244</td>\n      <td>94.075741</td>\n    </tr>\n    <tr>\n      <th>recency_days_q50</th>\n      <td>621.220833</td>\n      <td>84.802164</td>\n      <td>501.331516</td>\n      <td>259.384988</td>\n    </tr>\n    <tr>\n      <th>recency_days_q75</th>\n      <td>823.246152</td>\n      <td>255.288302</td>\n      <td>781.281389</td>\n      <td>515.196192</td>\n    </tr>\n    <tr>\n      <th>median_dur_sec_q25</th>\n      <td>355.000000</td>\n      <td>22.500000</td>\n      <td>11.000000</td>\n      <td>12.000000</td>\n    </tr>\n    <tr>\n      <th>median_dur_sec_q50</th>\n      <td>494.000000</td>\n      <td>35.500000</td>\n      <td>37.000000</td>\n      <td>34.000000</td>\n    </tr>\n    <tr>\n      <th>median_dur_sec_q75</th>\n      <td>748.500000</td>\n      <td>59.125000</td>\n      <td>72.500000</td>\n      <td>60.500000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n::: callout-tip\n\n### Why timing-share medians often look like 0 or 1\n\nMany contacts have only 1–2 calls. With tiny denominators, share features become extreme.\nSo for timing behaviour we also report **means** and **% of contacts with any activity** in each time band.\n:::\n\n---\n\n### Timing patterns (mean shares + % with any non-business-hours / Sunday activity)\n\n::: {#61dab895 .cell execution_count=48}\n``` {.python .cell-code}\nshare_cols = [\n    \"share_business_hours\", \"share_evening\", \"share_early_morning\", \"share_late_night\",\n    \"share_open_day\", \"share_closed_day\"\n]\n\ntiming_mean = (\n    contact_labeled\n    .groupby(\"cluster\")[share_cols]\n    .mean()\n    .add_suffix(\"_mean\")\n    .reset_index()\n)\n\ntiming_any = (\n    contact_labeled\n    .assign(\n        any_evening=lambda x: (x[\"share_evening\"] > 0).astype(int),\n        any_early_morning=lambda x: (x[\"share_early_morning\"] > 0).astype(int),\n        any_late_night=lambda x: (x[\"share_late_night\"] > 0).astype(int),\n        any_sunday=lambda x: (x[\"share_closed_day\"] > 0).astype(int),\n    )\n    .groupby(\"cluster\")[[\"any_evening\", \"any_early_morning\", \"any_late_night\", \"any_sunday\"]]\n    .mean()\n    .mul(100)\n    .add_suffix(\"_pct_contacts\")\n    .reset_index()\n)\n\ntiming_summary = (\n    timing_mean\n    .merge(timing_any, on=\"cluster\", how=\"left\")\n    .merge(cluster_sizes.reset_index(), on=\"cluster\", how=\"left\")\n    .sort_values(\"cluster\")\n    .reset_index(drop=True)\n)\n\ntiming_summary.T\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>cluster</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>share_business_hours_mean</th>\n      <td>0.874036</td>\n      <td>0.837563</td>\n      <td>0.905810</td>\n      <td>0.874097</td>\n    </tr>\n    <tr>\n      <th>share_evening_mean</th>\n      <td>0.077054</td>\n      <td>0.108877</td>\n      <td>0.065602</td>\n      <td>0.094157</td>\n    </tr>\n    <tr>\n      <th>share_early_morning_mean</th>\n      <td>0.033106</td>\n      <td>0.040734</td>\n      <td>0.020386</td>\n      <td>0.022898</td>\n    </tr>\n    <tr>\n      <th>share_late_night_mean</th>\n      <td>0.015803</td>\n      <td>0.012827</td>\n      <td>0.008203</td>\n      <td>0.008848</td>\n    </tr>\n    <tr>\n      <th>share_open_day_mean</th>\n      <td>0.917799</td>\n      <td>0.935881</td>\n      <td>0.957316</td>\n      <td>0.950137</td>\n    </tr>\n    <tr>\n      <th>share_closed_day_mean</th>\n      <td>0.082201</td>\n      <td>0.064119</td>\n      <td>0.042684</td>\n      <td>0.049863</td>\n    </tr>\n    <tr>\n      <th>any_evening_pct_contacts</th>\n      <td>15.384615</td>\n      <td>67.045455</td>\n      <td>9.130707</td>\n      <td>31.555556</td>\n    </tr>\n    <tr>\n      <th>any_early_morning_pct_contacts</th>\n      <td>8.791209</td>\n      <td>48.295455</td>\n      <td>3.189493</td>\n      <td>12.000000</td>\n    </tr>\n    <tr>\n      <th>any_late_night_pct_contacts</th>\n      <td>5.494505</td>\n      <td>25.568182</td>\n      <td>1.438399</td>\n      <td>3.111111</td>\n    </tr>\n    <tr>\n      <th>any_sunday_pct_contacts</th>\n      <td>15.384615</td>\n      <td>54.545455</td>\n      <td>5.753596</td>\n      <td>19.111111</td>\n    </tr>\n    <tr>\n      <th>n_contacts</th>\n      <td>91.000000</td>\n      <td>176.000000</td>\n      <td>1599.000000</td>\n      <td>225.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Segment names + action policy (stakeholder-ready layer)\n\nCluster IDs are arbitrary. We map clusters to segment labels using **behavioural signatures** from `cards`.\n\n::: {#acfd0a52 .cell execution_count=49}\n``` {.python .cell-code}\nsig = cards[[\n    \"cluster\", \"n_contacts\",\n    \"n_calls\", \"total_dur_sec\", \"median_dur_sec\", \"long_call_share\",\n    \"recency_days\", \"tenure_days\"\n]].copy()\n\n# Identify clusters by signatures (robust against label permutation)\none_off_cluster = int(sig.sort_values([\"n_calls\", \"total_dur_sec\"], ascending=[True, True]).iloc[0][\"cluster\"])\ncore_active_cluster = int(sig.sort_values([\"n_calls\", \"n_contacts\"], ascending=[False, False]).iloc[0][\"cluster\"])\nrare_deep_cluster = int(sig.sort_values([\"median_dur_sec\", \"long_call_share\"], ascending=[False, False]).iloc[0][\"cluster\"])\n\nremaining = sorted(set(sig[\"cluster\"]) - {one_off_cluster, core_active_cluster, rare_deep_cluster})\nassert len(remaining) == 1, \"Expected exactly one remaining cluster for 'warm occasional'.\"\nwarm_cluster = int(remaining[0])\n\nsegment_map = {\n    one_off_cluster: {\n        \"label\": \"One-off / low-engagement contacts\",\n        \"primary_action\": \"Deprioritize by default; automate nurture; reactivate only if new activity appears\"\n    },\n    core_active_cluster: {\n        \"label\": \"Core active relationships\",\n        \"primary_action\": \"Retention + priority servicing; assign owner; proactive follow-ups\"\n    },\n    warm_cluster: {\n        \"label\": \"Warm occasional contacts\",\n        \"primary_action\": \"Nurture cadence (monthly/quarterly); structured check-ins to increase engagement\"\n    },\n    rare_deep_cluster: {\n        \"label\": \"Rare deep conversations\",\n        \"primary_action\": \"High-touch when active; preserve context; personalized re-engagement when dormant\"\n    }\n}\n\ncluster_labels = (\n    pd.DataFrame([\n        {\"cluster\": k, \"segment_label\": v[\"label\"], \"primary_action\": v[\"primary_action\"]}\n        for k, v in segment_map.items()\n    ])\n    .merge(cluster_sizes.reset_index(), on=\"cluster\", how=\"left\")\n    .sort_values(\"cluster\")\n    .reset_index(drop=True)\n)\n\ncluster_labels\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cluster</th>\n      <th>segment_label</th>\n      <th>primary_action</th>\n      <th>n_contacts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Rare deep conversations</td>\n      <td>High-touch when active; preserve context; pers...</td>\n      <td>91</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Core active relationships</td>\n      <td>Retention + priority servicing; assign owner; ...</td>\n      <td>176</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>One-off / low-engagement contacts</td>\n      <td>Deprioritize by default; automate nurture; rea...</td>\n      <td>1599</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Warm occasional contacts</td>\n      <td>Nurture cadence (monthly/quarterly); structure...</td>\n      <td>225</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Cluster narrative (auto-generated from the cards)\n\nShort and operational: “what it looks like” + “what to do”.\n\n::: {#98ad59e2 .cell execution_count=50}\n``` {.python .cell-code}\ndef fmt(x, nd=1):\n    if pd.isna(x):\n        return \"NA\"\n    return f\"{float(x):.{nd}f}\"\n\ncards_idx = cards.set_index(\"cluster\")\nlabels_idx = cluster_labels.set_index(\"cluster\")\n\nnarr_rows = []\nfor c in sorted(cards_idx.index):\n    row = cards_idx.loc[c]\n    seg = labels_idx.loc[c, \"segment_label\"]\n    act = labels_idx.loc[c, \"primary_action\"]\n\n    narrative = (\n        f\"{seg}: median calls={fmt(row['n_calls'],1)}, \"\n        f\"median active days={fmt(row['n_days_active'],1)}, \"\n        f\"median total duration (sec)={fmt(row['total_dur_sec'],0)}, \"\n        f\"median call duration (sec)={fmt(row['median_dur_sec'],0)}, \"\n        f\"median recency (days)={fmt(row['recency_days'],0)}.\"\n    )\n\n    narr_rows.append({\n        \"cluster\": int(c),\n        \"segment_label\": seg,\n        \"n_contacts\": int(row[\"n_contacts\"]),\n        \"cluster_story\": narrative,\n        \"default_action\": act\n    })\n\ncluster_story = pd.DataFrame(narr_rows).sort_values(\"cluster\").reset_index(drop=True)\ncluster_story\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cluster</th>\n      <th>segment_label</th>\n      <th>n_contacts</th>\n      <th>cluster_story</th>\n      <th>default_action</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Rare deep conversations</td>\n      <td>91</td>\n      <td>Rare deep conversations: median calls=2.0, med...</td>\n      <td>High-touch when active; preserve context; pers...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Core active relationships</td>\n      <td>176</td>\n      <td>Core active relationships: median calls=35.5, ...</td>\n      <td>Retention + priority servicing; assign owner; ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>One-off / low-engagement contacts</td>\n      <td>1599</td>\n      <td>One-off / low-engagement contacts: median call...</td>\n      <td>Deprioritize by default; automate nurture; rea...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Warm occasional contacts</td>\n      <td>225</td>\n      <td>Warm occasional contacts: median calls=5.0, me...</td>\n      <td>Nurture cadence (monthly/quarterly); structure...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### “What decisions can the business make with these segments?”\n\nPolicy rules we can implement in a CRM without exposing identifiers.\n\n::: {#9168591d .cell execution_count=51}\n``` {.python .cell-code}\nrules = pd.DataFrame([\n    {\n        \"segment_label\": \"Core active relationships\",\n        \"decision_use\": \"Prioritization and retention\",\n        \"operational_rule\": \"Assign owner and SLA; proactive check-ins\",\n        \"typical_pattern\": \"High calls, many active days, low recency\"\n    },\n    {\n        \"segment_label\": \"Warm occasional contacts\",\n        \"decision_use\": \"Nurture and growth\",\n        \"operational_rule\": \"Monthly/quarterly outreach; reminders; targeted offers\",\n        \"typical_pattern\": \"Moderate repeat calls, moderate tenure, mid recency\"\n    },\n    {\n        \"segment_label\": \"Rare deep conversations\",\n        \"decision_use\": \"High-touch exceptions and win-back\",\n        \"operational_rule\": \"Personalized follow-up; preserve context; targeted reactivation if dormant\",\n        \"typical_pattern\": \"Few calls but long conversations (high median duration / long-call share)\"\n    },\n    {\n        \"segment_label\": \"One-off / low-engagement contacts\",\n        \"decision_use\": \"Noise filtering and automation\",\n        \"operational_rule\": \"Exclude from priority lists; automate nurture only\",\n        \"typical_pattern\": \"One call, one day, small total duration, often old\"\n    },\n])\n\nrules\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>segment_label</th>\n      <th>decision_use</th>\n      <th>operational_rule</th>\n      <th>typical_pattern</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Core active relationships</td>\n      <td>Prioritization and retention</td>\n      <td>Assign owner and SLA; proactive check-ins</td>\n      <td>High calls, many active days, low recency</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Warm occasional contacts</td>\n      <td>Nurture and growth</td>\n      <td>Monthly/quarterly outreach; reminders; targete...</td>\n      <td>Moderate repeat calls, moderate tenure, mid re...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Rare deep conversations</td>\n      <td>High-touch exceptions and win-back</td>\n      <td>Personalized follow-up; preserve context; targ...</td>\n      <td>Few calls but long conversations (high median ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>One-off / low-engagement contacts</td>\n      <td>Noise filtering and automation</td>\n      <td>Exclude from priority lists; automate nurture ...</td>\n      <td>One call, one day, small total duration, often...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Store artifacts for later phases (robustness, sensitivity, PCA)\n\n::: {#3ee28158 .cell execution_count=52}\n``` {.python .cell-code}\nprep[\"K_FINAL\"] = K_FINAL\nprep[\"kmeans\"] = kmeans\nprep[\"labels\"] = labels\n\nprep[\"contact_clusters\"] = contact_clusters\nprep[\"cluster_sizes\"] = cluster_sizes\n\nprep[\"cards\"] = cards\nprep[\"spread\"] = spread\nprep[\"timing_summary\"] = timing_summary\nprep[\"cluster_labels\"] = cluster_labels\nprep[\"cluster_story\"] = cluster_story\nprep[\"rules\"] = rules\n\nlist(prep.keys())[:15]\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```\n['MODEL_FEATURES',\n 'FEATURE_BLOCKS',\n 'feature_names',\n 'imputer',\n 'scaler',\n 'X_raw',\n 'X_clip',\n 'X',\n 'clip_table',\n 'dominance',\n 'preprocess_audit',\n 'X_uncapped',\n 'K_FINAL',\n 'kmeans',\n 'labels']\n```\n:::\n:::\n\n\n---\n\n\n## Phase 6 — Robustness and sensitivity (do the clusters “hold up”?)\n\nA good clustering result is not just “a nice plot.” It should be **stable** when we rerun the algorithm and **reasonably consistent** when we change *feature blocks*.\n\nIn this phase we test two kinds of robustness:\n\n1. **Seed stability:** If we change the random seed (different K-means initializations), do we get essentially the same segmentation?\n2. **Feature-block sensitivity:** If we drop one block (volume, duration, recency, timing), do the segments remain broadly similar?\n\nWe quantify stability using **Adjusted Rand Index (ARI)**:\n\n\n- $ARI = 1$ means two clusterings are identical up to label permutation.\n- $ARI \\approx 0$ means agreement is no better than random.\n- Negative $ARI$ can happen (worse than random agreement), usually a warning sign.\n\n::: callout-important\n### Training matrix\nAll robustness tests use `prep[\"X\"]` (robust-scaled + capped) so distances are fair and numerically stable.\n\n### Privacy\nWe do not merge names/phone numbers into any robustness outputs. We only evaluate labels as arrays.\n:::\n\n---\n\n### Seed stability test (ARI across many random seeds)\n\nWe run K-means many times with different `random_state`, always using $K = 4$, and compare each run to a baseline labeling.\n\n::: {#2f32c991 .cell execution_count=53}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# -------------------------\n# Setup\n# -------------------------\nX = prep[\"X\"]\nK_FINAL = int(prep.get(\"K_FINAL\", 4))\n\n# Baseline labels (use Phase 5 if available)\nif \"labels\" in prep and prep.get(\"labels\", None) is not None:\n    base_labels = np.asarray(prep[\"labels\"])\nelse:\n    km0 = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42)\n    base_labels = km0.fit_predict(X)\n\n# -------------------------\n# Run many seeds\n# -------------------------\nSEEDS = list(range(0, 50))  # employer-ready default\nrows = []\n\nfor s in SEEDS:\n    km = KMeans(n_clusters=K_FINAL, n_init=50, random_state=s)\n    lab = km.fit_predict(X)\n    ari = float(adjusted_rand_score(base_labels, lab))\n    rows.append({\"seed\": s, \"ari_vs_baseline\": ari, \"inertia\": float(km.inertia_)})\n\nseed_stability = (\n    pd.DataFrame(rows)\n    .sort_values(\"ari_vs_baseline\", ascending=True)\n    .reset_index(drop=True)\n)\n\nseed_audit = pd.DataFrame({\n    \"K_FINAL\": [K_FINAL],\n    \"n_seeds_tested\": [len(SEEDS)],\n    \"ari_min\": [float(seed_stability[\"ari_vs_baseline\"].min())],\n    \"ari_p05\": [float(seed_stability[\"ari_vs_baseline\"].quantile(0.05))],\n    \"ari_median\": [float(seed_stability[\"ari_vs_baseline\"].median())],\n    \"ari_p95\": [float(seed_stability[\"ari_vs_baseline\"].quantile(0.95))],\n    \"ari_max\": [float(seed_stability[\"ari_vs_baseline\"].max())],\n})\n\nseed_audit, seed_stability.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\n(   K_FINAL  n_seeds_tested  ari_min  ari_p05  ari_median  ari_p95  ari_max\n 0        4              50      1.0      1.0         1.0      1.0      1.0,\n    seed  ari_vs_baseline       inertia\n 0     0              1.0  26086.981574\n 1     1              1.0  26086.981574\n 2     2              1.0  26086.981574\n 3     3              1.0  26086.981574\n 4     4              1.0  26086.981574\n 5     5              1.0  26086.981574\n 6     6              1.0  26086.981574\n 7     7              1.0  26086.981574\n 8     8              1.0  26086.981574\n 9     9              1.0  26086.981574)\n```\n:::\n:::\n\n\n::: callout-tip\n\n### How to interpret seed ARI\n\nA practical rule of thumb:\n\n* **Very stable:** most ARI values $\\ge 0.90$\n* **Reasonably stable:** most ARI values $\\ge 0.75$\n* **Unstable:** many ARI values $< 0.60$\n\nIf stability is weak, we usually revisit preprocessing, $K$, or consider a different clustering method.\n:::\n\n---\n\n### “Worst-case” rerun inspection (label-aligned)\n\nEven when ARI is $1.0$, K-means can output the same partition with different numeric labels (label permutation).\nSo we align labels before comparing cluster sizes.\n\n::: {#8fa33ccf .cell execution_count=54}\n``` {.python .cell-code}\n# Identify worst seed run (lowest ARI vs baseline)\nworst = seed_stability.iloc[0]\nworst_seed = int(worst[\"seed\"])\n\nkm_worst = KMeans(n_clusters=K_FINAL, n_init=50, random_state=worst_seed)\nlabels_worst = km_worst.fit_predict(X)\n\nari_worst = float(adjusted_rand_score(base_labels, labels_worst))\n\n# Contingency table: baseline cluster IDs (rows) vs worst run IDs (cols)\nct = pd.crosstab(\n    pd.Series(base_labels, name=\"baseline\"),\n    pd.Series(labels_worst, name=\"worst\")\n)\n\n# Map each worst-cluster to the baseline cluster it overlaps with most\nmapping = ct.idxmax(axis=0).to_dict()\n\nlabels_worst_aligned = np.vectorize(mapping.get)(labels_worst)\n\n# Compare cluster sizes after alignment\nbase_sizes = pd.Series(base_labels).value_counts().sort_index()\nworst_sizes = pd.Series(labels_worst_aligned).value_counts().sort_index()\n\nsize_compare = pd.DataFrame({\n    \"cluster_id\": sorted(set(base_sizes.index) | set(worst_sizes.index)),\n}).set_index(\"cluster_id\")\n\nsize_compare[\"baseline_n\"] = base_sizes.reindex(size_compare.index).fillna(0).astype(int)\nsize_compare[\"worst_n_aligned\"] = worst_sizes.reindex(size_compare.index).fillna(0).astype(int)\nsize_compare[\"baseline_pct\"] = (size_compare[\"baseline_n\"] / size_compare[\"baseline_n\"].sum() * 100).round(2)\nsize_compare[\"worst_pct_aligned\"] = (size_compare[\"worst_n_aligned\"] / size_compare[\"worst_n_aligned\"].sum() * 100).round(2)\n\nworst_seed, float(worst[\"ari_vs_baseline\"]), ari_worst, mapping, size_compare.reset_index()\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```\n(0,\n 1.0,\n 1.0,\n {0: 0, 1: 1, 2: 3, 3: 2},\n    cluster_id  baseline_n  worst_n_aligned  baseline_pct  worst_pct_aligned\n 0           0          91               91          4.35               4.35\n 1           1         176              176          8.42               8.42\n 2           2        1599             1599         76.47              76.47\n 3           3         225              225         10.76              10.76)\n```\n:::\n:::\n\n\n---\n\n### Feature-block sensitivity (drop-one-block tests)\n\nNow we test whether the segmentation depends too heavily on a single block.\nWe refit K-means on reduced matrices and compare to baseline using ARI.\n\nWe do “drop one block at a time” on:\n\n* volume\n* duration\n* recency\n* timing\n\n::: callout-note\n\n### Why ARI is valid here\n\nCluster labels are arbitrary (cluster 0 in one run is not “the same” as cluster 0 in another).\nARI is invariant to label permutation, so it compares structure, not label IDs.\n:::\n\n::: {#4e7dd054 .cell execution_count=55}\n``` {.python .cell-code}\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\n\n# Rebuild X for each variant using the SAME preprocessing recipe as Phase 3:\n# - drop constant cols\n# - clip p01–p99 in raw space\n# - median impute + RobustScaler\n# - cap in scaled space\n\nSCALED_CAP = float(prep.get(\"scaled_cap\", 10.0))  # safe default\n\nFEATURE_BLOCKS = prep[\"FEATURE_BLOCKS\"]\nMODEL_FEATURES = prep[\"MODEL_FEATURES\"]\ndf_view = contact_features_view  # raw-unit feature table\n\ndef build_X_from_features(df, features, clip_lo=0.01, clip_hi=0.99, scaled_cap=10.0):\n    X_raw = df[features].copy()\n\n    # Drop truly constant columns\n    nunique = X_raw.nunique(dropna=False)\n    constant_cols = nunique[nunique <= 1].index.tolist()\n    X_raw2 = X_raw.drop(columns=constant_cols) if constant_cols else X_raw.copy()\n\n    # Clip extremes in raw space\n    X_clip = X_raw2.copy()\n    for col in X_clip.columns:\n        lo = float(X_clip[col].quantile(clip_lo))\n        hi = float(X_clip[col].quantile(clip_hi))\n        X_clip[col] = X_clip[col].clip(lower=lo, upper=hi)\n\n    # Impute + robust scale\n    imputer = SimpleImputer(strategy=\"median\")\n    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n    X_imp = imputer.fit_transform(X_clip)\n    X_scaled = scaler.fit_transform(X_imp)\n\n    # Final cap in scaled space\n    X_final = np.clip(X_scaled, -scaled_cap, scaled_cap)\n\n    return X_final, list(X_clip.columns)\n\n# Baseline rebuild for consistent comparisons\nX_base, base_feature_names = build_X_from_features(\n    df_view, MODEL_FEATURES, clip_lo=0.01, clip_hi=0.99, scaled_cap=SCALED_CAP\n)\nbase_labels_for_sens = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42).fit_predict(X_base)\n\n# Define variants: drop one block at a time (excluding \"category\")\nvariants = {}\nfor block in [\"volume\", \"duration\", \"recency\", \"timing\"]:\n    keep = []\n    for b, feats in FEATURE_BLOCKS.items():\n        if b == \"category\":\n            continue\n        if b != block:\n            keep += feats\n    variants[f\"drop_{block}\"] = keep\n\nrows = []\nfor name, feats in variants.items():\n    X_var, feat_names_var = build_X_from_features(\n        df_view, feats, clip_lo=0.01, clip_hi=0.99, scaled_cap=SCALED_CAP\n    )\n    lab = KMeans(n_clusters=K_FINAL, n_init=50, random_state=42).fit_predict(X_var)\n    ari = float(adjusted_rand_score(base_labels_for_sens, lab))\n    rows.append({\n        \"variant\": name,\n        \"n_features\": int(X_var.shape[1]),\n        \"ari_vs_baseline\": ari\n    })\n\nblock_sensitivity = (\n    pd.DataFrame(rows)\n    .sort_values(\"ari_vs_baseline\", ascending=False)\n    .reset_index(drop=True)\n)\n\nblock_sensitivity\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>variant</th>\n      <th>n_features</th>\n      <th>ari_vs_baseline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>drop_timing</td>\n      <td>14</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>drop_volume</td>\n      <td>15</td>\n      <td>0.943619</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>drop_recency</td>\n      <td>18</td>\n      <td>0.738307</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>drop_duration</td>\n      <td>13</td>\n      <td>0.698273</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Sensitivity summary (decision-useful)\n\n::: {#0080344f .cell execution_count=56}\n``` {.python .cell-code}\nsens_audit = pd.DataFrame({\n    \"K_FINAL\": [K_FINAL],\n    \"seed_stability_ari_median\": [float(seed_audit[\"ari_median\"].iloc[0])],\n    \"seed_stability_ari_p05\": [float(seed_audit[\"ari_p05\"].iloc[0])],\n    \"seed_stability_ari_min\": [float(seed_audit[\"ari_min\"].iloc[0])],\n    \"block_sensitivity_ari_min\": [float(block_sensitivity[\"ari_vs_baseline\"].min())],\n    \"block_sensitivity_ari_median\": [float(block_sensitivity[\"ari_vs_baseline\"].median())],\n})\n\nsens_audit.T\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>K_FINAL</th>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>seed_stability_ari_median</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>seed_stability_ari_p05</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>seed_stability_ari_min</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>block_sensitivity_ari_min</th>\n      <td>0.698273</td>\n    </tr>\n    <tr>\n      <th>block_sensitivity_ari_median</th>\n      <td>0.840963</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: callout-important\n\n### What we conclude here (based on your results)\n\n* **Seed stability:** ARI is $1.0$ for all tested seeds $\\Rightarrow$ K-means solution is *fully stable*.\n* **Block sensitivity:** dropping **duration** or **recency** reduces agreement (ARI $\\approx 0.70$–$0.74$), which implies these blocks carry meaningful segmentation signal.\n\nThis is robust enough for stakeholder-facing use, with a clear explanation of what drives the segments.\n:::\n\n---\n\n### Store robustness artifacts (for later reporting)\n\n::: {#ca8cc000 .cell execution_count=57}\n``` {.python .cell-code}\nprep[\"seed_stability\"] = seed_stability\nprep[\"seed_audit\"] = seed_audit\n\nprep[\"labels_worst_seed\"] = labels_worst\nprep[\"labels_worst_seed_aligned\"] = labels_worst_aligned\nprep[\"size_compare_worst_seed\"] = size_compare.reset_index()\nprep[\"worst_seed_label_mapping\"] = mapping\n\nprep[\"block_sensitivity\"] = block_sensitivity\nprep[\"sens_audit\"] = sens_audit\n\nlist(prep.keys())[:25]\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```\n['MODEL_FEATURES',\n 'FEATURE_BLOCKS',\n 'feature_names',\n 'imputer',\n 'scaler',\n 'X_raw',\n 'X_clip',\n 'X',\n 'clip_table',\n 'dominance',\n 'preprocess_audit',\n 'X_uncapped',\n 'K_FINAL',\n 'kmeans',\n 'labels',\n 'contact_clusters',\n 'cluster_sizes',\n 'cards',\n 'spread',\n 'timing_summary',\n 'cluster_labels',\n 'cluster_story',\n 'rules',\n 'seed_stability',\n 'seed_audit']\n```\n:::\n:::\n\n\n---\n\n::: callout-important\n## Phase 6 summary (robustness verdict)\n\nBased on your results:\n\n- **Seed stability (random restarts):** $ARI = 1.00$ for all tested seeds $\\Rightarrow$ the $K=4$ K-means solution is **fully stable** on `prep[\"X\"]`.\n- **Feature-block sensitivity (drop-one-block):**\n  - Drop timing: $ARI = 1.00$ (timing is not driving the segmentation)\n  - Drop volume: $ARI \\approx 0.94$ (volume contributes, but is not the sole driver)\n  - Drop recency: $ARI \\approx 0.74$ (recency contains meaningful signal)\n  - Drop duration: $ARI \\approx 0.70$ (duration contains meaningful signal)\n\n**Decision-useful takeaway:** The segmentation is stable and not an artifact of random initialization.  \nThe clusters are primarily supported by **duration** and **recency** behaviour (with volume contributing), so these are the main behavioural levers to emphasize in stakeholder explanations.\n\n:::\n\n\n---\n\n\n\n## Phase 7 — PCA visualization (for interpretation only)\n\nPCA is **not** used to train clustering.  \nWe use PCA only to create a 2D “map” of contacts for storytelling, sanity-checking, and explaining separation.\n\n::: callout-warning\n### PCA is not the clustering model\n- **Clustering was trained on:** `prep[\"X\"]` (robust-scaled + capped)\n- **PCA is used for:** visualization only (2D projection)\nIf PCA looks messy, it does **not** automatically mean clustering is wrong. PCA compresses information into 2 dimensions.\n:::\n\n::: callout-important\n### Privacy\nWe plot only:\n\n- PCA coordinates\n- cluster labels\nWe do **not** plot names or phone numbers, and we keep `contact_id` hashed.\n:::\n\n---\n\n### Fit PCA ($2$ components) on the clustering matrix\n\n::: {#9ebc7fa8 .cell execution_count=58}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nX = prep[\"X\"]  # scaled + capped matrix used for clustering\nlabels = np.asarray(prep[\"labels\"])\nK_FINAL = int(prep.get(\"K_FINAL\", 4))\n\npca = PCA(n_components=2, random_state=42)\nZ = pca.fit_transform(X)\n\npca_audit = pd.DataFrame({\n    \"n_components\": [2],\n    \"explained_var_ratio_pc1\": [float(pca.explained_variance_ratio_[0])],\n    \"explained_var_ratio_pc2\": [float(pca.explained_variance_ratio_[1])],\n    \"explained_var_ratio_total_2pc\": [float(pca.explained_variance_ratio_[:2].sum())],\n})\n\npca_audit\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_components</th>\n      <th>explained_var_ratio_pc1</th>\n      <th>explained_var_ratio_pc2</th>\n      <th>explained_var_ratio_total_2pc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>0.628668</td>\n      <td>0.215786</td>\n      <td>0.844454</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Build a plotting table (hashed id + PCA coordinates + cluster)\n\n::: {#7b29f29c .cell execution_count=59}\n``` {.python .cell-code}\npca_view = pd.DataFrame({\n    \"contact_id\": contact_features_view[\"contact_id\"].values,  # hashed only\n    \"pc1\": Z[:, 0],\n    \"pc2\": Z[:, 1],\n    \"cluster\": labels\n})\n\npca_view.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contact_id</th>\n      <th>pc1</th>\n      <th>pc2</th>\n      <th>cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00047e187745</td>\n      <td>-3.211533</td>\n      <td>-0.439848</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000d71073f3e</td>\n      <td>17.283469</td>\n      <td>-2.797619</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001c36642ecb</td>\n      <td>6.364608</td>\n      <td>-1.535155</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00ad9a0b1291</td>\n      <td>-2.066420</td>\n      <td>3.971317</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00ed0d18f9e3</td>\n      <td>-1.794263</td>\n      <td>1.314460</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### PCA scatter plot (clusters on the 2D map)\n\n::: {#931aee16 .cell execution_count=60}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot each cluster separately for a clear legend\nfor c in sorted(pca_view[\"cluster\"].unique()):\n    sub = pca_view[pca_view[\"cluster\"] == c]\n    ax.scatter(sub[\"pc1\"], sub[\"pc2\"], s=12, alpha=0.7, label=f\"Cluster {c}\")\n\nax.set_title(f\"PCA map of contacts (K-means clusters, $K={K_FINAL}$)\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.legend(title=\"Cluster\", fontsize=9)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-61-output-1.png){width=662 height=524}\n:::\n:::\n\n\n---\n\n### Optional: cluster centroids projected into PCA space\n\nThis helps stakeholders see “where the centers sit” on the map.\n\n::: {#98bffd28 .cell execution_count=61}\n``` {.python .cell-code}\n# K-means centroids exist in scaled feature space; PCA was fit on the same space.\ncentroids = prep[\"kmeans\"].cluster_centers_\ncentroids_2d = pca.transform(centroids)\n\ncent = pd.DataFrame({\n    \"cluster\": list(range(K_FINAL)),\n    \"pc1_centroid\": centroids_2d[:, 0],\n    \"pc2_centroid\": centroids_2d[:, 1],\n})\n\ncent\n```\n\n::: {.cell-output .cell-output-display execution_count=61}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cluster</th>\n      <th>pc1_centroid</th>\n      <th>pc2_centroid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>4.245796</td>\n      <td>11.770443</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>15.100966</td>\n      <td>-1.621873</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-2.582659</td>\n      <td>-0.175937</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4.824594</td>\n      <td>-2.241496</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#c3857cf7 .cell execution_count=62}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor c in sorted(pca_view[\"cluster\"].unique()):\n    sub = pca_view[pca_view[\"cluster\"] == c]\n    ax.scatter(sub[\"pc1\"], sub[\"pc2\"], s=10, alpha=0.45, label=f\"Cluster {c}\")\n\n# Centroids\nax.scatter(cent[\"pc1_centroid\"], cent[\"pc2_centroid\"], s=120, marker=\"X\", label=\"Centroids\")\n\nax.set_title(f\"PCA map with centroids (K={K_FINAL})\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.legend(fontsize=9)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](walsoft_contact_segmentation_files/figure-html/cell-63-output-1.png){width=662 height=523}\n:::\n:::\n\n\n---\n\n### Interpretation guidance (what we conclude from PCA)\n\n::: callout-note\nPCA is a **compression** from many dimensions down to 2.\n\nUse this plot to check:\n\n* whether clusters are *roughly separated* (good sign),\n* whether some clusters overlap heavily (could be fine; segmentation can still be valid),\n* whether a small cluster looks like extreme outliers (could indicate a “special handling” segment).\n\nDo **not** use PCA to choose $K$ or to claim “the true number of clusters.”\n:::\n\n---\n\n### Store PCA artifacts (for the final report)\n\n::: {#84e55902 .cell execution_count=63}\n``` {.python .cell-code}\nprep[\"pca_model\"] = pca\nprep[\"pca_audit\"] = pca_audit\nprep[\"pca_view\"] = pca_view\nprep[\"pca_centroids_2d\"] = cent\n\nlist(prep.keys())[:30]\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```\n['MODEL_FEATURES',\n 'FEATURE_BLOCKS',\n 'feature_names',\n 'imputer',\n 'scaler',\n 'X_raw',\n 'X_clip',\n 'X',\n 'clip_table',\n 'dominance',\n 'preprocess_audit',\n 'X_uncapped',\n 'K_FINAL',\n 'kmeans',\n 'labels',\n 'contact_clusters',\n 'cluster_sizes',\n 'cards',\n 'spread',\n 'timing_summary',\n 'cluster_labels',\n 'cluster_story',\n 'rules',\n 'seed_stability',\n 'seed_audit',\n 'labels_worst_seed',\n 'labels_worst_seed_aligned',\n 'size_compare_worst_seed',\n 'worst_seed_label_mapping',\n 'block_sensitivity']\n```\n:::\n:::\n\n\n---\n\n\n## What PCA tells us (interpretation only)\n\nPCA is a 2D projection of the same scaled training matrix used for clustering, `prep[\"X\"]`.\nIt does **not** change the model; it only helps us *see* separation.\n\n### Variance captured\n\nFrom `pca_audit`, the first two components explain:\n\n- $PC1 \\approx 0.629$\n- $PC2 \\approx 0.216$\n- Total (2D) $\\approx 0.844$\n\nSo about **84.4%** of the variance in the scaled feature space is visible in the 2D map.\nThat is high enough that the plot is a meaningful sanity-check.\n\n### Visual separation (sanity-check)\n\nThe PCA map shows clear structure:\n\n- One cluster sits far to the **right** on $PC1$ (strong separation in the main direction of variance).\n- One cluster is concentrated far to the **left** (tight wedge near low $PC1$ values).\n- Two clusters occupy the middle/bottom region with partial overlap, suggesting they differ in *multiple* dimensions (not fully separable in 2D, which is normal).\n\nThis supports the earlier interpretation: the segmentation is not an artifact of one random run (Phase 6 already proved stability), and the clusters correspond to genuinely different behavioral regimes in feature space.\n\n### Centroids on the PCA map\n\nProjected centroids help explain “where the centers sit”:\n\n- The centroid that is far right on $PC1$ corresponds to a high-intensity / high-activity regime.\n- The centroid that is far left corresponds to low-engagement / near-baseline contacts.\n- The centroid that is high on $PC2$ indicates a behavior pattern that is not just “more or less activity,” but a different mix (often duration/recency-related structure).\n\n**Important:** overlap in PCA does not invalidate clusters. K-means was fit in the full feature space; PCA compresses information into 2 dimensions.\n\n\n---\n\n## Phase 8 — Final deliverables (stakeholder-ready outputs)\n\nIn this phase we package the work into **decision-ready artifacts** that can be used in a CRM or reporting workflow **without exposing identities**.\n\n::: callout-important\n### Privacy and operational use\n- We **do not** join names or phone numbers into any outputs.\n- All contact identifiers remain **hashed**.\n- Default is **no file export**. You can enable export if you want portfolio artifacts.\n:::\n\n---\n\n### Assemble the “final segment table” (labels + key medians)\n\n::: {#cd8aeaea .cell execution_count=64}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n# Guardrails\nrequired = [\"cards\", \"cluster_labels\", \"rules\", \"sens_audit\", \"pca_audit\"]\nmissing = [k for k in required if k not in prep]\nassert len(missing) == 0, f\"Missing Phase artifacts in prep: {missing}\"\n\ncards = prep[\"cards\"].copy()\ncluster_labels = prep[\"cluster_labels\"].copy()\nrules = prep[\"rules\"].copy()\n\n# Join: cluster -> segment label + action\nfinal_segments = (\n    cards.merge(cluster_labels[[\"cluster\", \"segment_label\", \"primary_action\"]], on=\"cluster\", how=\"left\")\n    .sort_values(\"n_contacts\", ascending=False)\n    .reset_index(drop=True)\n)\n\n# Reorder columns for a stakeholder view\nfront = [\"cluster\", \"segment_label\", \"n_contacts\", \"primary_action\"]\nrest = [c for c in final_segments.columns if c not in front]\nfinal_segments = final_segments[front + rest]\n\nfinal_segments\n```\n\n::: {.cell-output .cell-output-display execution_count=64}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cluster</th>\n      <th>segment_label</th>\n      <th>n_contacts</th>\n      <th>primary_action</th>\n      <th>n_calls</th>\n      <th>n_days_active</th>\n      <th>calls_per_active_day</th>\n      <th>total_dur_sec</th>\n      <th>median_dur_sec</th>\n      <th>long_call_share</th>\n      <th>recency_days</th>\n      <th>tenure_days</th>\n      <th>share_business_hours</th>\n      <th>share_evening</th>\n      <th>share_early_morning</th>\n      <th>share_late_night</th>\n      <th>share_open_day</th>\n      <th>share_closed_day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>One-off / low-engagement contacts</td>\n      <td>1599</td>\n      <td>Deprioritize by default; automate nurture; rea...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>61.0</td>\n      <td>37.0</td>\n      <td>0.0</td>\n      <td>501.331516</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>Warm occasional contacts</td>\n      <td>225</td>\n      <td>Nurture cadence (monthly/quarterly); structure...</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>1.333333</td>\n      <td>270.0</td>\n      <td>34.0</td>\n      <td>0.0</td>\n      <td>259.384988</td>\n      <td>312.908426</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Core active relationships</td>\n      <td>176</td>\n      <td>Retention + priority servicing; assign owner; ...</td>\n      <td>35.5</td>\n      <td>21.0</td>\n      <td>1.707143</td>\n      <td>2776.0</td>\n      <td>35.5</td>\n      <td>0.0</td>\n      <td>84.802164</td>\n      <td>676.594664</td>\n      <td>0.911879</td>\n      <td>0.047619</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.983416</td>\n      <td>0.016584</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>Rare deep conversations</td>\n      <td>91</td>\n      <td>High-touch when active; preserve context; pers...</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.200000</td>\n      <td>1553.0</td>\n      <td>494.0</td>\n      <td>0.6</td>\n      <td>621.220833</td>\n      <td>0.813924</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Decision rules (CRM / outreach policy)\n\n::: {#4cb064b9 .cell execution_count=65}\n``` {.python .cell-code}\nrules\n```\n\n::: {.cell-output .cell-output-display execution_count=65}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>segment_label</th>\n      <th>decision_use</th>\n      <th>operational_rule</th>\n      <th>typical_pattern</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Core active relationships</td>\n      <td>Prioritization and retention</td>\n      <td>Assign owner and SLA; proactive check-ins</td>\n      <td>High calls, many active days, low recency</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Warm occasional contacts</td>\n      <td>Nurture and growth</td>\n      <td>Monthly/quarterly outreach; reminders; targete...</td>\n      <td>Moderate repeat calls, moderate tenure, mid re...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Rare deep conversations</td>\n      <td>High-touch exceptions and win-back</td>\n      <td>Personalized follow-up; preserve context; targ...</td>\n      <td>Few calls but long conversations (high median ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>One-off / low-engagement contacts</td>\n      <td>Noise filtering and automation</td>\n      <td>Exclude from priority lists; automate nurture ...</td>\n      <td>One call, one day, small total duration, often...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### “Executive summary” (one screen)\n\n::: {#7098bcdd .cell execution_count=66}\n``` {.python .cell-code}\nsens_audit = prep[\"sens_audit\"].copy()\npca_audit = prep[\"pca_audit\"].copy()\n\n# Key stats\nn_contacts_total = int(prep[\"contact_clusters\"].shape[0])\nk_final = int(prep.get(\"K_FINAL\", 4))\n\nseed_median = float(sens_audit[\"seed_stability_ari_median\"].iloc[0])\nseed_min = float(sens_audit[\"seed_stability_ari_min\"].iloc[0])\nblock_min = float(sens_audit[\"block_sensitivity_ari_min\"].iloc[0])\n\npca_total2 = float(pca_audit[\"explained_var_ratio_total_2pc\"].iloc[0])\n\nexec_summary = pd.DataFrame([{\n    \"n_contacts_clustered\": n_contacts_total,\n    \"K_FINAL\": k_final,\n    \"seed_stability_ARI_median\": seed_median,\n    \"seed_stability_ARI_min\": seed_min,\n    \"block_sensitivity_ARI_min\": block_min,\n    \"pca_variance_explained_2PC\": pca_total2,\n}])\n\nexec_summary\n```\n\n::: {.cell-output .cell-output-display execution_count=66}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_contacts_clustered</th>\n      <th>K_FINAL</th>\n      <th>seed_stability_ARI_median</th>\n      <th>seed_stability_ARI_min</th>\n      <th>block_sensitivity_ARI_min</th>\n      <th>pca_variance_explained_2PC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2091</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.698273</td>\n      <td>0.844454</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\n### Findings (plain-English, decision-first)\n\n::: {#fc8b72ff .cell execution_count=67}\n``` {.python .cell-code}\n# Convert the segment table into a short narrative summary\nseg_counts = final_segments[[\"segment_label\", \"n_contacts\"]].copy()\nseg_counts[\"share_pct\"] = (seg_counts[\"n_contacts\"] / seg_counts[\"n_contacts\"].sum() * 100).round(2)\n\nseg_counts\n```\n\n::: {.cell-output .cell-output-display execution_count=67}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>segment_label</th>\n      <th>n_contacts</th>\n      <th>share_pct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One-off / low-engagement contacts</td>\n      <td>1599</td>\n      <td>76.47</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Warm occasional contacts</td>\n      <td>225</td>\n      <td>10.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Core active relationships</td>\n      <td>176</td>\n      <td>8.42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rare deep conversations</td>\n      <td>91</td>\n      <td>4.35</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#7564c410 .cell execution_count=68}\n``` {.python .cell-code}\n# Build a compact “what it is + what to do” list (no IDs)\nlabels_idx = cluster_labels.set_index(\"segment_label\")\n\nrows = []\nfor _, r in seg_counts.iterrows():\n    seg = r[\"segment_label\"]\n    n = int(r[\"n_contacts\"])\n    pct = float(r[\"share_pct\"])\n    action = labels_idx.loc[seg, \"primary_action\"] if seg in labels_idx.index else \"TBD\"\n    rows.append({\"segment_label\": seg, \"n_contacts\": n, \"share_pct\": pct, \"default_action\": action})\n\nfindings_table = pd.DataFrame(rows).sort_values(\"n_contacts\", ascending=False).reset_index(drop=True)\nfindings_table\n```\n\n::: {.cell-output .cell-output-display execution_count=68}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>segment_label</th>\n      <th>n_contacts</th>\n      <th>share_pct</th>\n      <th>default_action</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One-off / low-engagement contacts</td>\n      <td>1599</td>\n      <td>76.47</td>\n      <td>Deprioritize by default; automate nurture; rea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Warm occasional contacts</td>\n      <td>225</td>\n      <td>10.76</td>\n      <td>Nurture cadence (monthly/quarterly); structure...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Core active relationships</td>\n      <td>176</td>\n      <td>8.42</td>\n      <td>Retention + priority servicing; assign owner; ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rare deep conversations</td>\n      <td>91</td>\n      <td>4.35</td>\n      <td>High-touch when active; preserve context; pers...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: callout-note\n\n### What we learned from your results\n\n* **The solution is fully stable to random initialization** (seed ARI $\\approx 1$ across tested seeds), so you can rerun K-means and get the same segmentation (up to label permutation).\n* **Recency and duration matter**: dropping either block reduces agreement (lower ARI), so those blocks carry meaningful segmentation signal.\n* **PCA is supportive, not decisive**: the first two PCs explain a large share of variance, and the 2D map shows visible separation patterns that align with the segmentation.\n:::\n\n---\n\n### Limitations and “how not to misuse this”\n\n::: callout-warning\n\n### Limitations\n\n* This is **behavior-based segmentation**, not a causal model. It supports prioritization and outreach strategy, not “ground truth identities.”\n* K-means assumes roughly spherical clusters in the feature space. If your business needs non-spherical shapes, consider alternative methods later (e.g., GMM, HDBSCAN).\n* Very sparse contacts (1–2 calls) can make share features extreme; we handled this by using medians + mean/%-any checks for timing.\n:::\n\n---\n\n### Optional: export portfolio artifacts (OFF by default)\n\n::: {#2dcce863 .cell execution_count=69}\n``` {.python .cell-code}\nSAVE_FILES = False  # keep False unless you explicitly want exports\n\nif SAVE_FILES:\n    # These contain NO names/phone numbers. contact_id stays hashed.\n    final_segments.to_csv(\"final_segments_cluster_cards.csv\", index=False)\n    rules.to_csv(\"segment_policy_rules.csv\", index=False)\n    findings_table.to_csv(\"segment_findings_summary.csv\", index=False)\n    exec_summary.to_csv(\"executive_summary_metrics.csv\", index=False)\n\n    print(\"Saved CSV files (hashed IDs only, no raw identifiers).\")\nelse:\n    print(\"SAVE_FILES=False (no files written).\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSAVE_FILES=False (no files written).\n```\n:::\n:::\n\n\n---\n\n\n## Phase 10 — Final wrap-up (competition framing + decision narrative)\n\n### Competition prompt (what we were given)\n\n**We were given** a historical phone-call behaviour dataset where each record represents a call event.  \nThe dataset includes **timestamps, call duration, and categorised call context**, but **no business labels** such as “good customer” or “bad customer”.\n\n**Task:** Without supervision (no labels), we had to build a **contact-level segmentation** that a business could actually use for prioritisation, outreach cadence, and relationship management.\n\n**Hard constraints:**\n\n- **Privacy-first:** do not use names or phone numbers; keep identifiers hashed.\n- **Operational:** output must translate into **clear segment actions**, not just clusters.\n- **Professional robustness:** results must be stable, not a “nice plot”.\n\n---\n\n### What we had to produce (deliverable definition)\n\nWe had to deliver:\n\n1. A **clean contact-level feature table** (one row per contact).\n2. A **distance-ready clustering matrix** (robust preprocessing).\n3. A final segmentation using **K-means with $K=4$**.\n4. **Stakeholder-ready interpretation** (cluster “cards” + segment labels + default actions).\n5. **Robustness evidence** (seed stability and feature sensitivity).\n6. **Interpretation-only visual sanity-check** (PCA map).\n\n---\n\n### What we built (end-to-end pipeline)\n\n1. **Audit** the raw call-event data (types, missingness, constraints).\n2. **Aggregate to contacts** (the clustering unit), producing features that capture:\n   - volume/intensity (calls, active days),\n   - relationship depth proxy (duration metrics, long-call share),\n   - recency/tenure (dormancy vs continuity),\n   - timing mix (business hours vs off-hours).\n3. **Preprocess for K-means**:\n   - clip extreme values in raw space,\n   - median imputation,\n   - robust scaling,\n   - cap scaled values for numerical stability.\n4. **Fit K-means** with $K=4$ and attach labels to contacts (hashed IDs only).\n5. **Interpret** clusters using raw units and translate into segments + actions.\n6. **Validate robustness** using ARI across seeds and drop-one-block sensitivity.\n7. **Use PCA only for visualization** (interpretation, not training).\n\n---\n\n### The segments (what we found + what to do)\n\nFrom the cluster cards and behaviour-based mapping, we obtained four operational segments:\n\n- **One-off / low-engagement contacts** (largest share)  \n  **Action:** deprioritize by default; automate nurture; reactivate only if new activity appears.\n\n- **Warm occasional contacts**  \n  **Action:** nurture cadence (monthly/quarterly); structured check-ins to increase engagement.\n\n- **Core active relationships**  \n  **Action:** retention and priority servicing; assign owner; proactive follow-ups.\n\n- **Rare deep conversations**  \n  **Action:** high-touch when active; preserve context; personalized re-engagement when dormant.\n\n---\n\n### Proof the solution “holds up” (robustness)\n\n**Seed stability:** ARI was $1.00$ across all tested seeds.  \nSo the segmentation is fully stable to random initialization (up to label permutation).\n\n**Feature sensitivity:** dropping duration or recency reduces agreement (ARI drops to about $0.70$–$0.74$).  \nInterpretation: **duration and recency carry meaningful segmentation signal**; timing contributes less to the final partition.\n\n---\n\n### What PCA contributed (interpretation only)\n\nPCA is a 2D projection of `prep[\"X\"]` for storytelling and sanity-checking.\n\n- The first two components explain about $84.45\\%$ of variance.\n- The map shows visible structure consistent with the segmentation.\n- PCA overlap does not invalidate clusters because K-means was trained in the full feature space.\n\n---\n\n### Decision use: what a business can do with these segments\n\n- **Routing:** prioritize “Core active” + “Rare deep” to high-touch handling.\n- **Cadence:** automate outreach for “Warm occasional”; suppress manual effort for “One-off”.\n- **Reactivation:** trigger win-back campaigns for dormant “Rare deep” and “Warm occasional”.\n- **Service levels:** define SLA tiers using segment label and recency.\n\n---\n\n### Limitations (how not to misuse this)\n\n- This is behaviour segmentation, not identity inference and not causal.\n- K-means prefers spherical separation in feature space; alternative methods can be tested later.\n- Very sparse contacts can make timing shares extreme; we mitigated via robust summaries and mean/%-any checks.\n\n::: callout-note\n**Final deliverables:**\n\n- A stable $K=4$ segmentation (hashed IDs only).\n- Stakeholder-ready cluster cards + action policy rules.\n- Robustness evidence (seed ARI + block sensitivity).\n- PCA map used only for interpretation and storytelling.\n:::\n\n\n---\n\n",
    "supporting": [
      "walsoft_contact_segmentation_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}